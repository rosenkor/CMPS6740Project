{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3NrNBvq8WqK"
      },
      "source": [
        "# Federated Reinforcement Learning for Recommendation Systems\n",
        "\n",
        "This notebook combines multiple Python files implementing a federated reinforcement learning system for recommendations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8Sjywjw8WqP"
      },
      "source": [
        "## Environment Setup\n",
        "\n",
        "First, let's import all required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOLDz-Vi9pJD",
        "outputId": "3a2ff0d4-a30b-4d72-c96c-39ed809c16e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting recsim-v2\n",
            "  Downloading recsim_v2-0.2.7.tar.gz (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.8/71.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from recsim-v2) (1.4.0)\n",
            "Requirement already satisfied: dopamine-rl>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from recsim-v2) (4.0.9)\n",
            "Collecting gin-config-v2 (from recsim-v2)\n",
            "  Downloading gin_config_v2-0.8.0.tar.gz (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from recsim-v2) (0.25.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from recsim-v2) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from recsim-v2) (1.13.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from recsim-v2) (2.17.0)\n",
            "Requirement already satisfied: gin-config>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from dopamine-rl>=2.0.5->recsim-v2) (0.5.0)\n",
            "Requirement already satisfied: opencv-python>=3.4.8.29 in /usr/local/lib/python3.10/dist-packages (from dopamine-rl>=2.0.5->recsim-v2) (4.10.0.84)\n",
            "Requirement already satisfied: flax>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from dopamine-rl>=2.0.5->recsim-v2) (0.8.5)\n",
            "Requirement already satisfied: jax>=0.1.72 in /usr/local/lib/python3.10/dist-packages (from dopamine-rl>=2.0.5->recsim-v2) (0.4.33)\n",
            "Requirement already satisfied: jaxlib>=0.1.51 in /usr/local/lib/python3.10/dist-packages (from dopamine-rl>=2.0.5->recsim-v2) (0.4.33)\n",
            "Requirement already satisfied: Pillow>=7.0.0 in /usr/local/lib/python3.10/dist-packages (from dopamine-rl>=2.0.5->recsim-v2) (10.4.0)\n",
            "Requirement already satisfied: pygame>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from dopamine-rl>=2.0.5->recsim-v2) (2.6.1)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.10/dist-packages (from dopamine-rl>=2.0.5->recsim-v2) (2.2.2)\n",
            "Requirement already satisfied: tf-slim>=1.0 in /usr/local/lib/python3.10/dist-packages (from dopamine-rl>=2.0.5->recsim-v2) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-probability>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from dopamine-rl>=2.0.5->recsim-v2) (0.24.0)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from dopamine-rl>=2.0.5->recsim-v2) (4.66.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym->recsim-v2) (3.1.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym->recsim-v2) (0.0.8)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->recsim-v2) (0.44.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax>=0.2.0->dopamine-rl>=2.0.5->recsim-v2) (1.1.0)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax>=0.2.0->dopamine-rl>=2.0.5->recsim-v2) (0.2.3)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax>=0.2.0->dopamine-rl>=2.0.5->recsim-v2) (0.6.4)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax>=0.2.0->dopamine-rl>=2.0.5->recsim-v2) (0.1.67)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax>=0.2.0->dopamine-rl>=2.0.5->recsim-v2) (13.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax>=0.2.0->dopamine-rl>=2.0.5->recsim-v2) (6.0.2)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow->recsim-v2) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow->recsim-v2) (0.13.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->dopamine-rl>=2.0.5->recsim-v2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->dopamine-rl>=2.0.5->recsim-v2) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->dopamine-rl>=2.0.5->recsim-v2) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->recsim-v2) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->recsim-v2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->recsim-v2) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->recsim-v2) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow->recsim-v2) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow->recsim-v2) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow->recsim-v2) (3.0.6)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.13.0->dopamine-rl>=2.0.5->recsim-v2) (4.4.2)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.13.0->dopamine-rl>=2.0.5->recsim-v2) (0.1.8)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax>=0.2.0->dopamine-rl>=2.0.5->recsim-v2) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax>=0.2.0->dopamine-rl>=2.0.5->recsim-v2) (2.18.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow->recsim-v2) (3.0.2)\n",
            "Requirement already satisfied: chex>=0.1.86 in /usr/local/lib/python3.10/dist-packages (from optax->flax>=0.2.0->dopamine-rl>=2.0.5->recsim-v2) (0.1.87)\n",
            "Requirement already satisfied: etils[epy] in /usr/local/lib/python3.10/dist-packages (from optax->flax>=0.2.0->dopamine-rl>=2.0.5->recsim-v2) (1.10.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.2.0->dopamine-rl>=2.0.5->recsim-v2) (1.6.0)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.2.0->dopamine-rl>=2.0.5->recsim-v2) (4.11.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.86->optax->flax>=0.2.0->dopamine-rl>=2.0.5->recsim-v2) (0.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.2.0->dopamine-rl>=2.0.5->recsim-v2) (0.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.2.0->dopamine-rl>=2.0.5->recsim-v2) (2024.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.2.0->dopamine-rl>=2.0.5->recsim-v2) (6.4.5)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.2.0->dopamine-rl>=2.0.5->recsim-v2) (3.20.2)\n",
            "Building wheels for collected packages: recsim-v2, gin-config-v2\n",
            "  Building wheel for recsim-v2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for recsim-v2: filename=recsim_v2-0.2.7-py3-none-any.whl size=109746 sha256=9d44f21675fb3e1a1e4e819b2bf3844e3a499467a9f7f46d3b59d3d526cc5a19\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/0c/8d/b2de8b95d998c4e167919fd3c0f2101d3fb36523a4105dda8d\n",
            "  Building wheel for gin-config-v2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gin-config-v2: filename=gin_config_v2-0.8.0-py3-none-any.whl size=60935 sha256=1b4cf0ccfb91157f03f8d7e91f4bf6ba69fbd3001f97b8a9e8894a2d7044219e\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/88/cd/fa1c40eb56836f9c58eee25ac182c14556a5af1eeffc6858c5\n",
            "Successfully built recsim-v2 gin-config-v2\n",
            "Installing collected packages: gin-config-v2, recsim-v2\n",
            "Successfully installed gin-config-v2-0.8.0 recsim-v2-0.2.7\n"
          ]
        }
      ],
      "source": [
        "!pip install recsim-v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-twKUlbS8WqQ",
        "outputId": "3a653ac2-b83a-4a33-9a61-a59f234e8db6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:55: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs; see https://jax.readthedocs.io/en/latest/aot.html. For example, replace xla_computation(f)(*xs) with jit(f).lower(*xs).compiler_ir('hlo'). See CHANGELOG.md for 0.4.30 for more examples.\n",
            "  from jax import xla_computation as _xla_computation\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import datetime\n",
        "import time\n",
        "from collections import deque\n",
        "from pathlib import Path\n",
        "from torch import nn\n",
        "from torch.nn.utils import weight_norm\n",
        "from gym import spaces\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# RecSim imports\n",
        "from recsim import document\n",
        "from recsim import user\n",
        "from recsim.choice_model import MultinomialLogitChoiceModel\n",
        "from recsim.simulator import environment\n",
        "from recsim.simulator import recsim_gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXCiVkybTBkq",
        "outputId": "0a810b23-03c0-4642-e0f4-1fb0d8983379"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "#!pip uninstall gin-config-v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJS8FHSQOFU_"
      },
      "outputs": [],
      "source": [
        "#!pip show gin-config-v2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMahL-n38WqR"
      },
      "source": [
        "## Replay Memory Implementation\n",
        "\n",
        "Implementation of experience replay buffer for storing transitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8h_Apx6s8WqR"
      },
      "outputs": [],
      "source": [
        "class ReplayMemory():\n",
        "    def __init__(self, capacity, state_shape, action_shape):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.capacity = capacity\n",
        "        self.state_memory = torch.zeros((capacity,) + state_shape, device=self.device)\n",
        "        self.action_memory = torch.zeros((capacity,) + action_shape, device=self.device)\n",
        "        self.reward_memory = torch.zeros((capacity,), device=self.device)\n",
        "        self.next_state_memory = torch.zeros((capacity,) + state_shape, device=self.device)\n",
        "        self.terminals_memory = torch.zeros((capacity,), dtype=torch.bool, device=self.device)\n",
        "        self.click_memory = torch.zeros((capacity,) + action_shape, dtype=torch.int, device=self.device)\n",
        "        self.position = 0\n",
        "        self.full = False\n",
        "\n",
        "    def push(self, state, action, reward, click, next_state, done):\n",
        "        self.state_memory[self.position] = state\n",
        "        self.action_memory[self.position] = action\n",
        "        self.reward_memory[self.position] = reward\n",
        "        self.click_memory[self.position] = click\n",
        "        self.next_state_memory[self.position] = next_state\n",
        "        self.terminals_memory[self.position] = done\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "        self.full = self.full or self.position == 0\n",
        "\n",
        "    def recall(self, indices):\n",
        "        states = self.state_memory[indices]\n",
        "        actions = self.action_memory[indices]\n",
        "        rewards = self.reward_memory[indices]\n",
        "        clicks = self.click_memory[indices]\n",
        "        next_states = self.next_state_memory[indices]\n",
        "        terminals = self.terminals_memory[indices]\n",
        "        return states, actions, rewards, clicks, next_states, terminals\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.capacity if self.full else self.position"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SujKGGGl8WqS"
      },
      "source": [
        "## Neural Network Models\n",
        "\n",
        "Implementation of the Q-Network and MLP Network architectures used in the recommendation system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YLfC56G8WqS"
      },
      "outputs": [],
      "source": [
        "class QNet(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.online = nn.Sequential(\n",
        "            nn.Linear(input_dim, 4096),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(4096, output_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.target = nn.Sequential(\n",
        "            nn.Linear(input_dim, 4096),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(4096, output_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.target.eval()\n",
        "        for p in self.target.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, inputs, model):\n",
        "        if model == \"online\":\n",
        "            return self.online(inputs)\n",
        "        elif model == \"target\":\n",
        "            return self.target(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95mSjDKb8WqT"
      },
      "outputs": [],
      "source": [
        "class MLPNet(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.online = nn.Sequential(\n",
        "            nn.Linear(input_dim, 2048),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(2048, 2048),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(2048, 2048),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(2048, 2048),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(2048, output_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.target = nn.Sequential(\n",
        "            nn.Linear(input_dim, 2048),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(2048, 2048),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(2048, 2048),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(2048, 2048),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(2048, output_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.target.eval()\n",
        "        for p in self.target.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, inputs, model):\n",
        "        if model == \"online\":\n",
        "            return self.online(inputs)\n",
        "        elif model == \"target\":\n",
        "            return self.target(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDR89mwq8WqU"
      },
      "source": [
        "## Slate Q-Learning Implementation\n",
        "\n",
        "This section implements the SlateQ class which handles slate-based Q-learning for recommendation selection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMKN-PW18WqU"
      },
      "outputs": [],
      "source": [
        "class SlateQ():\n",
        "    def __init__(self, user_features, doc_features, num_of_candidates, slate_size, batch_size):\n",
        "        self.user_features = user_features\n",
        "        self.num_of_candidates = num_of_candidates\n",
        "        self.doc_features = doc_features\n",
        "        self.slate_size = slate_size\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def score_documents_torch(self, user_obs, doc_obs, no_click_mass=1.0, is_mnl=True, min_normalizer=-1.0):\n",
        "        user_obs = user_obs.view(-1)\n",
        "        doc_obs = doc_obs.view(-1)\n",
        "        assert user_obs.shape == torch.Size([self.user_features])\n",
        "        assert doc_obs.shape == torch.Size([self.num_of_candidates])\n",
        "\n",
        "        scores = torch.sum(input=torch.mul(doc_obs.view(-1, 1),\n",
        "                                          user_obs.view(1, -1)).view(self.num_of_candidates,\n",
        "                                                                     self.user_features),\n",
        "                          dim=1)\n",
        "\n",
        "        all_scores = torch.cat([scores, torch.tensor([no_click_mass], device=self.device)], dim=0)\n",
        "\n",
        "        if is_mnl:\n",
        "            all_scores = torch.nn.functional.softmax(all_scores, dim=0)\n",
        "        else:\n",
        "            all_scores = all_scores - min_normalizer\n",
        "\n",
        "        assert all_scores.shape == torch.Size([self.num_of_candidates + 1])\n",
        "        return all_scores[:-1], all_scores[-1]\n",
        "\n",
        "    def compute_probs_torch(self, slate, scores_torch, score_no_click_torch):\n",
        "        slate = slate.squeeze()\n",
        "        scores_torch = scores_torch.squeeze()\n",
        "        assert slate.shape == torch.Size([self.slate_size])\n",
        "        assert scores_torch.shape == torch.Size([self.num_of_candidates])\n",
        "\n",
        "        all_scores = torch.cat([\n",
        "            torch.gather(scores_torch, 0, slate).view(-1),\n",
        "            score_no_click_torch.view(-1)\n",
        "        ], dim=0)\n",
        "\n",
        "        all_probs = all_scores / torch.sum(all_scores)\n",
        "        assert all_probs.shape == torch.Size([self.slate_size + 1])\n",
        "        return all_probs[:-1]\n",
        "\n",
        "    def select_slate_greedy(self, s_no_click, s, q):\n",
        "        s = s.view(-1)\n",
        "        q = q.view(-1)\n",
        "        assert s.shape == torch.Size([self.num_of_candidates])\n",
        "        assert q.shape == torch.Size([self.num_of_candidates])\n",
        "\n",
        "        def argmax(v, mask_inner):\n",
        "            return torch.argmax((v - torch.min(v) + 1) * mask_inner, dim=0)\n",
        "\n",
        "        numerator = torch.tensor(0., device=self.device)\n",
        "        denominator = torch.tensor(0., device=self.device) + s_no_click\n",
        "        mask_inner = torch.ones(q.size(0), device=self.device)\n",
        "\n",
        "        def set_element(v, i, x):\n",
        "            mask_inner = torch.nn.functional.one_hot(i, v.shape[0])\n",
        "            v_new = torch.ones_like(v) * x\n",
        "            return torch.where(mask_inner == 1, v_new, v)\n",
        "\n",
        "        for _ in range(self.slate_size):\n",
        "            k = argmax((numerator + s * q) / (denominator + s), mask_inner)\n",
        "            mask_inner = set_element(mask_inner, k, 0)\n",
        "            numerator = numerator + torch.gather(s * q, 0, k)\n",
        "            denominator = denominator + torch.gather(s, 0, k)\n",
        "\n",
        "        output_slate = torch.where(mask_inner == 0)[0].squeeze()\n",
        "        assert output_slate.shape == torch.Size([self.slate_size])\n",
        "        return output_slate\n",
        "\n",
        "    def compute_target_greedy_q(self, reward, gamma, next_q_values, next_states, terminals):\n",
        "        assert reward.shape == torch.Size([self.batch_size])\n",
        "        assert next_q_values.shape == torch.Size([self.batch_size, self.num_of_candidates])\n",
        "\n",
        "        next_user_obs = next_states[:, :self.user_features]\n",
        "        next_doc_obs = next_states[:, self.user_features:(self.user_features + self.num_of_candidates * self.doc_features)]\n",
        "\n",
        "        assert next_user_obs.shape == torch.Size([self.batch_size, self.user_features])\n",
        "        assert next_doc_obs.shape == torch.Size([self.batch_size, self.num_of_candidates])\n",
        "\n",
        "        next_greedy_q_list = []\n",
        "        for i in range(self.batch_size):\n",
        "            s, s_no_click = self.score_documents_torch(next_user_obs[i], next_doc_obs[i])\n",
        "            q = next_q_values[i]\n",
        "            slate = self.select_slate_greedy(s_no_click, s, q)\n",
        "            p_selected = self.compute_probs_torch(slate, s, s_no_click)\n",
        "            q_selected = torch.gather(q, 0, slate)\n",
        "            next_greedy_q_list.append(\n",
        "                torch.sum(input=p_selected * q_selected)\n",
        "            )\n",
        "\n",
        "        next_greedy_q_values = torch.stack(next_greedy_q_list)\n",
        "        target_q_values = reward + gamma * next_greedy_q_values * (1. - terminals.float())\n",
        "\n",
        "        assert target_q_values.shape == torch.Size([self.batch_size])\n",
        "        return target_q_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeflCtsl8WqV"
      },
      "source": [
        "## Recommendation Environment Implementation\n",
        "\n",
        "This section implements the recommendation environment classes including document and user state handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuIdwAtp8WqV"
      },
      "outputs": [],
      "source": [
        "class LTSDocument(document.AbstractDocument):\n",
        "    def __init__(self, doc_id, kaleness):\n",
        "        self.kaleness = kaleness\n",
        "        super(LTSDocument, self).__init__(doc_id)\n",
        "\n",
        "    def create_observation(self):\n",
        "        return np.array([self.kaleness])\n",
        "\n",
        "    @staticmethod\n",
        "    def observation_space():\n",
        "        return spaces.Box(shape=(1,), dtype=np.float32, low=0.0, high=1.0)\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Document {} with kaleness {}.\".format(self._doc_id, self.kaleness)\n",
        "\n",
        "class LTSDocumentSampler(document.AbstractDocumentSampler):\n",
        "    def __init__(self, seed, doc_ctor=LTSDocument, **kwargs):\n",
        "        super(LTSDocumentSampler, self).__init__(doc_ctor, **kwargs)\n",
        "        self._doc_count = 0\n",
        "        self.seed = seed\n",
        "        self._rng = np.random.RandomState(self.seed)\n",
        "\n",
        "    def sample_document(self):\n",
        "        doc_features = {}\n",
        "        doc_features['doc_id'] = self._doc_count\n",
        "        doc_features['kaleness'] = self._rng.random_sample()\n",
        "        self._doc_count += 1\n",
        "        return self._doc_ctor(**doc_features)\n",
        "\n",
        "class LTSUserState(user.AbstractUserState):\n",
        "    def __init__(self, memory_discount, sensitivity, innovation_stddev,\n",
        "                 choc_mean, choc_stddev, kale_mean, kale_stddev,\n",
        "                 net_kaleness_exposure, time_budget, observation_noise_stddev=0.1):\n",
        "        # Transition model parameters\n",
        "        self.memory_discount = memory_discount\n",
        "        self.sensitivity = sensitivity\n",
        "        self.innovation_stddev = innovation_stddev\n",
        "\n",
        "        # Engagement parameters\n",
        "        self.choc_mean = choc_mean\n",
        "        self.choc_stddev = choc_stddev\n",
        "        self.kale_mean = kale_mean\n",
        "        self.kale_stddev = kale_stddev\n",
        "\n",
        "        # State variables\n",
        "        self.net_kaleness_exposure = net_kaleness_exposure\n",
        "        self.satisfaction = 1 / (1 + np.exp(-sensitivity * net_kaleness_exposure))\n",
        "        self.time_budget = time_budget\n",
        "        self._observation_noise = observation_noise_stddev\n",
        "\n",
        "    def create_observation(self):\n",
        "        \"\"\"User's state is not observable.\"\"\"\n",
        "        clip_low, clip_high = (-1.0 / (1.0 * self._observation_noise),\n",
        "                              1.0 / (1.0 * self._observation_noise))\n",
        "        noise = stats.truncnorm(clip_low, clip_high, loc=0.0,\n",
        "                               scale=self._observation_noise).rvs()\n",
        "        noisy_sat = self.satisfaction + noise\n",
        "        return np.array([noisy_sat, ])\n",
        "\n",
        "    @staticmethod\n",
        "    def observation_space():\n",
        "        return spaces.Box(shape=(1,), dtype=np.float32, low=-2.0, high=2.0)\n",
        "\n",
        "    def score_document(self, doc_obs):\n",
        "        return 1 - doc_obs\n",
        "\n",
        "class LTSStaticUserSampler(user.AbstractUserSampler):\n",
        "    _state_parameters = None\n",
        "\n",
        "    def __init__(self,\n",
        "                 user_ctor=LTSUserState,\n",
        "                 memory_discount=0.9,\n",
        "                 sensitivity=0.01,\n",
        "                 innovation_stddev=0.05,\n",
        "                 choc_mean=5.0,\n",
        "                 choc_stddev=1.0,\n",
        "                 kale_mean=4.0,\n",
        "                 kale_stddev=1.0,\n",
        "                 time_budget=122,\n",
        "                 **kwargs):\n",
        "        self._state_parameters = {\n",
        "            'memory_discount': memory_discount,\n",
        "            'sensitivity': sensitivity,\n",
        "            'innovation_stddev': innovation_stddev,\n",
        "            'choc_mean': choc_mean,\n",
        "            'choc_stddev': choc_stddev,\n",
        "            'kale_mean': kale_mean,\n",
        "            'kale_stddev': kale_stddev,\n",
        "            'time_budget': time_budget\n",
        "        }\n",
        "        super(LTSStaticUserSampler, self).__init__(user_ctor, **kwargs)\n",
        "\n",
        "    def sample_user(self):\n",
        "        starting_nke = ((self._rng.random_sample() - .5) *\n",
        "                        (1 / (1.0 - self._state_parameters['memory_discount'])))\n",
        "        self._state_parameters['net_kaleness_exposure'] = starting_nke\n",
        "        return self._user_ctor(**self._state_parameters)\n",
        "\n",
        "class LTSResponse(user.AbstractResponse):\n",
        "    MAX_ENGAGEMENT_MAGNITUDE = 100.0\n",
        "\n",
        "    def __init__(self, clicked=False, engagement=0.0):\n",
        "        self.clicked = clicked\n",
        "        self.engagement = engagement\n",
        "\n",
        "    def create_observation(self):\n",
        "        return {'click': int(self.clicked), 'engagement': np.array(self.engagement)}\n",
        "\n",
        "    @classmethod\n",
        "    def response_space(cls):\n",
        "        return spaces.Dict({\n",
        "            'click': spaces.Discrete(2),\n",
        "            'engagement': spaces.Box(\n",
        "                low=0.0,\n",
        "                high=cls.MAX_ENGAGEMENT_MAGNITUDE,\n",
        "                shape=tuple(),\n",
        "                dtype=np.float32)\n",
        "        })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rpO5keQ8WqV"
      },
      "source": [
        "## RecSim Environment and User Model Implementation\n",
        "\n",
        "This section implements the main recommendation environment and user model classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDu2ugKz8WqW"
      },
      "outputs": [],
      "source": [
        "def user_init(self, slate_size, seed=0):\n",
        "    super(LTSUserModel, self).__init__(LTSResponse,\n",
        "                                       LTSStaticUserSampler(LTSUserState, seed=seed),\n",
        "                                       slate_size)\n",
        "    self.choice_model = MultinomialLogitChoiceModel({})\n",
        "\n",
        "def simulate_response(self, slate_documents):\n",
        "    # List of empty responses\n",
        "    responses = [self._response_model_ctor() for _ in slate_documents]\n",
        "\n",
        "    # Get click from choice model\n",
        "    self.choice_model.score_documents(\n",
        "        self._user_state, [doc.create_observation() for doc in slate_documents]\n",
        "    )\n",
        "    scores = self.choice_model.scores\n",
        "    selected_index = self.choice_model.choose_item()\n",
        "\n",
        "    # Populate clicked item\n",
        "    self._generate_response(slate_documents[selected_index],\n",
        "                           responses[selected_index])\n",
        "    return responses\n",
        "\n",
        "def generate_response(self, doc, response):\n",
        "    response.clicked = True\n",
        "    # linear interpolation between choc and kale\n",
        "    engagement_loc = (doc.kaleness * self._user_state.choc_mean\n",
        "                     + (1 - doc.kaleness) * self._user_state.kale_mean)\n",
        "    engagement_loc *= self._user_state.satisfaction\n",
        "    engagement_scale = (doc.kaleness * self._user_state.choc_stddev\n",
        "                       + ((1 - doc.kaleness)\n",
        "                          * self._user_state.kale_stddev))\n",
        "    log_engagement = np.random.normal(loc=engagement_loc,\n",
        "                                     scale=engagement_scale)\n",
        "    response.engagement = np.exp(log_engagement)\n",
        "\n",
        "def update_state(self, slate_documents, responses):\n",
        "    for doc, response in zip(slate_documents, responses):\n",
        "        if response.clicked:\n",
        "            innovation = np.random.normal(scale=self._user_state.innovation_stddev)\n",
        "            net_kaleness_exposure = (self._user_state.memory_discount\n",
        "                                    * self._user_state.net_kaleness_exposure\n",
        "                                    - 2.0 * (doc.kaleness - 0.5)\n",
        "                                    + innovation\n",
        "                                    )\n",
        "            self._user_state.net_kaleness_exposure = net_kaleness_exposure\n",
        "            satisfaction = 1 / (1.0 + np.exp(-self._user_state.sensitivity\n",
        "                                            * net_kaleness_exposure\n",
        "                                            ))\n",
        "            self._user_state.satisfaction = satisfaction\n",
        "            self._user_state.time_budget -= 1\n",
        "            return\n",
        "\n",
        "def is_terminal(self):\n",
        "    \"\"\"Returns a boolean indicating if the session is over.\"\"\"\n",
        "    return self._user_state.time_budget <= 0\n",
        "\n",
        "def clicked_engagement_reward(responses):\n",
        "    reward = 0.0\n",
        "    for response in responses:\n",
        "        if response.clicked:\n",
        "            reward += response.engagement\n",
        "    return reward\n",
        "\n",
        "LTSUserModel = type(\"LTSUserModel\", (user.AbstractUserModel,),\n",
        "                    {\"__init__\": user_init,\n",
        "                     \"is_terminal\": is_terminal,\n",
        "                     \"update_state\": update_state,\n",
        "                     \"simulate_response\": simulate_response,\n",
        "                     \"_generate_response\": generate_response})\n",
        "\n",
        "class RecsimEnv():\n",
        "    def __init__(self, num_candidates, slate_size, resample_documents, env_seed_0, env_seed_1):\n",
        "        assert num_candidates >= slate_size\n",
        "        self.num_candidates = num_candidates\n",
        "        self.slate_size = slate_size\n",
        "        self.resample_documents = resample_documents\n",
        "\n",
        "        # Document models\n",
        "        self.doc_model_1 = LTSDocumentSampler(env_seed_0)\n",
        "        self.doc_model_2 = LTSDocumentSampler(env_seed_1)\n",
        "\n",
        "        # User model\n",
        "        self.user_model = LTSUserModel(slate_size)\n",
        "\n",
        "        # Environments\n",
        "        self.env_0 = environment.Environment(\n",
        "            self.user_model,\n",
        "            self.doc_model_1,\n",
        "            num_candidates,\n",
        "            slate_size,\n",
        "            resample_documents,\n",
        "        )\n",
        "\n",
        "        self.env_1 = environment.Environment(\n",
        "            self.user_model,\n",
        "            self.doc_model_2,\n",
        "            num_candidates,\n",
        "            slate_size,\n",
        "            resample_documents)\n",
        "\n",
        "        self.lts_gym_env_0 = recsim_gym.RecSimGymEnv(self.env_0, clicked_engagement_reward)\n",
        "        self.lts_gym_env_1 = recsim_gym.RecSimGymEnv(self.env_1, clicked_engagement_reward)\n",
        "\n",
        "    def env_ini(self, id):\n",
        "        if id == 0:\n",
        "            output = self.lts_gym_env_0.reset()\n",
        "        elif id == 1:\n",
        "            output = self.lts_gym_env_1.reset()\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid id {id}.\")\n",
        "\n",
        "        user, doc, response = output.values()\n",
        "        doc_id = np.array(list(doc.keys())).astype(int)\n",
        "        doc_fea = np.array(list(doc.values()))\n",
        "        click = np.zeros([self.slate_size], dtype=int)\n",
        "        engagement = np.zeros([self.slate_size])\n",
        "        reward = np.array(0.)\n",
        "        done = False\n",
        "\n",
        "        return user.astype(np.float32), doc_id.astype(np.float32), doc_fea.astype(np.float32), \\\n",
        "               click.astype(np.float32), engagement.astype(np.float32), reward.astype(np.float32), done\n",
        "\n",
        "    def env_step(self, slate, id):\n",
        "        if id == 0:\n",
        "            output = self.lts_gym_env_0.step(slate)\n",
        "        elif id == 1:\n",
        "            output = self.lts_gym_env_1.step(slate)\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid id {id}.\")\n",
        "\n",
        "        user, doc, response = output[0].values()\n",
        "        reward = np.array(output[1])\n",
        "        doc_id = np.array(list(doc.keys())).astype(int)\n",
        "        doc_fea = np.array(list(doc.values()))\n",
        "        click = np.array(list(item['click'] for item in response))\n",
        "        engagement = np.array(list(item['engagement'] for item in response))\n",
        "        done = output[2]\n",
        "\n",
        "        return user.astype(np.float32), doc_id.astype(np.float32), doc_fea.astype(np.float32), \\\n",
        "               click.astype(np.float32), engagement.astype(np.float32), reward.astype(np.float32), done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C79U_lMq8WqW"
      },
      "source": [
        "## Logger Implementation\n",
        "\n",
        "This section implements the logging functionality to track training progress and metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9xrtapw8WqW"
      },
      "outputs": [],
      "source": [
        "class Logger:\n",
        "    def __init__(self, save_dir):\n",
        "        self.save_log = save_dir / \"log\"\n",
        "        with open(self.save_log, \"w\") as f:\n",
        "            f.write(\n",
        "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanLength':>15}\"\n",
        "                f\"{'MeanReward_alpha':>15}{'MeanLoss_alpha':>15}{'MeanQValue_alpha':>15}\"\n",
        "                f\"{'MeanReward_beta':>15}{'MeanLoss_beta':>15}{'MeanQValue_beta':>15}\"\n",
        "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
        "            )\n",
        "\n",
        "        # Save paths for plots\n",
        "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
        "        self.ep_rewards_alpha_plot = save_dir / \"reward_alpha_plot.jpg\"\n",
        "        self.ep_avg_losses_alpha_plot = save_dir / \"loss_alpha_plot.jpg\"\n",
        "        self.ep_avg_qs_alpha_plot = save_dir / \"q_alpha_plot.jpg\"\n",
        "        self.ep_rewards_beta_plot = save_dir / \"reward_beta_plot.jpg\"\n",
        "        self.ep_avg_losses_beta_plot = save_dir / \"loss_beta_plot.jpg\"\n",
        "        self.ep_avg_qs_beta_plot = save_dir / \"q_beta_plot.jpg\"\n",
        "\n",
        "        # History metrics\n",
        "        self.ep_lengths = []\n",
        "        self.ep_rewards_alpha = []\n",
        "        self.ep_avg_losses_alpha = []\n",
        "        self.ep_avg_qs_alpha = []\n",
        "        self.ep_rewards_beta = []\n",
        "        self.ep_avg_losses_beta = []\n",
        "        self.ep_avg_qs_beta = []\n",
        "\n",
        "        # Moving averages\n",
        "        self.moving_avg_ep_lengths = []\n",
        "        self.moving_avg_ep_rewards_alpha = []\n",
        "        self.moving_avg_ep_avg_losses_alpha = []\n",
        "        self.moving_avg_ep_avg_qs_alpha = []\n",
        "        self.moving_avg_ep_rewards_beta = []\n",
        "        self.moving_avg_ep_avg_losses_beta = []\n",
        "        self.moving_avg_ep_avg_qs_beta = []\n",
        "\n",
        "        # Current episode metric\n",
        "        self.init_episode()\n",
        "\n",
        "        # Timing\n",
        "        self.record_time = time.time()\n",
        "\n",
        "    def log_step(self, reward_alpha, loss_alpha, q_alpha, reward_beta, loss_beta, q_beta):\n",
        "        self.curr_ep_reward_alpha += reward_alpha\n",
        "        self.curr_ep_reward_beta += reward_beta\n",
        "        self.curr_ep_length += 1\n",
        "\n",
        "        if loss_alpha:\n",
        "            self.curr_ep_loss_alpha += loss_alpha\n",
        "            self.curr_ep_q_alpha += q_alpha\n",
        "            self.curr_ep_loss_length_alpha += 1\n",
        "\n",
        "        if loss_beta:\n",
        "            self.curr_ep_loss_beta += loss_beta\n",
        "            self.curr_ep_q_beta += q_beta\n",
        "            self.curr_ep_loss_length_beta += 1\n",
        "\n",
        "    def log_episode(self):\n",
        "        \"Mark end of episode\"\n",
        "        self.ep_rewards_alpha.append(self.curr_ep_reward_alpha)\n",
        "        self.ep_rewards_beta.append(self.curr_ep_reward_beta)\n",
        "        self.ep_lengths.append(self.curr_ep_length)\n",
        "\n",
        "        if self.curr_ep_loss_length_alpha == 0:\n",
        "            ep_avg_loss_alpha = 0\n",
        "            ep_avg_q_alpha = 0\n",
        "        else:\n",
        "            ep_avg_loss_alpha = np.round(self.curr_ep_loss_alpha / self.curr_ep_loss_length_alpha, 5)\n",
        "            ep_avg_q_alpha = np.round(self.curr_ep_q_alpha / self.curr_ep_loss_length_alpha, 5)\n",
        "\n",
        "        if self.curr_ep_loss_length_beta == 0:\n",
        "            ep_avg_loss_beta = 0\n",
        "            ep_avg_q_beta = 0\n",
        "        else:\n",
        "            ep_avg_loss_beta = np.round(self.curr_ep_loss_beta / self.curr_ep_loss_length_beta, 5)\n",
        "            ep_avg_q_beta = np.round(self.curr_ep_q_beta / self.curr_ep_loss_length_beta, 5)\n",
        "\n",
        "        self.ep_avg_losses_alpha.append(ep_avg_loss_alpha)\n",
        "        self.ep_avg_qs_alpha.append(ep_avg_q_alpha)\n",
        "        self.ep_avg_losses_beta.append(ep_avg_loss_beta)\n",
        "        self.ep_avg_qs_beta.append(ep_avg_q_beta)\n",
        "\n",
        "        self.init_episode()\n",
        "\n",
        "    def init_episode(self):\n",
        "        self.curr_ep_length = 0\n",
        "        self.curr_ep_reward_alpha = 0.0\n",
        "        self.curr_ep_loss_alpha = 0.0\n",
        "        self.curr_ep_q_alpha = 0.0\n",
        "        self.curr_ep_loss_length_alpha = 0\n",
        "        self.curr_ep_reward_beta = 0.0\n",
        "        self.curr_ep_loss_beta = 0.0\n",
        "        self.curr_ep_q_beta = 0.0\n",
        "        self.curr_ep_loss_length_beta = 0\n",
        "\n",
        "    def record(self, episode, epsilon, step):\n",
        "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
        "        mean_ep_reward_alpha = np.round(np.mean(self.ep_rewards_alpha[-100:]), 3)\n",
        "        mean_ep_loss_alpha = np.round(np.mean(self.ep_avg_losses_alpha[-100:]), 3)\n",
        "        mean_ep_q_alpha = np.round(np.mean(self.ep_avg_qs_alpha[-100:]), 3)\n",
        "        mean_ep_reward_beta = np.round(np.mean(self.ep_rewards_beta[-100:]), 3)\n",
        "        mean_ep_loss_beta = np.round(np.mean(self.ep_avg_losses_beta[-100:]), 3)\n",
        "        mean_ep_q_beta = np.round(np.mean(self.ep_avg_qs_beta[-100:]), 3)\n",
        "\n",
        "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
        "        self.moving_avg_ep_rewards_alpha.append(mean_ep_reward_alpha)\n",
        "        self.moving_avg_ep_avg_losses_alpha.append(mean_ep_loss_alpha)\n",
        "        self.moving_avg_ep_avg_qs_alpha.append(mean_ep_q_alpha)\n",
        "        self.moving_avg_ep_rewards_beta.append(mean_ep_reward_beta)\n",
        "        self.moving_avg_ep_avg_losses_beta.append(mean_ep_loss_beta)\n",
        "        self.moving_avg_ep_avg_qs_beta.append(mean_ep_q_beta)\n",
        "\n",
        "        last_record_time = self.record_time\n",
        "        self.record_time = time.time()\n",
        "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
        "\n",
        "        print(\n",
        "            f\"Episode {episode} - \"\n",
        "            f\"Step {step} - \"\n",
        "            f\"Epsilon {epsilon} - \"\n",
        "            f\"Mean Length {mean_ep_length} - \"\n",
        "            f\"Mean Reward {mean_ep_reward_alpha} - \"\n",
        "            f\"Mean Loss {mean_ep_loss_alpha} - \"\n",
        "            f\"Mean Q Value {mean_ep_q_alpha} - \"\n",
        "            f\"Mean Reward {mean_ep_reward_beta} - \"\n",
        "            f\"Mean Loss {mean_ep_loss_beta} - \"\n",
        "            f\"Mean Q Value {mean_ep_q_beta} - \"\n",
        "            f\"Time Delta {time_since_last_record} - \"\n",
        "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
        "        )\n",
        "\n",
        "        with open(self.save_log, \"a\") as f:\n",
        "            f.write(\n",
        "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
        "                f\"{mean_ep_length:15.3f}\"\n",
        "                f\"{mean_ep_reward_alpha:15.3f}{mean_ep_loss_alpha:15.3f}{mean_ep_q_alpha:15.3f}\"\n",
        "                f\"{mean_ep_reward_beta:15.3f}{mean_ep_loss_beta:15.3f}{mean_ep_q_beta:15.3f}\"\n",
        "                f\"{time_since_last_record:15.3f}\"\n",
        "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
        "            )\n",
        "\n",
        "        # Plot metrics\n",
        "        for metric in [\"ep_lengths\", \"ep_rewards_alpha\", \"ep_avg_losses_alpha\", \"ep_avg_qs_alpha\",\n",
        "                      \"ep_rewards_beta\", \"ep_avg_losses_beta\", \"ep_avg_qs_beta\"]:\n",
        "            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n",
        "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
        "            plt.clf()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJuxEfXV8WqY"
      },
      "source": [
        "## Agents Implementation\n",
        "\n",
        "This section implements the core agent classes for the federated recommendation system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMN-E-xD8WqY"
      },
      "outputs": [],
      "source": [
        "class AgentAlpha(SlateQ):\n",
        "    def __init__(self, user_features, doc_features, num_of_candidates, slate_size, batch_size, num_contex,\n",
        "                 capacity=2000):\n",
        "        self.user_features = user_features\n",
        "        self.doc_features = doc_features\n",
        "        self.num_of_candidates = num_of_candidates\n",
        "        self.slate_size = slate_size\n",
        "        self.batch_size = batch_size\n",
        "        self.num_contex = num_contex\n",
        "        self.state_dim = user_features + (\n",
        "            doc_features * num_of_candidates + num_of_candidates) + num_contex * slate_size\n",
        "        self.action_dim = slate_size\n",
        "\n",
        "        self.response = deque(maxlen=num_contex)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.net = QNet(self.state_dim, self.num_of_candidates).to(self.device)\n",
        "        self.replay = ReplayMemory(capacity, (self.state_dim,), (self.action_dim,))\n",
        "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.01)\n",
        "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
        "        self.gamma = 0.9\n",
        "\n",
        "    def compute_q_local_ini(self, env):\n",
        "        for _ in range(self.num_contex):\n",
        "            self.response.append(torch.zeros([self.slate_size], device=self.device))\n",
        "\n",
        "        user, doc_id, doc_fea, click, engagement, reward, done = env.env_ini(0)\n",
        "        state = torch.cat([\n",
        "            torch.tensor(user, device=self.device).view(-1),\n",
        "            torch.tensor(doc_fea, device=self.device).view(-1),\n",
        "            torch.tensor(doc_id, device=self.device).to(torch.float).view(-1)\n",
        "        ])\n",
        "\n",
        "        for responses in self.response:\n",
        "            state = torch.cat([state, responses.squeeze()])\n",
        "\n",
        "        assert state.shape == torch.Size([self.state_dim])\n",
        "        self.state = state\n",
        "        return self.net(state, \"online\")\n",
        "\n",
        "    def compute_q_local(self):\n",
        "        return self.net.forward(self.state.view(1, -1), \"online\")\n",
        "\n",
        "    def recommend(self, q_fed_alpha, env):\n",
        "        user_obs = self.state[:self.user_features]\n",
        "        doc_obs = self.state[self.user_features:(self.user_features + self.num_of_candidates * self.doc_features)]\n",
        "        s, s_no_click = super().score_documents_torch(user_obs, doc_obs)\n",
        "        slate = super().select_slate_greedy(s_no_click, s, q_fed_alpha)\n",
        "\n",
        "        user, doc_id, doc_fea, click, engagement, reward, done = env.env_step(slate.cpu().numpy().tolist(), 0)\n",
        "        self.response.append(torch.tensor(engagement, device=self.device))\n",
        "\n",
        "        next_state = torch.cat([\n",
        "            torch.tensor(user, device=self.device).view(-1),\n",
        "            torch.tensor(doc_fea, device=self.device).view(-1),\n",
        "            torch.tensor(doc_id, device=self.device).to(torch.float).view(-1)\n",
        "        ])\n",
        "\n",
        "        for responses in self.response:\n",
        "            next_state = torch.cat([next_state, responses.view(-1)])\n",
        "\n",
        "        self.replay.push(self.state.view(-1), slate.view(-1),\n",
        "                        torch.tensor(reward, device=self.device).view(-1),\n",
        "                        torch.tensor(click, device=self.device),\n",
        "                        next_state.squeeze(),\n",
        "                        torch.tensor(done, device=self.device).view(-1))\n",
        "\n",
        "        self.state = next_state\n",
        "        return done, reward\n",
        "\n",
        "    def recommend_random(self, env):\n",
        "        nums = list(range(self.num_of_candidates))\n",
        "        random.shuffle(nums)\n",
        "        slate = nums[:self.slate_size]\n",
        "\n",
        "        user, doc_id, doc_fea, click, engagement, reward, done = env.env_step(slate, 0)\n",
        "        self.response.append(torch.tensor(engagement, device=self.device))\n",
        "\n",
        "        next_state = torch.cat([\n",
        "            torch.tensor(user, device=self.device).view(-1),\n",
        "            torch.tensor(doc_fea, device=self.device).view(-1),\n",
        "            torch.tensor(doc_id, device=self.device).to(torch.float).view(-1)\n",
        "        ])\n",
        "\n",
        "        for responses in self.response:\n",
        "            next_state = torch.cat([next_state, responses.squeeze()])\n",
        "\n",
        "        self.replay.push(self.state.squeeze(),\n",
        "                        torch.tensor(slate, device=self.device),\n",
        "                        torch.tensor(reward, device=self.device).view(1),\n",
        "                        torch.tensor(click, device=self.device),\n",
        "                        next_state.squeeze(),\n",
        "                        torch.tensor(done, device=self.device).view(1))\n",
        "\n",
        "        self.state = next_state\n",
        "        return done, reward\n",
        "\n",
        "    def compute_q_local_batch(self, ids):\n",
        "        self.batch_states, self.batch_actions, self.batch_rewards, self.batch_clicks, \\\n",
        "        self.batch_next_states, self.batch_terminals = self.replay.recall(ids)\n",
        "        return self.net.forward(self.batch_states, \"online\"), self.net.forward(self.batch_next_states, \"target\")\n",
        "\n",
        "    def update_q_net(self, q, q_next, agent_fed):\n",
        "        assert q.shape == torch.Size([self.batch_size, self.num_of_candidates])\n",
        "        assert q_next.shape == torch.Size([self.batch_size, self.num_of_candidates])\n",
        "\n",
        "        doc_id = self.batch_states[:, (self.user_features + self.num_of_candidates * self.doc_features):\n",
        "                                  (self.user_features + self.num_of_candidates * self.doc_features + self.num_of_candidates)]\n",
        "\n",
        "        assert doc_id.shape == torch.Size([self.batch_size, self.num_of_candidates])\n",
        "\n",
        "        selected_item = self.batch_actions * self.batch_clicks\n",
        "        selected_item = selected_item.type(torch.int)\n",
        "        assert selected_item.shape == torch.Size([self.batch_size, self.slate_size])\n",
        "        selected_item = torch.sum(selected_item, dim=1, keepdim=True)\n",
        "\n",
        "        q = torch.gather(q, 1, selected_item)\n",
        "        q_next = super().compute_target_greedy_q(self.batch_rewards, self.gamma, q_next,\n",
        "                                                self.batch_next_states, self.batch_terminals)\n",
        "\n",
        "        loss = self.loss_fn(q.view(self.batch_size,1), q_next.view(self.batch_size,1))\n",
        "        agent_fed.optimizer.zero_grad()\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward(retain_graph=True)\n",
        "        agent_fed.optimizer.step()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss, q_next\n",
        "\n",
        "class AgentBeta(SlateQ):\n",
        "    def __init__(self, user_features, doc_features, num_of_candidates, slate_size, batch_size, capacity=2000):\n",
        "        self.user_features = user_features\n",
        "        self.doc_features = doc_features\n",
        "        self.num_of_candidates = num_of_candidates\n",
        "        self.slate_size = slate_size\n",
        "        self.batch_size = batch_size\n",
        "        self.state_dim = user_features + (doc_features * num_of_candidates + num_of_candidates)\n",
        "        self.action_dim = slate_size\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.net = QNet(self.state_dim, self.num_of_candidates).to(self.device)\n",
        "        self.replay = ReplayMemory(capacity, (self.state_dim,), (self.action_dim,))\n",
        "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.01)\n",
        "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
        "        self.gamma = 0.9\n",
        "\n",
        "    def compute_q_local_ini(self, env):\n",
        "        user, doc_id, doc_fea, click, engagement, reward, done = env.env_ini(1)\n",
        "        state = torch.cat([\n",
        "            torch.tensor(user, device=self.device).view(-1),\n",
        "            torch.tensor(doc_fea, device=self.device).view(-1),\n",
        "            torch.tensor(doc_id, device=self.device).to(torch.float).view(-1)\n",
        "        ])\n",
        "\n",
        "        assert state.shape == torch.Size([self.state_dim])\n",
        "        self.state = state\n",
        "        return self.net(state, \"online\")\n",
        "\n",
        "    def compute_q_local(self):\n",
        "        return self.net.forward(self.state.view(1, -1), \"online\")\n",
        "\n",
        "    def recommend(self, q_fed_beta, env):\n",
        "        user_obs = self.state[:self.user_features]\n",
        "        doc_obs = self.state[self.user_features:(self.user_features + self.num_of_candidates * self.doc_features)]\n",
        "        s, s_no_click = super().score_documents_torch(user_obs, doc_obs)\n",
        "        slate = super().select_slate_greedy(s_no_click, s, q_fed_beta)\n",
        "\n",
        "        user, doc_id, doc_fea, click, engagement, reward, done = env.env_step(slate.cpu().numpy().tolist(), 1)\n",
        "        next_state = torch.cat([\n",
        "            torch.tensor(user, device=self.device).view(-1),\n",
        "            torch.tensor(doc_fea, device=self.device).view(-1),\n",
        "            torch.tensor(doc_id, device=self.device).to(torch.float).view(-1)\n",
        "        ])\n",
        "\n",
        "        self.replay.push(self.state.view(-1), slate.view(-1),\n",
        "                        torch.tensor(reward, device=self.device).view(-1),\n",
        "                        torch.tensor(click, device=self.device),\n",
        "                        next_state.squeeze(),\n",
        "                        torch.tensor(done, device=self.device).view(-1))\n",
        "\n",
        "        self.state = next_state\n",
        "        return done, reward\n",
        "\n",
        "    def recommend_random(self, env):\n",
        "        nums = list(range(self.num_of_candidates))\n",
        "        random.shuffle(nums)\n",
        "        slate = nums[:self.slate_size]\n",
        "\n",
        "        user, doc_id, doc_fea, click, engagement, reward, done = env.env_step(slate, 1)\n",
        "        next_state = torch.cat([\n",
        "            torch.tensor(user, device=self.device).view(-1),\n",
        "            torch.tensor(doc_fea, device=self.device).view(-1),\n",
        "            torch.tensor(doc_id, device=self.device).to(torch.float).view(-1)\n",
        "        ])\n",
        "\n",
        "        self.replay.push(self.state.squeeze(),\n",
        "                        torch.tensor(slate, device=self.device),\n",
        "                        torch.tensor(reward, device=self.device).view(1),\n",
        "                        torch.tensor(click, device=self.device),\n",
        "                        next_state.squeeze(),\n",
        "                        torch.tensor(done, device=self.device).view(1))\n",
        "\n",
        "        self.state = next_state\n",
        "        return done, reward\n",
        "\n",
        "    def compute_q_local_batch(self, ids):\n",
        "        self.batch_states, self.batch_actions, _, _, _, _ = self.replay.recall(ids)\n",
        "        return self.net.forward(self.batch_states, \"online\")\n",
        "\n",
        "    def update_q_net(self, q_online, q_target, agent_fed):\n",
        "        assert q_online.shape == torch.Size([self.batch_size, self.num_of_candidates])\n",
        "\n",
        "        q_target = q_target.view(self.batch_size, 1)\n",
        "        user_obs = self.batch_states[:, :self.user_features]\n",
        "        doc_obs = self.batch_states[:, self.user_features:(self.user_features + self.num_of_candidates * self.doc_features)]\n",
        "\n",
        "        assert user_obs.shape == torch.Size([self.batch_size, self.user_features])\n",
        "        assert doc_obs.shape == torch.Size([self.batch_size, self.num_of_candidates])\n",
        "\n",
        "        greedy_q_list = []\n",
        "        for i in range(self.batch_size):\n",
        "            s, s_no_click = super().score_documents_torch(user_obs[i], doc_obs[i])\n",
        "            q = q_online[i]\n",
        "            slate = super().select_slate_greedy(s_no_click, s, q)\n",
        "            p_selected = super().compute_probs_torch(slate, s, s_no_click)\n",
        "            q_selected = torch.gather(q, 0, slate)\n",
        "            greedy_q_list.append(torch.sum(input=p_selected * q_selected))\n",
        "\n",
        "        greedy_q_values = torch.stack(greedy_q_list)\n",
        "        greedy_q_values = greedy_q_values.view(self.batch_size, 1)\n",
        "\n",
        "        loss = self.loss_fn(greedy_q_values.view(self.batch_size,1), q_target.view(self.batch_size,1))\n",
        "        agent_fed.optimizer.zero_grad()\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        agent_fed.optimizer.step()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss\n",
        "\n",
        "class AgentFed():\n",
        "    def __init__(self, user_features, doc_features, num_of_candidates, slate_size, batch_size, capacity=2000):\n",
        "        self.user_features = user_features\n",
        "        self.doc_features = doc_features\n",
        "        self.num_of_candidates = num_of_candidates\n",
        "        self.slate_size = slate_size\n",
        "        self.batch_size = batch_size\n",
        "        self.capacity = capacity\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.exploration_rate = 1\n",
        "        self.exploration_rate_decay = 0.99995\n",
        "        self.exploration_rate_min = 0.\n",
        "        self.burnin = 5000  # min. experiences before training\n",
        "        self.learn_every = 3  # no. of experiences between updates to Q_online\n",
        "        self.sync_every = 500  # no. of experiences between Q_target & Q_online sync\n",
        "        self.curr_step = 0\n",
        "\n",
        "        self.net = MLPNet(num_of_candidates * 2, num_of_candidates).to(self.device)\n",
        "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.01)\n",
        "\n",
        "    def sync(self, agent_alpha, agent_beta):\n",
        "        self.net.target.load_state_dict(self.net.online.state_dict())\n",
        "        agent_alpha.net.target.load_state_dict(agent_alpha.net.online.state_dict())\n",
        "        agent_beta.net.target.load_state_dict(agent_beta.net.online.state_dict())\n",
        "\n",
        "    def act_ini(self, agent_alpha, agent_beta, env):\n",
        "        q_alpha = agent_alpha.compute_q_local_ini(env).view(1, -1)\n",
        "        q_beta = agent_beta.compute_q_local_ini(env).view(1, -1)\n",
        "        q_alpha_fed = self.net.forward(torch.cat([q_alpha, q_beta], dim=1), \"online\")\n",
        "        q_beta_fed = self.net.forward(torch.cat([q_beta, q_alpha], dim=1), \"online\")\n",
        "\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "            done_alpha, reward_alpha = agent_alpha.recommend_random(env)\n",
        "        else:\n",
        "            done_alpha, reward_alpha = agent_alpha.recommend(q_alpha_fed, env)\n",
        "\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "            done_beta, reward_beta = agent_beta.recommend_random(env)\n",
        "        else:\n",
        "            done_beta, reward_beta = agent_beta.recommend(q_beta_fed, env)\n",
        "\n",
        "        # decrease exploration_rate\n",
        "        self.exploration_rate *= self.exploration_rate_decay\n",
        "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
        "\n",
        "        # increment step\n",
        "        self.curr_step += 1\n",
        "        return done_alpha, reward_alpha, done_beta, reward_beta\n",
        "\n",
        "    def act(self, agent_alpha, agent_beta, env):\n",
        "        q_alpha = agent_alpha.compute_q_local().view(1, -1)\n",
        "        q_beta = agent_beta.compute_q_local().view(1, -1)\n",
        "        q_alpha_fed = self.net.forward(torch.cat([q_alpha, q_beta], dim=1), \"online\")\n",
        "        q_beta_fed = self.net.forward(torch.cat([q_beta, q_alpha], dim=1), \"online\")\n",
        "\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "            done_alpha, reward_alpha = agent_alpha.recommend_random(env)\n",
        "        else:\n",
        "            done_alpha, reward_alpha = agent_alpha.recommend(q_alpha_fed, env)\n",
        "\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "            done_beta, reward_beta = agent_beta.recommend_random(env)\n",
        "        else:\n",
        "            done_beta, reward_beta = agent_beta.recommend(q_beta_fed, env)\n",
        "\n",
        "        # decrease exploration_rate\n",
        "        self.exploration_rate *= self.exploration_rate_decay\n",
        "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
        "\n",
        "        if self.exploration_rate < 0.1:\n",
        "            self.exploration_rate = 0\n",
        "\n",
        "        # increment step\n",
        "        self.curr_step += 1\n",
        "        return done_alpha, reward_alpha, done_beta, reward_beta\n",
        "\n",
        "    def learn(self, agent_alpha, agent_beta):\n",
        "        if self.curr_step % self.sync_every == 0:\n",
        "            self.sync(agent_alpha, agent_beta)\n",
        "\n",
        "        if self.curr_step < self.burnin:\n",
        "            return None, None, None, None\n",
        "\n",
        "        if self.curr_step % self.learn_every != 0:\n",
        "            return None, None, None, None\n",
        "\n",
        "        nums = list(range(self.capacity))\n",
        "        random.shuffle(nums)\n",
        "        ids = nums[:self.batch_size]\n",
        "\n",
        "        batch_q_alpha_online, batch_q_alpha_target = agent_alpha.compute_q_local_batch(ids)\n",
        "        batch_q_beta_online = agent_beta.compute_q_local_batch(ids)\n",
        "\n",
        "        q_alpha_fed_online = self.net(torch.cat([batch_q_alpha_online, batch_q_beta_online], dim=1), \"online\")\n",
        "        q_alpha_fed_target = self.net(torch.cat([batch_q_alpha_target, batch_q_beta_online], dim=1), \"target\")\n",
        "\n",
        "        loss_alpha, q_alpha_target = agent_alpha.update_q_net(q_alpha_fed_online, q_alpha_fed_target, self)\n",
        "\n",
        "        batch_q_alpha_online_new, _ = agent_alpha.compute_q_local_batch(ids)\n",
        "        q_beta_fed_online = self.net(torch.cat([batch_q_beta_online, batch_q_alpha_online_new], dim=1), \"online\")\n",
        "\n",
        "        loss_beta = agent_beta.update_q_net(q_beta_fed_online, q_alpha_target, self)\n",
        "\n",
        "        return batch_q_alpha_online.detach().cpu().mean().item(), loss_alpha.detach().cpu(), \\\n",
        "               batch_q_beta_online.detach().cpu().mean().item(), loss_beta.detach().cpu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jf6gJPRY8WqX"
      },
      "source": [
        "## Training Setup and Execution\n",
        "\n",
        "This section includes the main training loops and experiment configurations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ibru5by8WqX"
      },
      "source": [
        "### Standard Training Setup (run_fed.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ias3FKUT8WqX",
        "outputId": "bf46a4dd-c69b-4f99-9f5e-7f22de607091"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using CUDA: True\n",
            "\n",
            "Episode 0 - Step 61 - Epsilon 0.9969545705045101 - Mean Length 60.0 - Mean Reward 866.153 - Mean Loss 0.0 - Mean Q Value 0.0 - Mean Reward 858.139 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 7.392 - Time 2024-11-11T04:39:27\n",
            "Episode 20 - Step 1281 - Epsilon 0.9379565984989742 - Mean Length 60.0 - Mean Reward 923.882 - Mean Loss 0.0 - Mean Q Value 0.0 - Mean Reward 967.961 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 7.328 - Time 2024-11-11T04:39:35\n",
            "Episode 40 - Step 2501 - Epsilon 0.8824500199869305 - Mean Length 60.0 - Mean Reward 940.432 - Mean Loss 0.0 - Mean Q Value 0.0 - Mean Reward 975.46 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 6.863 - Time 2024-11-11T04:39:42\n",
            "Episode 60 - Step 3721 - Epsilon 0.8302282206033068 - Mean Length 60.0 - Mean Reward 932.676 - Mean Loss 0.0 - Mean Q Value 0.0 - Mean Reward 973.894 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 7.097 - Time 2024-11-11T04:39:49\n",
            "Episode 80 - Step 4941 - Epsilon 0.7810968130482226 - Mean Length 60.0 - Mean Reward 948.024 - Mean Loss 0.0 - Mean Q Value 0.0 - Mean Reward 966.109 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 7.22 - Time 2024-11-11T04:39:56\n",
            "Episode 100 - Step 6161 - Epsilon 0.7348729135113439 - Mean Length 60.0 - Mean Reward 943.429 - Mean Loss 3.08 - Mean Q Value -0.038 - Mean Reward 956.149 - Mean Loss 3.145 - Mean Q Value 0.002 - Time Delta 54.467 - Time 2024-11-11T04:40:50\n",
            "Episode 120 - Step 7381 - Epsilon 0.6913844609161292 - Mean Length 60.0 - Mean Reward 953.609 - Mean Loss 6.068 - Mean Q Value -0.078 - Mean Reward 948.646 - Mean Loss 6.207 - Mean Q Value 0.002 - Time Delta 56.646 - Time 2024-11-11T04:41:47\n",
            "Episode 140 - Step 8601 - Epsilon 0.6504695764499779 - Mean Length 60.0 - Mean Reward 961.397 - Mean Loss 9.131 - Mean Q Value -0.118 - Mean Reward 942.306 - Mean Loss 9.345 - Mean Q Value 0.002 - Time Delta 56.694 - Time 2024-11-11T04:42:44\n",
            "Episode 160 - Step 9821 - Epsilon 0.6119759609962373 - Mean Length 60.0 - Mean Reward 965.62 - Mean Loss 12.286 - Mean Q Value -0.158 - Mean Reward 950.914 - Mean Loss 12.575 - Mean Q Value 0.002 - Time Delta 57.616 - Time 2024-11-11T04:43:41\n",
            "Episode 180 - Step 11041 - Epsilon 0.5757603282250823 - Mean Length 60.0 - Mean Reward 941.782 - Mean Loss 15.159000396728516 - Mean Q Value -0.198 - Mean Reward 963.635 - Mean Loss 15.522000312805176 - Mean Q Value 0.002 - Time Delta 57.248 - Time 2024-11-11T04:44:39\n",
            "Episode 200 - Step 12261 - Epsilon 0.5416878712330542 - Mean Length 60.0 - Mean Reward 941.889 - Mean Loss 14.859999656677246 - Mean Q Value -0.2 - Mean Reward 974.716 - Mean Loss 15.234000205993652 - Mean Q Value 0.0 - Time Delta 57.275 - Time 2024-11-11T04:45:36\n",
            "Episode 220 - Step 13481 - Epsilon 0.5096317607459198 - Mean Length 60.0 - Mean Reward 938.289 - Mean Loss 14.704000473022461 - Mean Q Value -0.2 - Mean Reward 986.876 - Mean Loss 15.07800006866455 - Mean Q Value 0.0 - Time Delta 57.581 - Time 2024-11-11T04:46:33\n",
            "Episode 240 - Step 14701 - Epsilon 0.4794726730169727 - Mean Length 60.0 - Mean Reward 935.112 - Mean Loss 14.625 - Mean Q Value -0.2 - Mean Reward 996.822 - Mean Loss 14.99899959564209 - Mean Q Value 0.0 - Time Delta 57.343 - Time 2024-11-11T04:47:31\n",
            "Episode 260 - Step 15921 - Epsilon 0.4510983456634622 - Mean Length 60.0 - Mean Reward 947.322 - Mean Loss 14.605999946594238 - Mean Q Value -0.2 - Mean Reward 1003.78 - Mean Loss 14.979999542236328 - Mean Q Value 0.0 - Time Delta 57.459 - Time 2024-11-11T04:48:28\n",
            "Episode 280 - Step 17141 - Epsilon 0.42440315978781235 - Mean Length 60.0 - Mean Reward 970.907 - Mean Loss 14.911999702453613 - Mean Q Value -0.2 - Mean Reward 994.508 - Mean Loss 15.28600025177002 - Mean Q Value 0.0 - Time Delta 57.582 - Time 2024-11-11T04:49:26\n",
            "Episode 300 - Step 18361 - Epsilon 0.3992877468281709 - Mean Length 60.0 - Mean Reward 979.995 - Mean Loss 15.180000305175781 - Mean Q Value -0.2 - Mean Reward 998.591 - Mean Loss 15.553999900817871 - Mean Q Value 0.0 - Time Delta 57.707 - Time 2024-11-11T04:50:24\n",
            "Episode 320 - Step 19581 - Epsilon 0.3756586186748197 - Mean Length 60.0 - Mean Reward 992.336 - Mean Loss 15.41100025177002 - Mean Q Value -0.2 - Mean Reward 1012.329 - Mean Loss 15.78499984741211 - Mean Q Value 0.0 - Time Delta 57.882 - Time 2024-11-11T04:51:21\n",
            "Episode 340 - Step 20801 - Epsilon 0.3534278196756246 - Mean Length 60.0 - Mean Reward 997.314 - Mean Loss 15.704000473022461 - Mean Q Value -0.2 - Mean Reward 1017.896 - Mean Loss 16.077999114990234 - Mean Q Value 0.0 - Time Delta 58.448 - Time 2024-11-11T04:52:20\n",
            "Episode 360 - Step 22021 - Epsilon 0.3325125992351901 - Mean Length 60.0 - Mean Reward 1000.006 - Mean Loss 15.74899959564209 - Mean Q Value -0.2 - Mean Reward 1011.039 - Mean Loss 16.12299919128418 - Mean Q Value 0.0 - Time Delta 58.347 - Time 2024-11-11T04:53:18\n",
            "Episode 380 - Step 23241 - Epsilon 0.31283510378899615 - Mean Length 60.0 - Mean Reward 1010.13 - Mean Loss 15.78499984741211 - Mean Q Value -0.2 - Mean Reward 1029.149 - Mean Loss 16.159000396728516 - Mean Q Value 0.0 - Time Delta 58.103 - Time 2024-11-11T04:54:16\n",
            "Episode 400 - Step 24461 - Epsilon 0.2943220870059435 - Mean Length 60.0 - Mean Reward 1014.065 - Mean Loss 15.967000007629395 - Mean Q Value -0.2 - Mean Reward 1031.59 - Mean Loss 16.341999053955078 - Mean Q Value 0.0 - Time Delta 58.19 - Time 2024-11-11T04:55:15\n",
            "Episode 420 - Step 25681 - Epsilon 0.2769046371406005 - Mean Length 60.0 - Mean Reward 1015.793 - Mean Loss 16.082000732421875 - Mean Q Value -0.2 - Mean Reward 1014.372 - Mean Loss 16.45599937438965 - Mean Q Value 0.0 - Time Delta 58.201 - Time 2024-11-11T04:56:13\n",
            "Episode 440 - Step 26901 - Epsilon 0.2605179205202465 - Mean Length 60.0 - Mean Reward 1014.445 - Mean Loss 16.020000457763672 - Mean Q Value -0.2 - Mean Reward 1006.879 - Mean Loss 16.395000457763672 - Mean Q Value 0.0 - Time Delta 58.012 - Time 2024-11-11T04:57:11\n",
            "Episode 460 - Step 28121 - Epsilon 0.24510094021189008 - Mean Length 60.0 - Mean Reward 1024.711 - Mean Loss 16.014999389648438 - Mean Q Value -0.2 - Mean Reward 998.685 - Mean Loss 16.389999389648438 - Mean Q Value 0.0 - Time Delta 58.127 - Time 2024-11-11T04:58:09\n",
            "Episode 480 - Step 29341 - Epsilon 0.23059630897093594 - Mean Length 60.0 - Mean Reward 1013.692 - Mean Loss 16.066999435424805 - Mean Q Value -0.2 - Mean Reward 995.349 - Mean Loss 16.441999435424805 - Mean Q Value 0.0 - Time Delta 58.326 - Time 2024-11-11T04:59:07\n",
            "Episode 500 - Step 30561 - Epsilon 0.21695003562634188 - Mean Length 60.0 - Mean Reward 1020.509 - Mean Loss 15.996000289916992 - Mean Q Value -0.2 - Mean Reward 998.524 - Mean Loss 16.371000289916992 - Mean Q Value 0.0 - Time Delta 58.501 - Time 2024-11-11T05:00:06\n",
            "Episode 520 - Step 31781 - Epsilon 0.20411132410711488 - Mean Length 60.0 - Mean Reward 1006.636 - Mean Loss 16.01300048828125 - Mean Q Value -0.2 - Mean Reward 1009.161 - Mean Loss 16.38800048828125 - Mean Q Value 0.0 - Time Delta 59.093 - Time 2024-11-11T05:01:05\n",
            "Episode 540 - Step 33001 - Epsilon 0.19203238436205666 - Mean Length 60.0 - Mean Reward 1008.643 - Mean Loss 15.878000259399414 - Mean Q Value -0.2 - Mean Reward 1007.616 - Mean Loss 16.253000259399414 - Mean Q Value 0.0 - Time Delta 58.775 - Time 2024-11-11T05:02:04\n",
            "Episode 560 - Step 34221 - Epsilon 0.180668254468941 - Mean Length 60.0 - Mean Reward 1011.425 - Mean Loss 16.007999420166016 - Mean Q Value -0.2 - Mean Reward 1038.106 - Mean Loss 16.382999420166016 - Mean Q Value 0.0 - Time Delta 58.886 - Time 2024-11-11T05:03:02\n",
            "Episode 580 - Step 35441 - Epsilon 0.16997663327094237 - Mean Length 60.0 - Mean Reward 1022.321 - Mean Loss 16.02400016784668 - Mean Q Value -0.2 - Mean Reward 1028.534 - Mean Loss 16.399999618530273 - Mean Q Value 0.0 - Time Delta 58.946 - Time 2024-11-11T05:04:01\n",
            "Episode 600 - Step 36661 - Epsilon 0.1599177229173448 - Mean Length 60.0 - Mean Reward 1016.031 - Mean Loss 16.066999435424805 - Mean Q Value -0.2 - Mean Reward 1039.794 - Mean Loss 16.44300079345703 - Mean Q Value 0.0 - Time Delta 58.99 - Time 2024-11-11T05:05:00\n",
            "Episode 620 - Step 37881 - Epsilon 0.1504540807224031 - Mean Length 60.0 - Mean Reward 1025.535 - Mean Loss 16.048999786376953 - Mean Q Value -0.2 - Mean Reward 1042.23 - Mean Loss 16.423999786376953 - Mean Q Value 0.0 - Time Delta 59.034 - Time 2024-11-11T05:05:59\n",
            "Episode 640 - Step 39101 - Epsilon 0.14155047979093155 - Mean Length 60.0 - Mean Reward 1027.628 - Mean Loss 16.19099998474121 - Mean Q Value -0.2 - Mean Reward 1037.634 - Mean Loss 16.56599998474121 - Mean Q Value 0.0 - Time Delta 58.741 - Time 2024-11-11T05:06:58\n",
            "Episode 660 - Step 40321 - Epsilon 0.13317377789181806 - Mean Length 60.0 - Mean Reward 1022.955 - Mean Loss 16.07900047302246 - Mean Q Value -0.2 - Mean Reward 1038.357 - Mean Loss 16.45400047302246 - Mean Q Value 0.0 - Time Delta 58.743 - Time 2024-11-11T05:07:57\n",
            "Episode 680 - Step 41541 - Epsilon 0.1252927940913654 - Mean Length 60.0 - Mean Reward 1014.798 - Mean Loss 16.06399917602539 - Mean Q Value -0.2 - Mean Reward 1043.131 - Mean Loss 16.440000534057617 - Mean Q Value 0.0 - Time Delta 58.582 - Time 2024-11-11T05:08:55\n",
            "Episode 700 - Step 42761 - Epsilon 0.11787819268725437 - Mean Length 60.0 - Mean Reward 1026.335 - Mean Loss 16.198999404907227 - Mean Q Value -0.2 - Mean Reward 1026.079 - Mean Loss 16.575000762939453 - Mean Q Value 0.0 - Time Delta 59.571 - Time 2024-11-11T05:09:55\n",
            "Episode 720 - Step 43981 - Epsilon 0.11090237401107712 - Mean Length 60.0 - Mean Reward 1033.352 - Mean Loss 16.20800018310547 - Mean Q Value -0.2 - Mean Reward 1019.257 - Mean Loss 16.583999633789062 - Mean Q Value 0.0 - Time Delta 58.981 - Time 2024-11-11T05:10:54\n",
            "Episode 740 - Step 45201 - Epsilon 0.10433937169298577 - Mean Length 60.0 - Mean Reward 1030.949 - Mean Loss 16.233999252319336 - Mean Q Value -0.2 - Mean Reward 1029.143 - Mean Loss 16.610000610351562 - Mean Q Value 0.0 - Time Delta 59.01 - Time 2024-11-11T05:11:53\n",
            "Episode 760 - Step 46421 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1014.934 - Mean Loss 16.235000610351562 - Mean Q Value -0.2 - Mean Reward 1014.807 - Mean Loss 16.611000061035156 - Mean Q Value 0.0 - Time Delta 59.184 - Time 2024-11-11T05:12:52\n",
            "Episode 780 - Step 47641 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1013.853 - Mean Loss 16.198999404907227 - Mean Q Value -0.2 - Mean Reward 1012.463 - Mean Loss 16.575000762939453 - Mean Q Value 0.0 - Time Delta 59.305 - Time 2024-11-11T05:13:52\n",
            "Episode 800 - Step 48861 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1021.69 - Mean Loss 16.048999786376953 - Mean Q Value -0.2 - Mean Reward 1017.495 - Mean Loss 16.424999237060547 - Mean Q Value 0.0 - Time Delta 59.13 - Time 2024-11-11T05:14:51\n",
            "Episode 820 - Step 50081 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1017.272 - Mean Loss 16.218000411987305 - Mean Q Value -0.2 - Mean Reward 1010.225 - Mean Loss 16.5939998626709 - Mean Q Value 0.0 - Time Delta 59.127 - Time 2024-11-11T05:15:50\n",
            "Episode 840 - Step 51301 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1018.514 - Mean Loss 16.16200065612793 - Mean Q Value -0.2 - Mean Reward 1017.984 - Mean Loss 16.53700065612793 - Mean Q Value 0.0 - Time Delta 59.308 - Time 2024-11-11T05:16:49\n",
            "Episode 860 - Step 52521 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1037.862 - Mean Loss 16.174999237060547 - Mean Q Value -0.2 - Mean Reward 1002.433 - Mean Loss 16.549999237060547 - Mean Q Value 0.0 - Time Delta 59.142 - Time 2024-11-11T05:17:48\n",
            "Episode 880 - Step 53741 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1039.888 - Mean Loss 16.256000518798828 - Mean Q Value -0.2 - Mean Reward 995.738 - Mean Loss 16.631000518798828 - Mean Q Value 0.0 - Time Delta 59.772 - Time 2024-11-11T05:18:48\n",
            "Episode 900 - Step 54961 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1028.24 - Mean Loss 16.305999755859375 - Mean Q Value -0.2 - Mean Reward 998.176 - Mean Loss 16.68199920654297 - Mean Q Value 0.0 - Time Delta 59.651 - Time 2024-11-11T05:19:48\n",
            "Episode 920 - Step 56181 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1025.384 - Mean Loss 16.174999237060547 - Mean Q Value -0.2 - Mean Reward 1008.755 - Mean Loss 16.551000595092773 - Mean Q Value 0.0 - Time Delta 59.55 - Time 2024-11-11T05:20:47\n",
            "Episode 940 - Step 57401 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1021.119 - Mean Loss 16.190000534057617 - Mean Q Value -0.2 - Mean Reward 1007.238 - Mean Loss 16.56599998474121 - Mean Q Value 0.0 - Time Delta 59.516 - Time 2024-11-11T05:21:47\n",
            "Episode 960 - Step 58621 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1013.389 - Mean Loss 16.097000122070312 - Mean Q Value -0.2 - Mean Reward 1007.475 - Mean Loss 16.474000930786133 - Mean Q Value 0.0 - Time Delta 59.484 - Time 2024-11-11T05:22:46\n",
            "Episode 980 - Step 59841 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1016.169 - Mean Loss 15.95300006866455 - Mean Q Value -0.2 - Mean Reward 1018.013 - Mean Loss 16.32900047302246 - Mean Q Value 0.0 - Time Delta 59.507 - Time 2024-11-11T05:23:46\n",
            "Episode 1000 - Step 61061 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1016.63 - Mean Loss 15.96399974822998 - Mean Q Value -0.2 - Mean Reward 1024.097 - Mean Loss 16.34000015258789 - Mean Q Value 0.0 - Time Delta 59.475 - Time 2024-11-11T05:24:45\n",
            "Episode 1020 - Step 62281 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1019.184 - Mean Loss 16.065000534057617 - Mean Q Value -0.2 - Mean Reward 1024.142 - Mean Loss 16.44099998474121 - Mean Q Value 0.0 - Time Delta 59.388 - Time 2024-11-11T05:25:45\n",
            "Episode 1040 - Step 63501 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1020.65 - Mean Loss 16.128000259399414 - Mean Q Value -0.2 - Mean Reward 1024.235 - Mean Loss 16.503999710083008 - Mean Q Value 0.0 - Time Delta 59.448 - Time 2024-11-11T05:26:44\n",
            "Episode 1060 - Step 64721 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1019.259 - Mean Loss 16.211000442504883 - Mean Q Value -0.2 - Mean Reward 1016.156 - Mean Loss 16.586999893188477 - Mean Q Value 0.0 - Time Delta 59.296 - Time 2024-11-11T05:27:43\n",
            "Episode 1080 - Step 65941 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1012.266 - Mean Loss 16.135000228881836 - Mean Q Value -0.2 - Mean Reward 1022.397 - Mean Loss 16.51099967956543 - Mean Q Value 0.0 - Time Delta 59.921 - Time 2024-11-11T05:28:43\n",
            "Episode 1100 - Step 67161 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1020.499 - Mean Loss 16.1299991607666 - Mean Q Value -0.2 - Mean Reward 1024.706 - Mean Loss 16.506999969482422 - Mean Q Value 0.0 - Time Delta 59.714 - Time 2024-11-11T05:29:43\n",
            "Episode 1120 - Step 68381 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1016.456 - Mean Loss 15.998000144958496 - Mean Q Value -0.2 - Mean Reward 1024.739 - Mean Loss 16.375 - Mean Q Value 0.0 - Time Delta 59.603 - Time 2024-11-11T05:30:43\n",
            "Episode 1140 - Step 69601 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1027.398 - Mean Loss 15.956000328063965 - Mean Q Value -0.2 - Mean Reward 1009.731 - Mean Loss 16.332000732421875 - Mean Q Value 0.0 - Time Delta 59.683 - Time 2024-11-11T05:31:42\n",
            "Episode 1160 - Step 70821 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1030.176 - Mean Loss 16.14299964904785 - Mean Q Value -0.2 - Mean Reward 1025.822 - Mean Loss 16.520000457763672 - Mean Q Value 0.0 - Time Delta 59.586 - Time 2024-11-11T05:32:42\n",
            "Episode 1180 - Step 72041 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1030.938 - Mean Loss 16.347999572753906 - Mean Q Value -0.2 - Mean Reward 1025.261 - Mean Loss 16.724000930786133 - Mean Q Value 0.0 - Time Delta 59.454 - Time 2024-11-11T05:33:41\n",
            "Episode 1200 - Step 73261 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1025.118 - Mean Loss 16.177000045776367 - Mean Q Value -0.2 - Mean Reward 1017.334 - Mean Loss 16.55299949645996 - Mean Q Value 0.0 - Time Delta 59.548 - Time 2024-11-11T05:34:41\n",
            "Episode 1220 - Step 74481 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1035.213 - Mean Loss 16.239999771118164 - Mean Q Value -0.2 - Mean Reward 1010.445 - Mean Loss 16.617000579833984 - Mean Q Value 0.0 - Time Delta 59.443 - Time 2024-11-11T05:35:40\n",
            "Episode 1240 - Step 75701 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1032.37 - Mean Loss 16.336999893188477 - Mean Q Value -0.2 - Mean Reward 1021.066 - Mean Loss 16.71299934387207 - Mean Q Value 0.0 - Time Delta 59.432 - Time 2024-11-11T05:36:40\n",
            "Episode 1260 - Step 76921 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1026.79 - Mean Loss 16.163999557495117 - Mean Q Value -0.2 - Mean Reward 1014.243 - Mean Loss 16.540000915527344 - Mean Q Value 0.0 - Time Delta 59.174 - Time 2024-11-11T05:37:39\n",
            "Episode 1280 - Step 78141 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1030.648 - Mean Loss 16.045000076293945 - Mean Q Value -0.2 - Mean Reward 1012.438 - Mean Loss 16.42099952697754 - Mean Q Value 0.0 - Time Delta 59.89 - Time 2024-11-11T05:38:39\n",
            "Episode 1300 - Step 79361 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1035.403 - Mean Loss 16.177000045776367 - Mean Q Value -0.2 - Mean Reward 1006.978 - Mean Loss 16.55299949645996 - Mean Q Value 0.0 - Time Delta 59.458 - Time 2024-11-11T05:39:38\n",
            "Episode 1320 - Step 80581 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1025.864 - Mean Loss 16.152000427246094 - Mean Q Value -0.2 - Mean Reward 1016.146 - Mean Loss 16.527999877929688 - Mean Q Value 0.0 - Time Delta 59.703 - Time 2024-11-11T05:40:38\n",
            "Episode 1340 - Step 81801 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1023.208 - Mean Loss 15.982000350952148 - Mean Q Value -0.2 - Mean Reward 1015.102 - Mean Loss 16.357999801635742 - Mean Q Value 0.0 - Time Delta 60.087 - Time 2024-11-11T05:41:38\n",
            "Episode 1360 - Step 83021 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1027.717 - Mean Loss 16.070999145507812 - Mean Q Value -0.2 - Mean Reward 1027.007 - Mean Loss 16.447999954223633 - Mean Q Value 0.0 - Time Delta 59.956 - Time 2024-11-11T05:42:38\n",
            "Episode 1380 - Step 84241 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1034.213 - Mean Loss 16.20199966430664 - Mean Q Value -0.2 - Mean Reward 1020.215 - Mean Loss 16.577999114990234 - Mean Q Value 0.0 - Time Delta 59.51 - Time 2024-11-11T05:43:37\n",
            "Episode 1400 - Step 85461 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1030.947 - Mean Loss 16.334999084472656 - Mean Q Value -0.2 - Mean Reward 1020.554 - Mean Loss 16.711000442504883 - Mean Q Value 0.0 - Time Delta 59.368 - Time 2024-11-11T05:44:37\n",
            "Episode 1420 - Step 86681 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1030.701 - Mean Loss 16.415000915527344 - Mean Q Value -0.2 - Mean Reward 1013.025 - Mean Loss 16.790000915527344 - Mean Q Value 0.0 - Time Delta 59.23 - Time 2024-11-11T05:45:36\n",
            "Episode 1440 - Step 87901 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1027.663 - Mean Loss 16.398000717163086 - Mean Q Value -0.2 - Mean Reward 1023.712 - Mean Loss 16.77400016784668 - Mean Q Value 0.0 - Time Delta 59.471 - Time 2024-11-11T05:46:36\n",
            "Episode 1460 - Step 89121 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1040.984 - Mean Loss 16.391000747680664 - Mean Q Value -0.2 - Mean Reward 1022.91 - Mean Loss 16.76799964904785 - Mean Q Value 0.0 - Time Delta 60.39 - Time 2024-11-11T05:47:36\n",
            "Episode 1480 - Step 90341 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1040.892 - Mean Loss 16.538000106811523 - Mean Q Value -0.2 - Mean Reward 1016.992 - Mean Loss 16.913999557495117 - Mean Q Value 0.0 - Time Delta 60.567 - Time 2024-11-11T05:48:37\n",
            "Episode 1500 - Step 91561 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1039.711 - Mean Loss 16.43600082397461 - Mean Q Value -0.2 - Mean Reward 1024.454 - Mean Loss 16.812000274658203 - Mean Q Value 0.0 - Time Delta 60.109 - Time 2024-11-11T05:49:37\n",
            "Episode 1520 - Step 92781 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1036.478 - Mean Loss 16.351999282836914 - Mean Q Value -0.2 - Mean Reward 1036.021 - Mean Loss 16.72800064086914 - Mean Q Value 0.0 - Time Delta 60.092 - Time 2024-11-11T05:50:37\n",
            "Episode 1540 - Step 94001 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1036.694 - Mean Loss 16.356000900268555 - Mean Q Value -0.2 - Mean Reward 1018.202 - Mean Loss 16.73200035095215 - Mean Q Value 0.0 - Time Delta 60.017 - Time 2024-11-11T05:51:37\n",
            "Episode 1560 - Step 95221 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1025.046 - Mean Loss 16.214000701904297 - Mean Q Value -0.2 - Mean Reward 994.569 - Mean Loss 16.589000701904297 - Mean Q Value 0.0 - Time Delta 60.055 - Time 2024-11-11T05:52:37\n",
            "Episode 1580 - Step 96441 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1013.307 - Mean Loss 15.9350004196167 - Mean Q Value -0.2 - Mean Reward 1015.988 - Mean Loss 16.309999465942383 - Mean Q Value 0.0 - Time Delta 59.912 - Time 2024-11-11T05:53:37\n",
            "Episode 1600 - Step 97661 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1012.098 - Mean Loss 15.87600040435791 - Mean Q Value -0.2 - Mean Reward 1002.592 - Mean Loss 16.25200080871582 - Mean Q Value 0.0 - Time Delta 60.019 - Time 2024-11-11T05:54:37\n",
            "Episode 1620 - Step 98881 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1028.129 - Mean Loss 15.909000396728516 - Mean Q Value -0.2 - Mean Reward 998.379 - Mean Loss 16.285999298095703 - Mean Q Value 0.0 - Time Delta 59.788 - Time 2024-11-11T05:55:37\n",
            "Episode 1640 - Step 100101 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1031.056 - Mean Loss 16.097000122070312 - Mean Q Value -0.2 - Mean Reward 994.397 - Mean Loss 16.474000930786133 - Mean Q Value 0.0 - Time Delta 60.578 - Time 2024-11-11T05:56:37\n",
            "Episode 1660 - Step 101321 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1017.307 - Mean Loss 16.13800048828125 - Mean Q Value -0.2 - Mean Reward 1020.359 - Mean Loss 16.513999938964844 - Mean Q Value 0.0 - Time Delta 59.961 - Time 2024-11-11T05:57:37\n",
            "Episode 1680 - Step 102541 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1033.236 - Mean Loss 16.194000244140625 - Mean Q Value -0.2 - Mean Reward 1010.691 - Mean Loss 16.56999969482422 - Mean Q Value 0.0 - Time Delta 60.025 - Time 2024-11-11T05:58:37\n",
            "Episode 1700 - Step 103761 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1031.338 - Mean Loss 16.24799919128418 - Mean Q Value -0.2 - Mean Reward 1013.599 - Mean Loss 16.624000549316406 - Mean Q Value 0.0 - Time Delta 60.178 - Time 2024-11-11T05:59:37\n",
            "Episode 1720 - Step 104981 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1014.704 - Mean Loss 16.20199966430664 - Mean Q Value -0.2 - Mean Reward 1014.931 - Mean Loss 16.577999114990234 - Mean Q Value 0.0 - Time Delta 60.351 - Time 2024-11-11T06:00:38\n",
            "Episode 1740 - Step 106201 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1014.966 - Mean Loss 16.073999404907227 - Mean Q Value -0.2 - Mean Reward 1031.37 - Mean Loss 16.450000762939453 - Mean Q Value 0.0 - Time Delta 60.254 - Time 2024-11-11T06:01:38\n",
            "Episode 1760 - Step 107421 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1028.31 - Mean Loss 16.166000366210938 - Mean Q Value -0.2 - Mean Reward 1040.906 - Mean Loss 16.542999267578125 - Mean Q Value 0.0 - Time Delta 60.155 - Time 2024-11-11T06:02:38\n",
            "Episode 1780 - Step 108641 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1022.307 - Mean Loss 16.202999114990234 - Mean Q Value -0.2 - Mean Reward 1046.534 - Mean Loss 16.57900047302246 - Mean Q Value 0.0 - Time Delta 59.794 - Time 2024-11-11T06:03:38\n",
            "Episode 1800 - Step 109861 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1019.12 - Mean Loss 16.19499969482422 - Mean Q Value -0.2 - Mean Reward 1050.917 - Mean Loss 16.57200050354004 - Mean Q Value 0.0 - Time Delta 59.698 - Time 2024-11-11T06:04:38\n",
            "Episode 1820 - Step 111081 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1042.794 - Mean Loss 16.274999618530273 - Mean Q Value -0.2 - Mean Reward 1052.008 - Mean Loss 16.652000427246094 - Mean Q Value 0.0 - Time Delta 59.943 - Time 2024-11-11T06:05:37\n",
            "Episode 1840 - Step 112301 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1041.469 - Mean Loss 16.51799964904785 - Mean Q Value -0.2 - Mean Reward 1047.997 - Mean Loss 16.895000457763672 - Mean Q Value 0.0 - Time Delta 59.863 - Time 2024-11-11T06:06:37\n",
            "Episode 1860 - Step 113521 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1042.456 - Mean Loss 16.48699951171875 - Mean Q Value -0.2 - Mean Reward 1037.216 - Mean Loss 16.863000869750977 - Mean Q Value 0.0 - Time Delta 59.908 - Time 2024-11-11T06:07:37\n",
            "Episode 1880 - Step 114741 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1043.568 - Mean Loss 16.50200080871582 - Mean Q Value -0.2 - Mean Reward 1035.988 - Mean Loss 16.878999710083008 - Mean Q Value 0.0 - Time Delta 59.739 - Time 2024-11-11T06:08:37\n",
            "Episode 1900 - Step 115961 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1032.958 - Mean Loss 16.458999633789062 - Mean Q Value -0.2 - Mean Reward 1036.667 - Mean Loss 16.834999084472656 - Mean Q Value 0.0 - Time Delta 60.054 - Time 2024-11-11T06:09:37\n",
            "Episode 1920 - Step 117181 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1030.021 - Mean Loss 16.381000518798828 - Mean Q Value -0.2 - Mean Reward 1037.408 - Mean Loss 16.756999969482422 - Mean Q Value 0.0 - Time Delta 60.031 - Time 2024-11-11T06:10:37\n",
            "Episode 1940 - Step 118401 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1032.762 - Mean Loss 16.22100067138672 - Mean Q Value -0.2 - Mean Reward 1026.045 - Mean Loss 16.597000122070312 - Mean Q Value 0.0 - Time Delta 60.028 - Time 2024-11-11T06:11:37\n",
            "Episode 1960 - Step 119621 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1028.459 - Mean Loss 16.204999923706055 - Mean Q Value -0.2 - Mean Reward 1027.051 - Mean Loss 16.579999923706055 - Mean Q Value 0.0 - Time Delta 59.663 - Time 2024-11-11T06:12:37\n",
            "Episode 1980 - Step 120841 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1031.579 - Mean Loss 16.18899917602539 - Mean Q Value -0.2 - Mean Reward 1003.068 - Mean Loss 16.565000534057617 - Mean Q Value 0.0 - Time Delta 59.869 - Time 2024-11-11T06:13:37\n",
            "Episode 2000 - Step 122061 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1045.543 - Mean Loss 16.27199935913086 - Mean Q Value -0.2 - Mean Reward 1016.467 - Mean Loss 16.648000717163086 - Mean Q Value 0.0 - Time Delta 59.526 - Time 2024-11-11T06:14:36\n",
            "Episode 2020 - Step 123281 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1037.916 - Mean Loss 16.341999053955078 - Mean Q Value -0.2 - Mean Reward 1006.962 - Mean Loss 16.718000411987305 - Mean Q Value 0.0 - Time Delta 59.425 - Time 2024-11-11T06:15:36\n",
            "Episode 2040 - Step 124501 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1038.715 - Mean Loss 16.371999740600586 - Mean Q Value -0.2 - Mean Reward 1023.421 - Mean Loss 16.74799919128418 - Mean Q Value 0.0 - Time Delta 59.698 - Time 2024-11-11T06:16:35\n",
            "Episode 2060 - Step 125721 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1045.645 - Mean Loss 16.44700050354004 - Mean Q Value -0.2 - Mean Reward 1020.497 - Mean Loss 16.822999954223633 - Mean Q Value 0.0 - Time Delta 59.767 - Time 2024-11-11T06:17:35\n",
            "Episode 2080 - Step 126941 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1033.572 - Mean Loss 16.39299964904785 - Mean Q Value -0.2 - Mean Reward 1041.909 - Mean Loss 16.768999099731445 - Mean Q Value 0.0 - Time Delta 59.925 - Time 2024-11-11T06:18:35\n",
            "Episode 2100 - Step 128161 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1028.725 - Mean Loss 16.354999542236328 - Mean Q Value -0.2 - Mean Reward 1030.887 - Mean Loss 16.731000900268555 - Mean Q Value 0.0 - Time Delta 59.942 - Time 2024-11-11T06:19:35\n",
            "Episode 2120 - Step 129381 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1026.381 - Mean Loss 16.27199935913086 - Mean Q Value -0.2 - Mean Reward 1033.259 - Mean Loss 16.648000717163086 - Mean Q Value 0.0 - Time Delta 59.954 - Time 2024-11-11T06:20:35\n",
            "Episode 2140 - Step 130601 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1023.561 - Mean Loss 16.23200035095215 - Mean Q Value -0.2 - Mean Reward 1041.531 - Mean Loss 16.608999252319336 - Mean Q Value 0.0 - Time Delta 59.71 - Time 2024-11-11T06:21:35\n",
            "Episode 2160 - Step 131821 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1024.003 - Mean Loss 16.141000747680664 - Mean Q Value -0.2 - Mean Reward 1039.982 - Mean Loss 16.517000198364258 - Mean Q Value 0.0 - Time Delta 59.657 - Time 2024-11-11T06:22:34\n",
            "Episode 2180 - Step 133041 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1029.42 - Mean Loss 16.21500015258789 - Mean Q Value -0.2 - Mean Reward 1031.313 - Mean Loss 16.590999603271484 - Mean Q Value 0.0 - Time Delta 59.777 - Time 2024-11-11T06:23:34\n",
            "Episode 2200 - Step 134261 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1029.209 - Mean Loss 16.18600082397461 - Mean Q Value -0.2 - Mean Reward 1035.036 - Mean Loss 16.562999725341797 - Mean Q Value 0.0 - Time Delta 60.119 - Time 2024-11-11T06:24:34\n",
            "Episode 2220 - Step 135481 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1027.354 - Mean Loss 16.20400047302246 - Mean Q Value -0.2 - Mean Reward 1032.713 - Mean Loss 16.58099937438965 - Mean Q Value 0.0 - Time Delta 59.15 - Time 2024-11-11T06:25:33\n",
            "Episode 2240 - Step 136701 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1014.378 - Mean Loss 16.090999603271484 - Mean Q Value -0.2 - Mean Reward 1014.397 - Mean Loss 16.468000411987305 - Mean Q Value 0.0 - Time Delta 59.784 - Time 2024-11-11T06:26:33\n",
            "Episode 2260 - Step 137921 - Epsilon 0 - Mean Length 60.0 - Mean Reward 997.005 - Mean Loss 16.003999710083008 - Mean Q Value -0.2 - Mean Reward 1022.886 - Mean Loss 16.381000518798828 - Mean Q Value 0.0 - Time Delta 59.465 - Time 2024-11-11T06:27:33\n",
            "Episode 2280 - Step 139141 - Epsilon 0 - Mean Length 60.0 - Mean Reward 994.314 - Mean Loss 15.776000022888184 - Mean Q Value -0.2 - Mean Reward 1024.789 - Mean Loss 16.152000427246094 - Mean Q Value 0.0 - Time Delta 60.011 - Time 2024-11-11T06:28:33\n",
            "Episode 2300 - Step 140361 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1002.272 - Mean Loss 15.689000129699707 - Mean Q Value -0.2 - Mean Reward 1014.396 - Mean Loss 16.06599998474121 - Mean Q Value 0.0 - Time Delta 60.298 - Time 2024-11-11T06:29:33\n",
            "Episode 2320 - Step 141581 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1005.606 - Mean Loss 15.781000137329102 - Mean Q Value -0.2 - Mean Reward 1015.604 - Mean Loss 16.156999588012695 - Mean Q Value 0.0 - Time Delta 60.476 - Time 2024-11-11T06:30:33\n",
            "Episode 2340 - Step 142801 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1013.63 - Mean Loss 15.930999755859375 - Mean Q Value -0.2 - Mean Reward 1027.67 - Mean Loss 16.305999755859375 - Mean Q Value 0.0 - Time Delta 59.92 - Time 2024-11-11T06:31:33\n",
            "Episode 2360 - Step 144021 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1031.473 - Mean Loss 16.038000106811523 - Mean Q Value -0.2 - Mean Reward 1018.987 - Mean Loss 16.413999557495117 - Mean Q Value 0.0 - Time Delta 59.898 - Time 2024-11-11T06:32:33\n",
            "Episode 2380 - Step 145241 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1032.547 - Mean Loss 16.18199920654297 - Mean Q Value -0.2 - Mean Reward 1022.979 - Mean Loss 16.558000564575195 - Mean Q Value 0.0 - Time Delta 59.877 - Time 2024-11-11T06:33:33\n",
            "Episode 2400 - Step 146461 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1028.298 - Mean Loss 16.31800079345703 - Mean Q Value -0.2 - Mean Reward 1023.703 - Mean Loss 16.694000244140625 - Mean Q Value 0.0 - Time Delta 59.798 - Time 2024-11-11T06:34:33\n",
            "Episode 2420 - Step 147681 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1021.332 - Mean Loss 16.267000198364258 - Mean Q Value -0.2 - Mean Reward 1030.813 - Mean Loss 16.643999099731445 - Mean Q Value 0.0 - Time Delta 59.88 - Time 2024-11-11T06:35:33\n",
            "Episode 2440 - Step 148901 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1020.526 - Mean Loss 16.18600082397461 - Mean Q Value -0.2 - Mean Reward 1026.604 - Mean Loss 16.562999725341797 - Mean Q Value 0.0 - Time Delta 60.071 - Time 2024-11-11T06:36:33\n",
            "Episode 2460 - Step 150121 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1017.671 - Mean Loss 16.094999313354492 - Mean Q Value -0.2 - Mean Reward 1032.498 - Mean Loss 16.47100067138672 - Mean Q Value 0.0 - Time Delta 59.68 - Time 2024-11-11T06:37:32\n",
            "Episode 2480 - Step 151341 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1026.582 - Mean Loss 16.200000762939453 - Mean Q Value -0.2 - Mean Reward 1029.326 - Mean Loss 16.57699966430664 - Mean Q Value 0.0 - Time Delta 59.733 - Time 2024-11-11T06:38:32\n",
            "Episode 2500 - Step 152561 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1023.333 - Mean Loss 16.336999893188477 - Mean Q Value -0.2 - Mean Reward 1029.949 - Mean Loss 16.71299934387207 - Mean Q Value 0.0 - Time Delta 59.607 - Time 2024-11-11T06:39:32\n",
            "Episode 2520 - Step 153781 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1035.929 - Mean Loss 16.30900001525879 - Mean Q Value -0.2 - Mean Reward 1030.76 - Mean Loss 16.684999465942383 - Mean Q Value 0.0 - Time Delta 59.252 - Time 2024-11-11T06:40:31\n",
            "Episode 2540 - Step 155001 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1038.989 - Mean Loss 16.40399932861328 - Mean Q Value -0.2 - Mean Reward 1029.034 - Mean Loss 16.780000686645508 - Mean Q Value 0.0 - Time Delta 59.346 - Time 2024-11-11T06:41:30\n",
            "Episode 2560 - Step 156221 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1035.908 - Mean Loss 16.552000045776367 - Mean Q Value -0.2 - Mean Reward 1013.22 - Mean Loss 16.929000854492188 - Mean Q Value 0.0 - Time Delta 59.448 - Time 2024-11-11T06:42:30\n",
            "Episode 2580 - Step 157441 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1026.168 - Mean Loss 16.3700008392334 - Mean Q Value -0.2 - Mean Reward 1032.033 - Mean Loss 16.746999740600586 - Mean Q Value 0.0 - Time Delta 59.298 - Time 2024-11-11T06:43:29\n",
            "Episode 2600 - Step 158661 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1043.826 - Mean Loss 16.166000366210938 - Mean Q Value -0.2 - Mean Reward 1043.381 - Mean Loss 16.542999267578125 - Mean Q Value 0.0 - Time Delta 59.662 - Time 2024-11-11T06:44:29\n",
            "Episode 2620 - Step 159881 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1029.336 - Mean Loss 16.347999572753906 - Mean Q Value -0.2 - Mean Reward 1026.538 - Mean Loss 16.724000930786133 - Mean Q Value 0.0 - Time Delta 59.943 - Time 2024-11-11T06:45:29\n",
            "Episode 2640 - Step 161101 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1029.337 - Mean Loss 16.18400001525879 - Mean Q Value -0.2 - Mean Reward 1016.688 - Mean Loss 16.56100082397461 - Mean Q Value 0.0 - Time Delta 59.741 - Time 2024-11-11T06:46:28\n",
            "Episode 2660 - Step 162321 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1013.584 - Mean Loss 15.940999984741211 - Mean Q Value -0.2 - Mean Reward 1022.823 - Mean Loss 16.316999435424805 - Mean Q Value 0.0 - Time Delta 59.79 - Time 2024-11-11T06:47:28\n",
            "Episode 2680 - Step 163541 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1017.261 - Mean Loss 15.866000175476074 - Mean Q Value -0.2 - Mean Reward 1001.15 - Mean Loss 16.242000579833984 - Mean Q Value 0.0 - Time Delta 59.92 - Time 2024-11-11T06:48:28\n",
            "Episode 2700 - Step 164761 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1011.513 - Mean Loss 15.968999862670898 - Mean Q Value -0.2 - Mean Reward 997.981 - Mean Loss 16.344999313354492 - Mean Q Value 0.0 - Time Delta 59.719 - Time 2024-11-11T06:49:28\n",
            "Episode 2720 - Step 165981 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1017.892 - Mean Loss 15.782999992370605 - Mean Q Value -0.2 - Mean Reward 1012.429 - Mean Loss 16.159000396728516 - Mean Q Value 0.0 - Time Delta 59.899 - Time 2024-11-11T06:50:28\n",
            "Episode 2740 - Step 167201 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1016.952 - Mean Loss 15.850000381469727 - Mean Q Value -0.2 - Mean Reward 1023.529 - Mean Loss 16.22599983215332 - Mean Q Value 0.0 - Time Delta 59.967 - Time 2024-11-11T06:51:28\n",
            "Episode 2760 - Step 168421 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1029.432 - Mean Loss 15.928000450134277 - Mean Q Value -0.2 - Mean Reward 1044.28 - Mean Loss 16.304000854492188 - Mean Q Value 0.0 - Time Delta 59.673 - Time 2024-11-11T06:52:27\n",
            "Episode 2780 - Step 169641 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1024.552 - Mean Loss 15.984999656677246 - Mean Q Value -0.2 - Mean Reward 1041.218 - Mean Loss 16.361000061035156 - Mean Q Value 0.0 - Time Delta 59.972 - Time 2024-11-11T06:53:27\n",
            "Episode 2800 - Step 170861 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1012.454 - Mean Loss 15.878000259399414 - Mean Q Value -0.2 - Mean Reward 1026.739 - Mean Loss 16.253999710083008 - Mean Q Value 0.0 - Time Delta 59.796 - Time 2024-11-11T06:54:27\n",
            "Episode 2820 - Step 172081 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1010.964 - Mean Loss 15.72599983215332 - Mean Q Value -0.2 - Mean Reward 1016.194 - Mean Loss 16.10099983215332 - Mean Q Value 0.0 - Time Delta 59.859 - Time 2024-11-11T06:55:27\n",
            "Episode 2840 - Step 173301 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1013.051 - Mean Loss 15.72700023651123 - Mean Q Value -0.2 - Mean Reward 1011.071 - Mean Loss 16.101999282836914 - Mean Q Value 0.0 - Time Delta 59.646 - Time 2024-11-11T06:56:27\n",
            "Episode 2860 - Step 174521 - Epsilon 0 - Mean Length 60.0 - Mean Reward 993.192 - Mean Loss 15.730999946594238 - Mean Q Value -0.2 - Mean Reward 988.493 - Mean Loss 16.106000900268555 - Mean Q Value 0.0 - Time Delta 59.497 - Time 2024-11-11T06:57:26\n",
            "Episode 2880 - Step 175741 - Epsilon 0 - Mean Length 60.0 - Mean Reward 995.423 - Mean Loss 15.63599967956543 - Mean Q Value -0.2 - Mean Reward 986.627 - Mean Loss 16.01099967956543 - Mean Q Value 0.0 - Time Delta 59.543 - Time 2024-11-11T06:58:26\n",
            "Episode 2900 - Step 176961 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1002.243 - Mean Loss 15.590999603271484 - Mean Q Value -0.2 - Mean Reward 994.541 - Mean Loss 15.965999603271484 - Mean Q Value 0.0 - Time Delta 59.32 - Time 2024-11-11T06:59:25\n",
            "Episode 2920 - Step 178181 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1001.512 - Mean Loss 15.861000061035156 - Mean Q Value -0.2 - Mean Reward 1002.047 - Mean Loss 16.23699951171875 - Mean Q Value 0.0 - Time Delta 59.277 - Time 2024-11-11T07:00:24\n",
            "Episode 2940 - Step 179401 - Epsilon 0 - Mean Length 60.0 - Mean Reward 994.473 - Mean Loss 15.76200008392334 - Mean Q Value -0.2 - Mean Reward 1025.204 - Mean Loss 16.13800048828125 - Mean Q Value 0.0 - Time Delta 59.225 - Time 2024-11-11T07:01:24\n",
            "Episode 2960 - Step 180621 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1002.833 - Mean Loss 15.711000442504883 - Mean Q Value -0.2 - Mean Reward 1042.354 - Mean Loss 16.086999893188477 - Mean Q Value 0.0 - Time Delta 59.732 - Time 2024-11-11T07:02:23\n",
            "Episode 2980 - Step 181841 - Epsilon 0 - Mean Length 60.0 - Mean Reward 999.046 - Mean Loss 15.781999588012695 - Mean Q Value -0.2 - Mean Reward 1045.337 - Mean Loss 16.159000396728516 - Mean Q Value 0.0 - Time Delta 59.533 - Time 2024-11-11T07:03:23\n",
            "Episode 3000 - Step 183061 - Epsilon 0 - Mean Length 60.0 - Mean Reward 997.167 - Mean Loss 15.833999633789062 - Mean Q Value -0.2 - Mean Reward 1045.119 - Mean Loss 16.211000442504883 - Mean Q Value 0.0 - Time Delta 59.711 - Time 2024-11-11T07:04:23\n",
            "Episode 3020 - Step 184281 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1006.266 - Mean Loss 15.793000221252441 - Mean Q Value -0.2 - Mean Reward 1052.209 - Mean Loss 16.16900062561035 - Mean Q Value 0.0 - Time Delta 59.608 - Time 2024-11-11T07:05:22\n",
            "Episode 3040 - Step 185501 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1027.196 - Mean Loss 16.047000885009766 - Mean Q Value -0.2 - Mean Reward 1019.641 - Mean Loss 16.42300033569336 - Mean Q Value 0.0 - Time Delta 59.526 - Time 2024-11-11T07:06:22\n",
            "Episode 3060 - Step 186721 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1034.536 - Mean Loss 16.31399917602539 - Mean Q Value -0.2 - Mean Reward 1001.707 - Mean Loss 16.690000534057617 - Mean Q Value 0.0 - Time Delta 59.515 - Time 2024-11-11T07:07:21\n",
            "Episode 3080 - Step 187941 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1046.67 - Mean Loss 16.496000289916992 - Mean Q Value -0.2 - Mean Reward 1004.299 - Mean Loss 16.871999740600586 - Mean Q Value 0.0 - Time Delta 59.271 - Time 2024-11-11T07:08:20\n",
            "Episode 3100 - Step 189161 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1046.431 - Mean Loss 16.659000396728516 - Mean Q Value -0.2 - Mean Reward 1011.313 - Mean Loss 17.03499984741211 - Mean Q Value 0.0 - Time Delta 59.176 - Time 2024-11-11T07:09:20\n",
            "Episode 3120 - Step 190381 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1045.445 - Mean Loss 16.62299919128418 - Mean Q Value -0.2 - Mean Reward 1001.465 - Mean Loss 16.999000549316406 - Mean Q Value 0.0 - Time Delta 59.855 - Time 2024-11-11T07:10:19\n",
            "Episode 3140 - Step 191601 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1045.519 - Mean Loss 16.51799964904785 - Mean Q Value -0.2 - Mean Reward 1018.176 - Mean Loss 16.895000457763672 - Mean Q Value 0.0 - Time Delta 59.614 - Time 2024-11-11T07:11:19\n",
            "Episode 3160 - Step 192821 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1049.525 - Mean Loss 16.417999267578125 - Mean Q Value -0.2 - Mean Reward 1018.537 - Mean Loss 16.795000076293945 - Mean Q Value 0.0 - Time Delta 59.63 - Time 2024-11-11T07:12:19\n",
            "Episode 3180 - Step 194041 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1037.593 - Mean Loss 16.285999298095703 - Mean Q Value -0.2 - Mean Reward 1009.469 - Mean Loss 16.663000106811523 - Mean Q Value 0.0 - Time Delta 59.459 - Time 2024-11-11T07:13:18\n",
            "Episode 3200 - Step 195261 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1052.874 - Mean Loss 16.246999740600586 - Mean Q Value -0.2 - Mean Reward 1001.619 - Mean Loss 16.624000549316406 - Mean Q Value 0.0 - Time Delta 59.568 - Time 2024-11-11T07:14:18\n",
            "Episode 3220 - Step 196481 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1038.857 - Mean Loss 16.319000244140625 - Mean Q Value -0.2 - Mean Reward 990.617 - Mean Loss 16.69499969482422 - Mean Q Value 0.0 - Time Delta 59.493 - Time 2024-11-11T07:15:17\n",
            "Episode 3240 - Step 197701 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1021.076 - Mean Loss 16.249000549316406 - Mean Q Value -0.2 - Mean Reward 986.327 - Mean Loss 16.625999450683594 - Mean Q Value 0.0 - Time Delta 59.461 - Time 2024-11-11T07:16:17\n",
            "Episode 3260 - Step 198921 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1018.817 - Mean Loss 16.15999984741211 - Mean Q Value -0.2 - Mean Reward 995.546 - Mean Loss 16.535999298095703 - Mean Q Value 0.0 - Time Delta 59.29 - Time 2024-11-11T07:17:16\n",
            "Episode 3280 - Step 200141 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1018.926 - Mean Loss 16.159000396728516 - Mean Q Value -0.2 - Mean Reward 1009.788 - Mean Loss 16.534000396728516 - Mean Q Value 0.0 - Time Delta 59.737 - Time 2024-11-11T07:18:16\n",
            "Episode 3300 - Step 201361 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1005.583 - Mean Loss 16.04800033569336 - Mean Q Value -0.2 - Mean Reward 1002.872 - Mean Loss 16.423999786376953 - Mean Q Value 0.0 - Time Delta 59.459 - Time 2024-11-11T07:19:15\n",
            "Episode 3320 - Step 202581 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1017.123 - Mean Loss 15.89900016784668 - Mean Q Value -0.2 - Mean Reward 1016.379 - Mean Loss 16.274999618530273 - Mean Q Value 0.0 - Time Delta 59.388 - Time 2024-11-11T07:20:15\n",
            "Episode 3340 - Step 203801 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1018.863 - Mean Loss 16.01099967956543 - Mean Q Value -0.2 - Mean Reward 1014.834 - Mean Loss 16.386999130249023 - Mean Q Value 0.0 - Time Delta 59.72 - Time 2024-11-11T07:21:14\n",
            "Episode 3360 - Step 205021 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1011.717 - Mean Loss 15.96399974822998 - Mean Q Value -0.2 - Mean Reward 1002.067 - Mean Loss 16.34000015258789 - Mean Q Value 0.0 - Time Delta 59.511 - Time 2024-11-11T07:22:14\n",
            "Episode 3380 - Step 206241 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1024.682 - Mean Loss 15.984000205993652 - Mean Q Value -0.2 - Mean Reward 1007.602 - Mean Loss 16.360000610351562 - Mean Q Value 0.0 - Time Delta 59.596 - Time 2024-11-11T07:23:13\n",
            "Episode 3400 - Step 207461 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1018.645 - Mean Loss 16.094999313354492 - Mean Q Value -0.2 - Mean Reward 1020.142 - Mean Loss 16.47100067138672 - Mean Q Value 0.0 - Time Delta 59.828 - Time 2024-11-11T07:24:13\n",
            "Episode 3420 - Step 208681 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1010.294 - Mean Loss 16.10099983215332 - Mean Q Value -0.2 - Mean Reward 1021.498 - Mean Loss 16.476999282836914 - Mean Q Value 0.0 - Time Delta 59.607 - Time 2024-11-11T07:25:13\n",
            "Episode 3440 - Step 209901 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1024.223 - Mean Loss 15.989999771118164 - Mean Q Value -0.2 - Mean Reward 1022.164 - Mean Loss 16.365999221801758 - Mean Q Value 0.0 - Time Delta 59.696 - Time 2024-11-11T07:26:13\n",
            "Episode 3460 - Step 211121 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1026.663 - Mean Loss 16.218000411987305 - Mean Q Value -0.2 - Mean Reward 1035.307 - Mean Loss 16.5939998626709 - Mean Q Value 0.0 - Time Delta 60.28 - Time 2024-11-11T07:27:13\n",
            "Episode 3480 - Step 212341 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1028.949 - Mean Loss 16.229999542236328 - Mean Q Value -0.2 - Mean Reward 1034.504 - Mean Loss 16.60700035095215 - Mean Q Value 0.0 - Time Delta 60.062 - Time 2024-11-11T07:28:13\n",
            "Episode 3500 - Step 213561 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1034.325 - Mean Loss 16.183000564575195 - Mean Q Value -0.2 - Mean Reward 1038.743 - Mean Loss 16.55900001525879 - Mean Q Value 0.0 - Time Delta 60.208 - Time 2024-11-11T07:29:13\n",
            "Episode 3520 - Step 214781 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1048.644 - Mean Loss 16.236000061035156 - Mean Q Value -0.2 - Mean Reward 1036.3 - Mean Loss 16.61199951171875 - Mean Q Value 0.0 - Time Delta 60.088 - Time 2024-11-11T07:30:13\n",
            "Episode 3540 - Step 216001 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1035.148 - Mean Loss 16.389999389648438 - Mean Q Value -0.2 - Mean Reward 1052.889 - Mean Loss 16.766000747680664 - Mean Q Value 0.0 - Time Delta 60.122 - Time 2024-11-11T07:31:13\n",
            "Episode 3560 - Step 217221 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1043.603 - Mean Loss 16.28700065612793 - Mean Q Value -0.2 - Mean Reward 1056.488 - Mean Loss 16.663000106811523 - Mean Q Value 0.0 - Time Delta 60.031 - Time 2024-11-11T07:32:13\n",
            "Episode 3580 - Step 218441 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1026.939 - Mean Loss 16.260000228881836 - Mean Q Value -0.2 - Mean Reward 1076.212 - Mean Loss 16.63599967956543 - Mean Q Value 0.0 - Time Delta 59.723 - Time 2024-11-11T07:33:13\n",
            "Episode 3600 - Step 219661 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1013.587 - Mean Loss 16.128999710083008 - Mean Q Value -0.2 - Mean Reward 1070.996 - Mean Loss 16.503999710083008 - Mean Q Value 0.0 - Time Delta 60.094 - Time 2024-11-11T07:34:13\n",
            "Episode 3620 - Step 220881 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1011.841 - Mean Loss 16.040000915527344 - Mean Q Value -0.2 - Mean Reward 1075.372 - Mean Loss 16.416000366210938 - Mean Q Value 0.0 - Time Delta 60.122 - Time 2024-11-11T07:35:13\n",
            "Episode 3640 - Step 222101 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1016.105 - Mean Loss 16.023000717163086 - Mean Q Value -0.2 - Mean Reward 1057.392 - Mean Loss 16.39900016784668 - Mean Q Value 0.0 - Time Delta 60.129 - Time 2024-11-11T07:36:13\n",
            "Episode 3660 - Step 223321 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1017.514 - Mean Loss 16.06399917602539 - Mean Q Value -0.2 - Mean Reward 1051.113 - Mean Loss 16.440000534057617 - Mean Q Value 0.0 - Time Delta 60.07 - Time 2024-11-11T07:37:13\n",
            "Episode 3680 - Step 224541 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1019.162 - Mean Loss 16.18600082397461 - Mean Q Value -0.2 - Mean Reward 1027.197 - Mean Loss 16.562000274658203 - Mean Q Value 0.0 - Time Delta 59.859 - Time 2024-11-11T07:38:13\n",
            "Episode 3700 - Step 225761 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1022.351 - Mean Loss 16.215999603271484 - Mean Q Value -0.2 - Mean Reward 1032.097 - Mean Loss 16.591999053955078 - Mean Q Value 0.0 - Time Delta 59.973 - Time 2024-11-11T07:39:13\n",
            "Episode 3720 - Step 226981 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1026.4 - Mean Loss 16.31999969482422 - Mean Q Value -0.2 - Mean Reward 1027.569 - Mean Loss 16.695999145507812 - Mean Q Value 0.0 - Time Delta 59.853 - Time 2024-11-11T07:40:13\n",
            "Episode 3740 - Step 228201 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1017.898 - Mean Loss 16.332000732421875 - Mean Q Value -0.2 - Mean Reward 1045.847 - Mean Loss 16.70800018310547 - Mean Q Value 0.0 - Time Delta 59.623 - Time 2024-11-11T07:41:13\n",
            "Episode 3760 - Step 229421 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1022.631 - Mean Loss 16.327999114990234 - Mean Q Value -0.2 - Mean Reward 1044.672 - Mean Loss 16.70400047302246 - Mean Q Value 0.0 - Time Delta 59.856 - Time 2024-11-11T07:42:13\n",
            "Episode 3780 - Step 230641 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1029.791 - Mean Loss 16.41900062561035 - Mean Q Value -0.2 - Mean Reward 1034.657 - Mean Loss 16.79599952697754 - Mean Q Value 0.0 - Time Delta 59.6 - Time 2024-11-11T07:43:12\n",
            "Episode 3800 - Step 231861 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1032.219 - Mean Loss 16.48699951171875 - Mean Q Value -0.2 - Mean Reward 1016.021 - Mean Loss 16.86400032043457 - Mean Q Value 0.0 - Time Delta 59.959 - Time 2024-11-11T07:44:12\n",
            "Episode 3820 - Step 233081 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1019.251 - Mean Loss 16.371999740600586 - Mean Q Value -0.2 - Mean Reward 1035.553 - Mean Loss 16.749000549316406 - Mean Q Value 0.0 - Time Delta 59.884 - Time 2024-11-11T07:45:12\n",
            "Episode 3840 - Step 234301 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1022.348 - Mean Loss 16.226999282836914 - Mean Q Value -0.2 - Mean Reward 1022.267 - Mean Loss 16.60300064086914 - Mean Q Value 0.0 - Time Delta 59.855 - Time 2024-11-11T07:46:12\n",
            "Episode 3860 - Step 235521 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1007.527 - Mean Loss 16.125999450683594 - Mean Q Value -0.2 - Mean Reward 1019.643 - Mean Loss 16.50200080871582 - Mean Q Value 0.0 - Time Delta 60.054 - Time 2024-11-11T07:47:12\n",
            "Episode 3880 - Step 236741 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1013.835 - Mean Loss 15.913000106811523 - Mean Q Value -0.2 - Mean Reward 1042.094 - Mean Loss 16.288000106811523 - Mean Q Value 0.0 - Time Delta 59.995 - Time 2024-11-11T07:48:12\n",
            "Episode 3900 - Step 237961 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1010.779 - Mean Loss 15.927000045776367 - Mean Q Value -0.2 - Mean Reward 1049.235 - Mean Loss 16.30299949645996 - Mean Q Value 0.0 - Time Delta 59.738 - Time 2024-11-11T07:49:12\n",
            "Episode 3920 - Step 239181 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1005.451 - Mean Loss 15.96399974822998 - Mean Q Value -0.2 - Mean Reward 1027.273 - Mean Loss 16.34000015258789 - Mean Q Value 0.0 - Time Delta 59.722 - Time 2024-11-11T07:50:11\n",
            "Episode 3940 - Step 240401 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1012.979 - Mean Loss 15.968999862670898 - Mean Q Value -0.2 - Mean Reward 1028.698 - Mean Loss 16.344999313354492 - Mean Q Value 0.0 - Time Delta 59.627 - Time 2024-11-11T07:51:11\n",
            "Episode 3960 - Step 241621 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1016.182 - Mean Loss 16.027000427246094 - Mean Q Value -0.2 - Mean Reward 1034.413 - Mean Loss 16.402999877929688 - Mean Q Value 0.0 - Time Delta 59.413 - Time 2024-11-11T07:52:10\n",
            "Episode 3980 - Step 242841 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1019.648 - Mean Loss 16.179000854492188 - Mean Q Value -0.2 - Mean Reward 1022.947 - Mean Loss 16.55500030517578 - Mean Q Value 0.0 - Time Delta 60.295 - Time 2024-11-11T07:53:11\n",
            "Episode 4000 - Step 244061 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1030.567 - Mean Loss 16.257999420166016 - Mean Q Value -0.2 - Mean Reward 1021.589 - Mean Loss 16.634000778198242 - Mean Q Value 0.0 - Time Delta 59.993 - Time 2024-11-11T07:54:11\n",
            "Episode 4020 - Step 245281 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1041.285 - Mean Loss 16.246000289916992 - Mean Q Value -0.2 - Mean Reward 1021.273 - Mean Loss 16.621000289916992 - Mean Q Value 0.0 - Time Delta 60.028 - Time 2024-11-11T07:55:11\n",
            "Episode 4040 - Step 246501 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1025.639 - Mean Loss 16.273000717163086 - Mean Q Value -0.2 - Mean Reward 1025.594 - Mean Loss 16.64900016784668 - Mean Q Value 0.0 - Time Delta 59.896 - Time 2024-11-11T07:56:11\n",
            "Episode 4060 - Step 247721 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1029.552 - Mean Loss 16.183000564575195 - Mean Q Value -0.2 - Mean Reward 1016.478 - Mean Loss 16.558000564575195 - Mean Q Value 0.0 - Time Delta 59.888 - Time 2024-11-11T07:57:11\n",
            "Episode 4080 - Step 248941 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1026.117 - Mean Loss 16.145000457763672 - Mean Q Value -0.2 - Mean Reward 1015.191 - Mean Loss 16.520999908447266 - Mean Q Value 0.0 - Time Delta 59.876 - Time 2024-11-11T07:58:10\n",
            "Episode 4100 - Step 250161 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1020.398 - Mean Loss 16.069000244140625 - Mean Q Value -0.2 - Mean Reward 1021.769 - Mean Loss 16.444000244140625 - Mean Q Value 0.0 - Time Delta 59.752 - Time 2024-11-11T07:59:10\n",
            "Episode 4120 - Step 251381 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1022.139 - Mean Loss 16.163999557495117 - Mean Q Value -0.2 - Mean Reward 1029.342 - Mean Loss 16.540000915527344 - Mean Q Value 0.0 - Time Delta 59.589 - Time 2024-11-11T08:00:10\n",
            "Episode 4140 - Step 252601 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1045.276 - Mean Loss 16.16200065612793 - Mean Q Value -0.2 - Mean Reward 1024.544 - Mean Loss 16.538000106811523 - Mean Q Value 0.0 - Time Delta 59.929 - Time 2024-11-11T08:01:10\n",
            "Episode 4160 - Step 253821 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1050.604 - Mean Loss 16.44300079345703 - Mean Q Value -0.2 - Mean Reward 1055.897 - Mean Loss 16.819000244140625 - Mean Q Value 0.0 - Time Delta 59.794 - Time 2024-11-11T08:02:10\n",
            "Episode 4180 - Step 255041 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1050.602 - Mean Loss 16.6200008392334 - Mean Q Value -0.2 - Mean Reward 1059.148 - Mean Loss 16.996999740600586 - Mean Q Value 0.0 - Time Delta 60.002 - Time 2024-11-11T08:03:10\n",
            "Episode 4200 - Step 256261 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1056.273 - Mean Loss 16.79199981689453 - Mean Q Value -0.2 - Mean Reward 1060.827 - Mean Loss 17.16900062561035 - Mean Q Value 0.0 - Time Delta 60.084 - Time 2024-11-11T08:04:10\n",
            "Episode 4220 - Step 257481 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1049.395 - Mean Loss 16.743999481201172 - Mean Q Value -0.2 - Mean Reward 1055.212 - Mean Loss 17.121000289916992 - Mean Q Value 0.0 - Time Delta 59.957 - Time 2024-11-11T08:05:10\n",
            "Episode 4240 - Step 258701 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1024.269 - Mean Loss 16.577999114990234 - Mean Q Value -0.2 - Mean Reward 1047.099 - Mean Loss 16.95400047302246 - Mean Q Value 0.0 - Time Delta 59.92 - Time 2024-11-11T08:06:09\n",
            "Episode 4260 - Step 259921 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1022.739 - Mean Loss 16.277000427246094 - Mean Q Value -0.2 - Mean Reward 1020.536 - Mean Loss 16.652999877929688 - Mean Q Value 0.0 - Time Delta 59.82 - Time 2024-11-11T08:07:09\n",
            "Episode 4280 - Step 261141 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1021.38 - Mean Loss 16.097999572753906 - Mean Q Value -0.2 - Mean Reward 1015.619 - Mean Loss 16.475000381469727 - Mean Q Value 0.0 - Time Delta 59.826 - Time 2024-11-11T08:08:09\n",
            "Episode 4300 - Step 262361 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1021.554 - Mean Loss 16.02199935913086 - Mean Q Value -0.2 - Mean Reward 1018.099 - Mean Loss 16.39900016784668 - Mean Q Value 0.0 - Time Delta 59.985 - Time 2024-11-11T08:09:09\n",
            "Episode 4320 - Step 263581 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1034.137 - Mean Loss 16.138999938964844 - Mean Q Value -0.2 - Mean Reward 1024.476 - Mean Loss 16.514999389648438 - Mean Q Value 0.0 - Time Delta 59.9 - Time 2024-11-11T08:10:09\n",
            "Episode 4340 - Step 264801 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1050.62 - Mean Loss 16.429000854492188 - Mean Q Value -0.2 - Mean Reward 1034.175 - Mean Loss 16.80500030517578 - Mean Q Value 0.0 - Time Delta 60.132 - Time 2024-11-11T08:11:09\n",
            "Episode 4360 - Step 266021 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1061.562 - Mean Loss 16.68000030517578 - Mean Q Value -0.2 - Mean Reward 1029.731 - Mean Loss 17.055999755859375 - Mean Q Value 0.0 - Time Delta 59.896 - Time 2024-11-11T08:12:09\n",
            "Episode 4380 - Step 267241 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1042.561 - Mean Loss 16.73900032043457 - Mean Q Value -0.2 - Mean Reward 1029.556 - Mean Loss 17.114999771118164 - Mean Q Value 0.0 - Time Delta 60.018 - Time 2024-11-11T08:13:09\n",
            "Episode 4400 - Step 268461 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1037.24 - Mean Loss 16.474000930786133 - Mean Q Value -0.2 - Mean Reward 1014.888 - Mean Loss 16.85099983215332 - Mean Q Value 0.0 - Time Delta 59.743 - Time 2024-11-11T08:14:09\n",
            "Episode 4420 - Step 269681 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1021.063 - Mean Loss 16.200000762939453 - Mean Q Value -0.2 - Mean Reward 1008.908 - Mean Loss 16.576000213623047 - Mean Q Value 0.0 - Time Delta 59.829 - Time 2024-11-11T08:15:09\n",
            "Episode 4440 - Step 270901 - Epsilon 0 - Mean Length 60.0 - Mean Reward 993.582 - Mean Loss 15.87600040435791 - Mean Q Value -0.2 - Mean Reward 1015.46 - Mean Loss 16.25200080871582 - Mean Q Value 0.0 - Time Delta 59.593 - Time 2024-11-11T08:16:08\n",
            "Episode 4460 - Step 272121 - Epsilon 0 - Mean Length 60.0 - Mean Reward 984.629 - Mean Loss 15.484999656677246 - Mean Q Value -0.2 - Mean Reward 1008.303 - Mean Loss 15.861000061035156 - Mean Q Value 0.0 - Time Delta 59.692 - Time 2024-11-11T08:17:08\n",
            "Episode 4480 - Step 273341 - Epsilon 0 - Mean Length 60.0 - Mean Reward 989.283 - Mean Loss 15.451000213623047 - Mean Q Value -0.2 - Mean Reward 1026.655 - Mean Loss 15.82699966430664 - Mean Q Value 0.0 - Time Delta 59.529 - Time 2024-11-11T08:18:07\n",
            "Episode 4500 - Step 274561 - Epsilon 0 - Mean Length 60.0 - Mean Reward 989.13 - Mean Loss 15.571999549865723 - Mean Q Value -0.2 - Mean Reward 1037.233 - Mean Loss 15.949000358581543 - Mean Q Value 0.0 - Time Delta 59.504 - Time 2024-11-11T08:19:07\n",
            "Episode 4520 - Step 275781 - Epsilon 0 - Mean Length 60.0 - Mean Reward 986.412 - Mean Loss 15.699000358581543 - Mean Q Value -0.2 - Mean Reward 1056.515 - Mean Loss 16.075000762939453 - Mean Q Value 0.0 - Time Delta 59.411 - Time 2024-11-11T08:20:06\n",
            "Episode 4540 - Step 277001 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1014.56 - Mean Loss 15.871999740600586 - Mean Q Value -0.2 - Mean Reward 1053.531 - Mean Loss 16.24799919128418 - Mean Q Value 0.0 - Time Delta 59.609 - Time 2024-11-11T08:21:06\n",
            "Episode 4560 - Step 278221 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1011.894 - Mean Loss 16.200000762939453 - Mean Q Value -0.2 - Mean Reward 1062.839 - Mean Loss 16.576000213623047 - Mean Q Value 0.0 - Time Delta 59.758 - Time 2024-11-11T08:22:06\n",
            "Episode 4580 - Step 279441 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1027.479 - Mean Loss 16.165000915527344 - Mean Q Value -0.2 - Mean Reward 1035.977 - Mean Loss 16.541000366210938 - Mean Q Value 0.0 - Time Delta 59.889 - Time 2024-11-11T08:23:06\n",
            "Episode 4600 - Step 280661 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1033.615 - Mean Loss 16.224000930786133 - Mean Q Value -0.2 - Mean Reward 1039.66 - Mean Loss 16.600000381469727 - Mean Q Value 0.0 - Time Delta 59.851 - Time 2024-11-11T08:24:05\n",
            "Episode 4620 - Step 281881 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1033.214 - Mean Loss 16.3439998626709 - Mean Q Value -0.2 - Mean Reward 1039.415 - Mean Loss 16.719999313354492 - Mean Q Value 0.0 - Time Delta 59.748 - Time 2024-11-11T08:25:05\n",
            "Episode 4640 - Step 283101 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1031.031 - Mean Loss 16.29199981689453 - Mean Q Value -0.2 - Mean Reward 1042.282 - Mean Loss 16.667999267578125 - Mean Q Value 0.0 - Time Delta 59.794 - Time 2024-11-11T08:26:05\n",
            "Episode 4660 - Step 284321 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1017.255 - Mean Loss 16.155000686645508 - Mean Q Value -0.2 - Mean Reward 1042.303 - Mean Loss 16.5310001373291 - Mean Q Value 0.0 - Time Delta 59.909 - Time 2024-11-11T08:27:05\n",
            "Episode 4680 - Step 285541 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1011.202 - Mean Loss 15.954000473022461 - Mean Q Value -0.2 - Mean Reward 1058.191 - Mean Loss 16.329999923706055 - Mean Q Value 0.0 - Time Delta 59.711 - Time 2024-11-11T08:28:05\n",
            "Episode 4700 - Step 286761 - Epsilon 0 - Mean Length 60.0 - Mean Reward 990.943 - Mean Loss 15.699000358581543 - Mean Q Value -0.2 - Mean Reward 1068.09 - Mean Loss 16.075000762939453 - Mean Q Value 0.0 - Time Delta 59.637 - Time 2024-11-11T08:29:04\n",
            "Episode 4720 - Step 287981 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1002.691 - Mean Loss 15.487000465393066 - Mean Q Value -0.2 - Mean Reward 1057.43 - Mean Loss 15.86299991607666 - Mean Q Value 0.0 - Time Delta 59.686 - Time 2024-11-11T08:30:04\n",
            "Episode 4740 - Step 289201 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1007.891 - Mean Loss 15.628000259399414 - Mean Q Value -0.2 - Mean Reward 1056.798 - Mean Loss 16.003999710083008 - Mean Q Value 0.0 - Time Delta 60.122 - Time 2024-11-11T08:31:04\n",
            "Episode 4760 - Step 290421 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1022.284 - Mean Loss 15.739999771118164 - Mean Q Value -0.2 - Mean Reward 1066.984 - Mean Loss 16.115999221801758 - Mean Q Value 0.0 - Time Delta 59.994 - Time 2024-11-11T08:32:04\n",
            "Episode 4780 - Step 291641 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1017.198 - Mean Loss 15.97599983215332 - Mean Q Value -0.2 - Mean Reward 1069.078 - Mean Loss 16.351999282836914 - Mean Q Value 0.0 - Time Delta 60.05 - Time 2024-11-11T08:33:04\n",
            "Episode 4800 - Step 292861 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1031.373 - Mean Loss 16.177000045776367 - Mean Q Value -0.2 - Mean Reward 1051.287 - Mean Loss 16.554000854492188 - Mean Q Value 0.0 - Time Delta 60.265 - Time 2024-11-11T08:34:04\n",
            "Episode 4820 - Step 294081 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1023.984 - Mean Loss 16.280000686645508 - Mean Q Value -0.2 - Mean Reward 1043.622 - Mean Loss 16.6560001373291 - Mean Q Value 0.0 - Time Delta 60.135 - Time 2024-11-11T08:35:05\n",
            "Episode 4840 - Step 295301 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1020.544 - Mean Loss 16.14299964904785 - Mean Q Value -0.2 - Mean Reward 1030.496 - Mean Loss 16.518999099731445 - Mean Q Value 0.0 - Time Delta 60.066 - Time 2024-11-11T08:36:05\n",
            "Episode 4860 - Step 296521 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1042.401 - Mean Loss 16.325000762939453 - Mean Q Value -0.2 - Mean Reward 1018.549 - Mean Loss 16.701000213623047 - Mean Q Value 0.0 - Time Delta 59.938 - Time 2024-11-11T08:37:05\n",
            "Episode 4880 - Step 297741 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1050.463 - Mean Loss 16.575000762939453 - Mean Q Value -0.2 - Mean Reward 1002.427 - Mean Loss 16.95199966430664 - Mean Q Value 0.0 - Time Delta 59.95 - Time 2024-11-11T08:38:04\n",
            "Episode 4900 - Step 298961 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1055.808 - Mean Loss 16.735000610351562 - Mean Q Value -0.2 - Mean Reward 1011.188 - Mean Loss 17.111000061035156 - Mean Q Value 0.0 - Time Delta 59.809 - Time 2024-11-11T08:39:04\n",
            "Episode 4920 - Step 300181 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1066.919 - Mean Loss 16.76799964904785 - Mean Q Value -0.2 - Mean Reward 1019.233 - Mean Loss 17.143999099731445 - Mean Q Value 0.0 - Time Delta 60.277 - Time 2024-11-11T08:40:05\n",
            "Episode 4940 - Step 301401 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1055.989 - Mean Loss 16.908000946044922 - Mean Q Value -0.2 - Mean Reward 1031.15 - Mean Loss 17.28499984741211 - Mean Q Value 0.0 - Time Delta 59.964 - Time 2024-11-11T08:41:05\n",
            "Episode 4960 - Step 302621 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1038.21 - Mean Loss 16.645999908447266 - Mean Q Value -0.2 - Mean Reward 1039.112 - Mean Loss 17.02199935913086 - Mean Q Value 0.0 - Time Delta 60.182 - Time 2024-11-11T08:42:05\n",
            "Episode 4980 - Step 303841 - Epsilon 0 - Mean Length 60.0 - Mean Reward 1028.121 - Mean Loss 16.3799991607666 - Mean Q Value -0.2 - Mean Reward 1055.798 - Mean Loss 16.756000518798828 - Mean Q Value 0.0 - Time Delta 59.998 - Time 2024-11-11T08:43:05\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Standard training setup\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print(f\"Using CUDA: {use_cuda}\")\n",
        "print()\n",
        "\n",
        "# Model parameters\n",
        "user_features = 1\n",
        "doc_features = 1\n",
        "num_of_candidates = 10\n",
        "slate_size = 3\n",
        "batch_size = 32\n",
        "num_contex = 5\n",
        "\n",
        "# Setup save directory\n",
        "save_dir = Path(\"checkpoints_fed\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
        "save_dir.mkdir(parents=True)\n",
        "\n",
        "# Initialize logger\n",
        "logger = Logger(save_dir)\n",
        "\n",
        "# Initialize agents\n",
        "agent_alpha = AgentAlpha(user_features, doc_features, num_of_candidates, slate_size, batch_size, num_contex)\n",
        "agent_beta = AgentBeta(user_features, doc_features, num_of_candidates, slate_size, batch_size)\n",
        "agent_fed = AgentFed(user_features, doc_features, num_of_candidates, slate_size, batch_size)\n",
        "\n",
        "# Initialize environment\n",
        "env = RecsimEnv(num_of_candidates, slate_size, True, 42, 42)\n",
        "\n",
        "# Training loop\n",
        "episodes = 5000\n",
        "for e in range(episodes):\n",
        "    agent_fed.act_ini(agent_alpha, agent_beta, env)\n",
        "\n",
        "    while True:\n",
        "        done_alpha, reward_alpha, done_beta, reward_beta = agent_fed.act(agent_alpha, agent_beta, env)\n",
        "        q_alpha, loss_alpha, q_beta, loss_beta = agent_fed.learn(agent_alpha, agent_beta)\n",
        "        logger.log_step(reward_alpha, loss_alpha, q_alpha, reward_beta, loss_beta, q_beta)\n",
        "\n",
        "        if done_beta:\n",
        "            break\n",
        "\n",
        "    logger.log_episode()\n",
        "\n",
        "    if e % 20 == 0:\n",
        "        logger.record(episode=e, epsilon=agent_fed.exploration_rate, step=agent_fed.curr_step)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}