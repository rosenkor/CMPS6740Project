{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3NrNBvq8WqK"
      },
      "source": [
        "# Federated Reinforcement Learning for Recommendation Systems\n",
        "\n",
        "This notebook combines implements a federated reinforcement learning system for recommendations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8Sjywjw8WqP"
      },
      "source": [
        "## Environment Setup\n",
        "\n",
        "First, let's import all required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOLDz-Vi9pJD",
        "outputId": "311e51de-8cb1-4498-b525-d126c80f45ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting recsim-v2\n",
            "  Downloading recsim_v2-0.2.7.tar.gz (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.8/71.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from recsim-v2) (1.4.0)\n",
            "Requirement already satisfied: dopamine-rl>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from recsim-v2) (4.0.9)\n",
            "Collecting gin-config-v2 (from recsim-v2)\n",
            "  Downloading gin_config_v2-0.8.0.tar.gz (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from recsim-v2) (0.25.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from recsim-v2) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from recsim-v2) (1.13.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from recsim-v2) (2.17.1)\n",
            "Requirement already satisfied: gin-config>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from dopamine-rl>=2.0.5->recsim-v2) (0.5.0)\n",
            "Requirement already satisfied: opencv-python>=3.4.8.29 in /usr/local/lib/python3.10/dist-packages (from dopamine-rl>=2.0.5->recsim-v2) (4.10.0.84)\n",
            "Requirement already satisfied: flax>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from dopamine-rl>=2.0.5->recsim-v2) (0.8.5)\n",
            "Requirement already satisfied: jax>=0.1.72 in /usr/local/lib/python3.10/dist-packages (from dopamine-rl>=2.0.5->recsim-v2) (0.4.33)\n",
            "Requirement already satisfied: jaxlib>=0.1.51 in /usr/local/lib/python3.10/dist-packages (from dopamine-rl>=2.0.5->recsim-v2) (0.4.33)\n",
            "Requirement already satisfied: Pillow>=7.0.0 in /usr/local/lib/python3.10/dist-packages (from dopamine-rl>=2.0.5->recsim-v2) (11.0.0)\n",
            "Requirement already satisfied: pygame>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from dopamine-rl>=2.0.5->recsim-v2) (2.6.1)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.10/dist-packages (from dopamine-rl>=2.0.5->recsim-v2) (2.2.2)\n",
            "Requirement already satisfied: tf-slim>=1.0 in /usr/local/lib/python3.10/dist-packages (from dopamine-rl>=2.0.5->recsim-v2) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-probability>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from dopamine-rl>=2.0.5->recsim-v2) (0.24.0)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from dopamine-rl>=2.0.5->recsim-v2) (4.66.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym->recsim-v2) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym->recsim-v2) (0.0.8)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (1.68.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->recsim-v2) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->recsim-v2) (0.45.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax>=0.2.0->dopamine-rl>=2.0.5->recsim-v2) (1.1.0)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax>=0.2.0->dopamine-rl>=2.0.5->recsim-v2) (0.2.4)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax>=0.2.0->dopamine-rl>=2.0.5->recsim-v2) (0.6.4)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax>=0.2.0->dopamine-rl>=2.0.5->recsim-v2) (0.1.69)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax>=0.2.0->dopamine-rl>=2.0.5->recsim-v2) (13.9.4)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax>=0.2.0->dopamine-rl>=2.0.5->recsim-v2) (6.0.2)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow->recsim-v2) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow->recsim-v2) (0.13.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->dopamine-rl>=2.0.5->recsim-v2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->dopamine-rl>=2.0.5->recsim-v2) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->dopamine-rl>=2.0.5->recsim-v2) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->recsim-v2) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->recsim-v2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->recsim-v2) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->recsim-v2) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow->recsim-v2) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow->recsim-v2) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow->recsim-v2) (3.1.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.13.0->dopamine-rl>=2.0.5->recsim-v2) (4.4.2)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.13.0->dopamine-rl>=2.0.5->recsim-v2) (0.1.8)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax>=0.2.0->dopamine-rl>=2.0.5->recsim-v2) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax>=0.2.0->dopamine-rl>=2.0.5->recsim-v2) (2.18.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow->recsim-v2) (3.0.2)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.10/dist-packages (from optax->flax>=0.2.0->dopamine-rl>=2.0.5->recsim-v2) (0.1.87)\n",
            "Requirement already satisfied: etils[epy] in /usr/local/lib/python3.10/dist-packages (from optax->flax>=0.2.0->dopamine-rl>=2.0.5->recsim-v2) (1.11.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.2.0->dopamine-rl>=2.0.5->recsim-v2) (1.6.0)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.2.0->dopamine-rl>=2.0.5->recsim-v2) (4.11.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.87->optax->flax>=0.2.0->dopamine-rl>=2.0.5->recsim-v2) (0.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.2.0->dopamine-rl>=2.0.5->recsim-v2) (0.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.2.0->dopamine-rl>=2.0.5->recsim-v2) (2024.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.2.0->dopamine-rl>=2.0.5->recsim-v2) (6.4.5)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.2.0->dopamine-rl>=2.0.5->recsim-v2) (3.21.0)\n",
            "Building wheels for collected packages: recsim-v2, gin-config-v2\n",
            "  Building wheel for recsim-v2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for recsim-v2: filename=recsim_v2-0.2.7-py3-none-any.whl size=109746 sha256=f7d64d5adfee4231465a0650c191a1cc9897e029d65aac3e204f3a888359f7b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/0c/8d/b2de8b95d998c4e167919fd3c0f2101d3fb36523a4105dda8d\n",
            "  Building wheel for gin-config-v2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gin-config-v2: filename=gin_config_v2-0.8.0-py3-none-any.whl size=60935 sha256=72237ae2f763a96147cf17f58f3bdd9a5d1d60830897d22f148062342c5d49b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/88/cd/fa1c40eb56836f9c58eee25ac182c14556a5af1eeffc6858c5\n",
            "Successfully built recsim-v2 gin-config-v2\n",
            "Installing collected packages: gin-config-v2, recsim-v2\n",
            "Successfully installed gin-config-v2-0.8.0 recsim-v2-0.2.7\n"
          ]
        }
      ],
      "source": [
        "!pip install recsim-v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-twKUlbS8WqQ",
        "outputId": "40223b22-0768-4c92-9da5-f1eb3a98f481"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:55: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs; see https://jax.readthedocs.io/en/latest/aot.html. For example, replace xla_computation(f)(*xs) with jit(f).lower(*xs).compiler_ir('hlo'). See CHANGELOG.md for 0.4.30 for more examples.\n",
            "  from jax import xla_computation as _xla_computation\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import datetime\n",
        "import time\n",
        "from collections import deque\n",
        "from pathlib import Path\n",
        "from torch import nn\n",
        "from torch.nn.utils import weight_norm\n",
        "from gym import spaces\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# RecSim imports\n",
        "from recsim import document\n",
        "from recsim import user\n",
        "from recsim.choice_model import MultinomialLogitChoiceModel\n",
        "from recsim.simulator import environment\n",
        "from recsim.simulator import recsim_gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMahL-n38WqR"
      },
      "source": [
        "## Replay Memory Implementation\n",
        "\n",
        "Implementation of experience replay buffer for storing transitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8h_Apx6s8WqR"
      },
      "outputs": [],
      "source": [
        "class ReplayMemory():\n",
        "    def __init__(self, capacity, state_shape, action_shape):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.capacity = capacity\n",
        "        self.state_memory = torch.zeros((capacity,) + state_shape, device=self.device)\n",
        "        self.action_memory = torch.zeros((capacity,) + action_shape, device=self.device)\n",
        "        self.reward_memory = torch.zeros((capacity,), device=self.device)\n",
        "        self.next_state_memory = torch.zeros((capacity,) + state_shape, device=self.device)\n",
        "        self.terminals_memory = torch.zeros((capacity,), dtype=torch.bool, device=self.device)\n",
        "        self.click_memory = torch.zeros((capacity,) + action_shape, dtype=torch.int, device=self.device)\n",
        "        self.position = 0\n",
        "        self.full = False\n",
        "\n",
        "    def push(self, state, action, reward, click, next_state, done):\n",
        "        self.state_memory[self.position] = state\n",
        "        self.action_memory[self.position] = action\n",
        "        self.reward_memory[self.position] = reward\n",
        "        self.click_memory[self.position] = click\n",
        "        self.next_state_memory[self.position] = next_state\n",
        "        self.terminals_memory[self.position] = done\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "        self.full = self.full or self.position == 0\n",
        "\n",
        "    def recall(self, indices):\n",
        "        states = self.state_memory[indices]\n",
        "        actions = self.action_memory[indices]\n",
        "        rewards = self.reward_memory[indices]\n",
        "        clicks = self.click_memory[indices]\n",
        "        next_states = self.next_state_memory[indices]\n",
        "        terminals = self.terminals_memory[indices]\n",
        "        return states, actions, rewards, clicks, next_states, terminals\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.capacity if self.full else self.position"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SujKGGGl8WqS"
      },
      "source": [
        "## Neural Network Models\n",
        "\n",
        "Implementation of the Q-Network and MLP Network architectures used in the recommendation system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YLfC56G8WqS"
      },
      "outputs": [],
      "source": [
        "class QNet(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.online = nn.Sequential(\n",
        "            nn.Linear(input_dim, 4096),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(4096, output_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.target = nn.Sequential(\n",
        "            nn.Linear(input_dim, 4096),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(4096, output_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.target.eval()\n",
        "        for p in self.target.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, inputs, model):\n",
        "        if model == \"online\":\n",
        "            return self.online(inputs)\n",
        "        elif model == \"target\":\n",
        "            return self.target(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95mSjDKb8WqT"
      },
      "outputs": [],
      "source": [
        "class MLPNet(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.online = nn.Sequential(\n",
        "            nn.Linear(input_dim, 2048),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(2048, 2048),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(2048, 2048),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(2048, 2048),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(2048, output_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.target = nn.Sequential(\n",
        "            nn.Linear(input_dim, 2048),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(2048, 2048),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(2048, 2048),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(2048, 2048),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(2048, output_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.target.eval()\n",
        "        for p in self.target.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, inputs, model):\n",
        "        if model == \"online\":\n",
        "            return self.online(inputs)\n",
        "        elif model == \"target\":\n",
        "            return self.target(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDR89mwq8WqU"
      },
      "source": [
        "## Slate Q-Learning Implementation\n",
        "\n",
        "This section implements the SlateQ class which handles slate-based Q-learning for recommendation selection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMKN-PW18WqU"
      },
      "outputs": [],
      "source": [
        "class SlateQ():\n",
        "    def __init__(self, user_features, doc_features, num_of_candidates, slate_size, batch_size):\n",
        "        self.user_features = user_features\n",
        "        self.num_of_candidates = num_of_candidates\n",
        "        self.doc_features = doc_features\n",
        "        self.slate_size = slate_size\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def score_documents_torch(self, user_obs, doc_obs, no_click_mass=1.0, is_mnl=True, min_normalizer=-1.0):\n",
        "        user_obs = user_obs.view(-1)\n",
        "        doc_obs = doc_obs.view(-1)\n",
        "        assert user_obs.shape == torch.Size([self.user_features])\n",
        "        assert doc_obs.shape == torch.Size([self.num_of_candidates])\n",
        "\n",
        "        scores = torch.sum(input=torch.mul(doc_obs.view(-1, 1),\n",
        "                                          user_obs.view(1, -1)).view(self.num_of_candidates,\n",
        "                                                                     self.user_features),\n",
        "                          dim=1)\n",
        "\n",
        "        all_scores = torch.cat([scores, torch.tensor([no_click_mass], device=self.device)], dim=0)\n",
        "\n",
        "        if is_mnl:\n",
        "            all_scores = torch.nn.functional.softmax(all_scores, dim=0)\n",
        "        else:\n",
        "            all_scores = all_scores - min_normalizer\n",
        "\n",
        "        assert all_scores.shape == torch.Size([self.num_of_candidates + 1])\n",
        "        return all_scores[:-1], all_scores[-1]\n",
        "\n",
        "    def compute_probs_torch(self, slate, scores_torch, score_no_click_torch):\n",
        "        slate = slate.squeeze()\n",
        "        scores_torch = scores_torch.squeeze()\n",
        "        assert slate.shape == torch.Size([self.slate_size])\n",
        "        assert scores_torch.shape == torch.Size([self.num_of_candidates])\n",
        "\n",
        "        all_scores = torch.cat([\n",
        "            torch.gather(scores_torch, 0, slate).view(-1),\n",
        "            score_no_click_torch.view(-1)\n",
        "        ], dim=0)\n",
        "\n",
        "        all_probs = all_scores / torch.sum(all_scores)\n",
        "        assert all_probs.shape == torch.Size([self.slate_size + 1])\n",
        "        return all_probs[:-1]\n",
        "\n",
        "    def select_slate_greedy(self, s_no_click, s, q):\n",
        "        s = s.view(-1)\n",
        "        q = q.view(-1)\n",
        "        assert s.shape == torch.Size([self.num_of_candidates])\n",
        "        assert q.shape == torch.Size([self.num_of_candidates])\n",
        "\n",
        "        def argmax(v, mask_inner):\n",
        "            return torch.argmax((v - torch.min(v) + 1) * mask_inner, dim=0)\n",
        "\n",
        "        numerator = torch.tensor(0., device=self.device)\n",
        "        denominator = torch.tensor(0., device=self.device) + s_no_click\n",
        "        mask_inner = torch.ones(q.size(0), device=self.device)\n",
        "\n",
        "        def set_element(v, i, x):\n",
        "            mask_inner = torch.nn.functional.one_hot(i, v.shape[0])\n",
        "            v_new = torch.ones_like(v) * x\n",
        "            return torch.where(mask_inner == 1, v_new, v)\n",
        "\n",
        "        for _ in range(self.slate_size):\n",
        "            k = argmax((numerator + s * q) / (denominator + s), mask_inner)\n",
        "            mask_inner = set_element(mask_inner, k, 0)\n",
        "            numerator = numerator + torch.gather(s * q, 0, k)\n",
        "            denominator = denominator + torch.gather(s, 0, k)\n",
        "\n",
        "        output_slate = torch.where(mask_inner == 0)[0].squeeze()\n",
        "        assert output_slate.shape == torch.Size([self.slate_size])\n",
        "        return output_slate\n",
        "\n",
        "    def compute_target_greedy_q(self, reward, gamma, next_q_values, next_states, terminals):\n",
        "        assert reward.shape == torch.Size([self.batch_size])\n",
        "        assert next_q_values.shape == torch.Size([self.batch_size, self.num_of_candidates])\n",
        "\n",
        "        next_user_obs = next_states[:, :self.user_features]\n",
        "        next_doc_obs = next_states[:, self.user_features:(self.user_features + self.num_of_candidates * self.doc_features)]\n",
        "\n",
        "        assert next_user_obs.shape == torch.Size([self.batch_size, self.user_features])\n",
        "        assert next_doc_obs.shape == torch.Size([self.batch_size, self.num_of_candidates])\n",
        "\n",
        "        next_greedy_q_list = []\n",
        "        for i in range(self.batch_size):\n",
        "            s, s_no_click = self.score_documents_torch(next_user_obs[i], next_doc_obs[i])\n",
        "            q = next_q_values[i]\n",
        "            slate = self.select_slate_greedy(s_no_click, s, q)\n",
        "            p_selected = self.compute_probs_torch(slate, s, s_no_click)\n",
        "            q_selected = torch.gather(q, 0, slate)\n",
        "            next_greedy_q_list.append(\n",
        "                torch.sum(input=p_selected * q_selected)\n",
        "            )\n",
        "\n",
        "        next_greedy_q_values = torch.stack(next_greedy_q_list)\n",
        "        target_q_values = reward + gamma * next_greedy_q_values * (1. - terminals.float())\n",
        "\n",
        "        assert target_q_values.shape == torch.Size([self.batch_size])\n",
        "        return target_q_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeflCtsl8WqV"
      },
      "source": [
        "## Recommendation Environment Implementation\n",
        "\n",
        "This section implements the recommendation environment classes including document and user state handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuIdwAtp8WqV"
      },
      "outputs": [],
      "source": [
        "class LTSDocument(document.AbstractDocument):\n",
        "    def __init__(self, doc_id, kaleness):\n",
        "        self.kaleness = kaleness\n",
        "        super(LTSDocument, self).__init__(doc_id)\n",
        "\n",
        "    def create_observation(self):\n",
        "        return np.array([self.kaleness])\n",
        "\n",
        "    @staticmethod\n",
        "    def observation_space():\n",
        "        return spaces.Box(shape=(1,), dtype=np.float32, low=0.0, high=1.0)\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Document {} with kaleness {}.\".format(self._doc_id, self.kaleness)\n",
        "\n",
        "class LTSDocumentSampler(document.AbstractDocumentSampler):\n",
        "    def __init__(self, seed, doc_ctor=LTSDocument, **kwargs):\n",
        "        super(LTSDocumentSampler, self).__init__(doc_ctor, **kwargs)\n",
        "        self._doc_count = 0\n",
        "        self.seed = seed\n",
        "        self._rng = np.random.RandomState(self.seed)\n",
        "\n",
        "    def sample_document(self):\n",
        "        doc_features = {}\n",
        "        doc_features['doc_id'] = self._doc_count\n",
        "        doc_features['kaleness'] = self._rng.random_sample()\n",
        "        self._doc_count += 1\n",
        "        return self._doc_ctor(**doc_features)\n",
        "\n",
        "class LTSUserState(user.AbstractUserState):\n",
        "    def __init__(self, memory_discount, sensitivity, innovation_stddev,\n",
        "                 choc_mean, choc_stddev, kale_mean, kale_stddev,\n",
        "                 net_kaleness_exposure, time_budget, observation_noise_stddev=0.1):\n",
        "        # Transition model parameters\n",
        "        self.memory_discount = memory_discount\n",
        "        self.sensitivity = sensitivity\n",
        "        self.innovation_stddev = innovation_stddev\n",
        "\n",
        "        # Engagement parameters\n",
        "        self.choc_mean = choc_mean\n",
        "        self.choc_stddev = choc_stddev\n",
        "        self.kale_mean = kale_mean\n",
        "        self.kale_stddev = kale_stddev\n",
        "\n",
        "        # State variables\n",
        "        self.net_kaleness_exposure = net_kaleness_exposure\n",
        "        self.satisfaction = 1 / (1 + np.exp(-sensitivity * net_kaleness_exposure))\n",
        "        self.time_budget = time_budget\n",
        "        self._observation_noise = observation_noise_stddev\n",
        "\n",
        "    def create_observation(self):\n",
        "        \"\"\"User's state is not observable.\"\"\"\n",
        "        clip_low, clip_high = (-1.0 / (1.0 * self._observation_noise),\n",
        "                              1.0 / (1.0 * self._observation_noise))\n",
        "        noise = stats.truncnorm(clip_low, clip_high, loc=0.0,\n",
        "                               scale=self._observation_noise).rvs()\n",
        "        noisy_sat = self.satisfaction + noise\n",
        "        return np.array([noisy_sat, ])\n",
        "\n",
        "    @staticmethod\n",
        "    def observation_space():\n",
        "        return spaces.Box(shape=(1,), dtype=np.float32, low=-2.0, high=2.0)\n",
        "\n",
        "    def score_document(self, doc_obs):\n",
        "        return 1 - doc_obs\n",
        "\n",
        "class LTSStaticUserSampler(user.AbstractUserSampler):\n",
        "    _state_parameters = None\n",
        "\n",
        "    def __init__(self,\n",
        "                 user_ctor=LTSUserState,\n",
        "                 memory_discount=0.9,\n",
        "                 sensitivity=0.01,\n",
        "                 innovation_stddev=0.05,\n",
        "                 choc_mean=5.0,\n",
        "                 choc_stddev=1.0,\n",
        "                 kale_mean=4.0,\n",
        "                 kale_stddev=1.0,\n",
        "                 time_budget=122,\n",
        "                 **kwargs):\n",
        "        self._state_parameters = {\n",
        "            'memory_discount': memory_discount,\n",
        "            'sensitivity': sensitivity,\n",
        "            'innovation_stddev': innovation_stddev,\n",
        "            'choc_mean': choc_mean,\n",
        "            'choc_stddev': choc_stddev,\n",
        "            'kale_mean': kale_mean,\n",
        "            'kale_stddev': kale_stddev,\n",
        "            'time_budget': time_budget\n",
        "        }\n",
        "        super(LTSStaticUserSampler, self).__init__(user_ctor, **kwargs)\n",
        "\n",
        "    def sample_user(self):\n",
        "        starting_nke = ((self._rng.random_sample() - .5) *\n",
        "                        (1 / (1.0 - self._state_parameters['memory_discount'])))\n",
        "        self._state_parameters['net_kaleness_exposure'] = starting_nke\n",
        "        return self._user_ctor(**self._state_parameters)\n",
        "\n",
        "class LTSResponse(user.AbstractResponse):\n",
        "    MAX_ENGAGEMENT_MAGNITUDE = 100.0\n",
        "\n",
        "    def __init__(self, clicked=False, engagement=0.0):\n",
        "        self.clicked = clicked\n",
        "        self.engagement = engagement\n",
        "\n",
        "    def create_observation(self):\n",
        "        return {'click': int(self.clicked), 'engagement': np.array(self.engagement)}\n",
        "\n",
        "    @classmethod\n",
        "    def response_space(cls):\n",
        "        return spaces.Dict({\n",
        "            'click': spaces.Discrete(2),\n",
        "            'engagement': spaces.Box(\n",
        "                low=0.0,\n",
        "                high=cls.MAX_ENGAGEMENT_MAGNITUDE,\n",
        "                shape=tuple(),\n",
        "                dtype=np.float32)\n",
        "        })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rpO5keQ8WqV"
      },
      "source": [
        "## RecSim Environment and User Model Implementation\n",
        "\n",
        "This section implements the main recommendation environment and user model classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDu2ugKz8WqW"
      },
      "outputs": [],
      "source": [
        "def user_init(self, slate_size, seed=0):\n",
        "    super(LTSUserModel, self).__init__(LTSResponse,\n",
        "                                       LTSStaticUserSampler(LTSUserState, seed=seed),\n",
        "                                       slate_size)\n",
        "    self.choice_model = MultinomialLogitChoiceModel({})\n",
        "\n",
        "def simulate_response(self, slate_documents):\n",
        "    # List of empty responses\n",
        "    responses = [self._response_model_ctor() for _ in slate_documents]\n",
        "\n",
        "    # Get click from choice model\n",
        "    self.choice_model.score_documents(\n",
        "        self._user_state, [doc.create_observation() for doc in slate_documents]\n",
        "    )\n",
        "    scores = self.choice_model.scores\n",
        "    selected_index = self.choice_model.choose_item()\n",
        "\n",
        "    # Populate clicked item\n",
        "    self._generate_response(slate_documents[selected_index],\n",
        "                           responses[selected_index])\n",
        "    return responses\n",
        "\n",
        "def generate_response(self, doc, response):\n",
        "    response.clicked = True\n",
        "    # linear interpolation between choc and kale\n",
        "    engagement_loc = (doc.kaleness * self._user_state.choc_mean\n",
        "                     + (1 - doc.kaleness) * self._user_state.kale_mean)\n",
        "    engagement_loc *= self._user_state.satisfaction\n",
        "    engagement_scale = (doc.kaleness * self._user_state.choc_stddev\n",
        "                       + ((1 - doc.kaleness)\n",
        "                          * self._user_state.kale_stddev))\n",
        "    log_engagement = np.random.normal(loc=engagement_loc,\n",
        "                                     scale=engagement_scale)\n",
        "    response.engagement = np.exp(log_engagement)\n",
        "\n",
        "def update_state(self, slate_documents, responses):\n",
        "    for doc, response in zip(slate_documents, responses):\n",
        "        if response.clicked:\n",
        "            innovation = np.random.normal(scale=self._user_state.innovation_stddev)\n",
        "            net_kaleness_exposure = (self._user_state.memory_discount\n",
        "                                    * self._user_state.net_kaleness_exposure\n",
        "                                    - 2.0 * (doc.kaleness - 0.5)\n",
        "                                    + innovation\n",
        "                                    )\n",
        "            self._user_state.net_kaleness_exposure = net_kaleness_exposure\n",
        "            satisfaction = 1 / (1.0 + np.exp(-self._user_state.sensitivity\n",
        "                                            * net_kaleness_exposure\n",
        "                                            ))\n",
        "            self._user_state.satisfaction = satisfaction\n",
        "            self._user_state.time_budget -= 1\n",
        "            return\n",
        "\n",
        "def is_terminal(self):\n",
        "    \"\"\"Returns a boolean indicating if the session is over.\"\"\"\n",
        "    return self._user_state.time_budget <= 0\n",
        "\n",
        "def clicked_engagement_reward(responses):\n",
        "    reward = 0.0\n",
        "    for response in responses:\n",
        "        if response.clicked:\n",
        "            reward += response.engagement\n",
        "    return reward\n",
        "\n",
        "LTSUserModel = type(\"LTSUserModel\", (user.AbstractUserModel,),\n",
        "                    {\"__init__\": user_init,\n",
        "                     \"is_terminal\": is_terminal,\n",
        "                     \"update_state\": update_state,\n",
        "                     \"simulate_response\": simulate_response,\n",
        "                     \"_generate_response\": generate_response})\n",
        "\n",
        "class RecsimEnv():\n",
        "    def __init__(self, num_candidates, slate_size, resample_documents, env_seed_0, env_seed_1):\n",
        "        assert num_candidates >= slate_size\n",
        "        self.num_candidates = num_candidates\n",
        "        self.slate_size = slate_size\n",
        "        self.resample_documents = resample_documents\n",
        "\n",
        "        # Document models\n",
        "        self.doc_model_1 = LTSDocumentSampler(env_seed_0)\n",
        "        self.doc_model_2 = LTSDocumentSampler(env_seed_1)\n",
        "\n",
        "        # User model\n",
        "        self.user_model = LTSUserModel(slate_size)\n",
        "\n",
        "        # Environments\n",
        "        self.env_0 = environment.Environment(\n",
        "            self.user_model,\n",
        "            self.doc_model_1,\n",
        "            num_candidates,\n",
        "            slate_size,\n",
        "            resample_documents,\n",
        "        )\n",
        "\n",
        "        self.env_1 = environment.Environment(\n",
        "            self.user_model,\n",
        "            self.doc_model_2,\n",
        "            num_candidates,\n",
        "            slate_size,\n",
        "            resample_documents)\n",
        "\n",
        "        self.lts_gym_env_0 = recsim_gym.RecSimGymEnv(self.env_0, clicked_engagement_reward)\n",
        "        self.lts_gym_env_1 = recsim_gym.RecSimGymEnv(self.env_1, clicked_engagement_reward)\n",
        "\n",
        "        self.lambda_attack = 0.1\n",
        "\n",
        "    def env_ini(self, id):\n",
        "        if id == 0:\n",
        "            output = self.lts_gym_env_0.reset()\n",
        "        elif id == 1:\n",
        "            output = self.lts_gym_env_1.reset()\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid id {id}.\")\n",
        "\n",
        "        user, doc, response = output.values()\n",
        "        doc_id = np.array(list(doc.keys())).astype(int)\n",
        "        doc_fea = np.array(list(doc.values()))\n",
        "        click = np.zeros([self.slate_size], dtype=int)\n",
        "        engagement = np.zeros([self.slate_size])\n",
        "        reward = np.array(0.)\n",
        "        done = False\n",
        "\n",
        "        return user.astype(np.float32), doc_id.astype(np.float32), doc_fea.astype(np.float32), \\\n",
        "               click.astype(np.float32), engagement.astype(np.float32), reward.astype(np.float32), done\n",
        "\n",
        "    def compute_reward(self, responses, agent_q_values, clean_q_values):\n",
        "        # Compute expected user engagement (sum of engagement values)\n",
        "        engagement_reward = sum([response['engagement'] for response in responses])\n",
        "\n",
        "        # Compute attack penalty\n",
        "        attack_penalty = self.lambda_attack * torch.norm(agent_q_values - clean_q_values, p=2).item()\n",
        "\n",
        "        # Total reward\n",
        "        total_reward = engagement_reward - attack_penalty\n",
        "\n",
        "        return total_reward\n",
        "\n",
        "    def env_step(self, slate, id, agent_q_values=None, clean_q_values=None):\n",
        "        if id == 0:\n",
        "            output = self.lts_gym_env_0.step(slate)\n",
        "        elif id == 1:\n",
        "            output = self.lts_gym_env_1.step(slate)\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid id {id}.\")\n",
        "\n",
        "        user, doc, response = output[0].values()\n",
        "        doc_id = np.array(list(doc.keys())).astype(int)\n",
        "        doc_fea = np.array(list(doc.values()))\n",
        "        click = np.array(list(item['click'] for item in response))\n",
        "        engagement = np.array(list(item['engagement'] for item in response))\n",
        "        done = output[2]\n",
        "\n",
        "        # Compute the reward using the new reward function\n",
        "        if agent_q_values is not None and clean_q_values is not None:\n",
        "            reward = self.compute_reward(response, agent_q_values, clean_q_values)\n",
        "        else:\n",
        "            reward = np.array(output[1])\n",
        "\n",
        "        return user.astype(np.float32), doc_id.astype(np.float32), doc_fea.astype(np.float32), \\\n",
        "               click.astype(np.float32), engagement.astype(np.float32), reward.astype(np.float32), done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C79U_lMq8WqW"
      },
      "source": [
        "## Logger Implementation\n",
        "\n",
        "This section implements the logging functionality to track training progress and metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9xrtapw8WqW"
      },
      "outputs": [],
      "source": [
        "class Logger:\n",
        "    def __init__(self, save_dir):\n",
        "        self.save_log = save_dir / \"log.txt\"\n",
        "\n",
        "        # Episode statistics\n",
        "        self.ep_rewards_alpha = []\n",
        "        self.ep_rewards_alpha_adv = []\n",
        "        self.ep_rewards_beta = []\n",
        "        self.ep_rewards_beta_adv = []\n",
        "        self.ep_lengths = []\n",
        "        self.ep_avg_losses_alpha = []\n",
        "        self.ep_avg_losses_alpha_adv = []\n",
        "        self.ep_avg_qs_alpha = []\n",
        "        self.ep_avg_qs_alpha_adv = []\n",
        "        self.ep_avg_losses_beta = []\n",
        "        self.ep_avg_losses_beta_adv = []\n",
        "        self.ep_avg_qs_beta = []\n",
        "        self.ep_avg_qs_beta_adv = []\n",
        "\n",
        "        # Moving averages\n",
        "        self.moving_avg_ep_rewards_alpha = []\n",
        "        self.moving_avg_ep_rewards_alpha_adv = []\n",
        "        self.moving_avg_ep_rewards_beta = []\n",
        "        self.moving_avg_ep_rewards_beta_adv = []\n",
        "        self.moving_avg_ep_lengths = []\n",
        "\n",
        "        # Initialize current episode stats\n",
        "        self.init_episode()\n",
        "\n",
        "        # Timing\n",
        "        self.record_time = time.time()\n",
        "\n",
        "    def init_episode(self):\n",
        "        \"\"\"Reset statistics for the current episode.\"\"\"\n",
        "        self.curr_ep_reward_alpha = 0.0\n",
        "        self.curr_ep_reward_alpha_adv = 0.0\n",
        "        self.curr_ep_reward_beta = 0.0\n",
        "        self.curr_ep_reward_beta_adv = 0.0\n",
        "        self.curr_ep_length = 0\n",
        "\n",
        "        # Clean state metrics\n",
        "        self.curr_ep_loss_alpha = 0.0\n",
        "        self.curr_ep_q_alpha = 0.0\n",
        "        self.curr_ep_loss_length_alpha = 0\n",
        "        self.curr_ep_loss_beta = 0.0\n",
        "        self.curr_ep_q_beta = 0.0\n",
        "        self.curr_ep_loss_length_beta = 0\n",
        "\n",
        "        # Adversarial state metrics\n",
        "        self.curr_ep_loss_alpha_adv = 0.0\n",
        "        self.curr_ep_q_alpha_adv = 0.0\n",
        "        self.curr_ep_loss_length_alpha_adv = 0\n",
        "        self.curr_ep_loss_beta_adv = 0.0\n",
        "        self.curr_ep_q_beta_adv = 0.0\n",
        "        self.curr_ep_loss_length_beta_adv = 0\n",
        "\n",
        "    def log_step(self, total_reward_alpha, total_reward_alpha_adv, loss_alpha_clean, loss_alpha_adv,\n",
        "                 q_alpha_clean, q_alpha_adv, reward_beta_clean, reward_beta_adv,\n",
        "                 loss_beta_clean, loss_beta_adv, q_beta_clean, q_beta_adv):\n",
        "        \"\"\"Log step-level metrics for both clean and adversarial states.\"\"\"\n",
        "        # Regular rewards\n",
        "        if total_reward_alpha is not None:\n",
        "            self.curr_ep_reward_alpha += total_reward_alpha\n",
        "        if reward_beta_clean is not None:\n",
        "            self.curr_ep_reward_beta += reward_beta_clean\n",
        "\n",
        "        # Adversarial rewards\n",
        "        if total_reward_alpha_adv is not None:\n",
        "            self.curr_ep_reward_alpha_adv += total_reward_alpha_adv\n",
        "        if reward_beta_adv is not None:\n",
        "            self.curr_ep_reward_beta_adv += reward_beta_adv\n",
        "\n",
        "        # Increment episode length\n",
        "        self.curr_ep_length += 1\n",
        "\n",
        "        # Regular Q-values and losses\n",
        "        if loss_alpha_clean is not None:\n",
        "            self.curr_ep_loss_alpha += loss_alpha_clean\n",
        "            self.curr_ep_q_alpha += q_alpha_clean\n",
        "            self.curr_ep_loss_length_alpha += 1\n",
        "        if loss_beta_clean is not None:\n",
        "            self.curr_ep_loss_beta += loss_beta_clean\n",
        "            self.curr_ep_q_beta += q_beta_clean\n",
        "            self.curr_ep_loss_length_beta += 1\n",
        "\n",
        "        # Adversarial Q-values and losses\n",
        "        if loss_alpha_adv is not None:\n",
        "            self.curr_ep_loss_alpha_adv += loss_alpha_adv\n",
        "            self.curr_ep_q_alpha_adv += q_alpha_adv\n",
        "            self.curr_ep_loss_length_alpha_adv += 1\n",
        "        if loss_beta_adv is not None:\n",
        "            self.curr_ep_loss_beta_adv += loss_beta_adv\n",
        "            self.curr_ep_q_beta_adv += q_beta_adv\n",
        "            self.curr_ep_loss_length_beta_adv += 1\n",
        "\n",
        "    def log_episode(self):\n",
        "        \"\"\"Aggregate episode-level statistics.\"\"\"\n",
        "        self.ep_rewards_alpha.append(self.curr_ep_reward_alpha)\n",
        "        self.ep_rewards_alpha_adv.append(self.curr_ep_reward_alpha_adv)\n",
        "        self.ep_rewards_beta.append(self.curr_ep_reward_beta)\n",
        "        self.ep_rewards_beta_adv.append(self.curr_ep_reward_beta_adv)\n",
        "        self.ep_lengths.append(self.curr_ep_length)\n",
        "\n",
        "        # Moving averages for rewards and lengths\n",
        "        self.moving_avg_ep_rewards_alpha.append(np.round(np.mean(self.ep_rewards_alpha[-100:]), 3))\n",
        "        self.moving_avg_ep_rewards_alpha_adv.append(np.round(np.mean(self.ep_rewards_alpha_adv[-100:]), 3))\n",
        "        self.moving_avg_ep_rewards_beta.append(np.round(np.mean(self.ep_rewards_beta[-100:]), 3))\n",
        "        self.moving_avg_ep_rewards_beta_adv.append(np.round(np.mean(self.ep_rewards_beta_adv[-100:]), 3))\n",
        "        self.moving_avg_ep_lengths.append(np.round(np.mean(self.ep_lengths[-100:]), 3))\n",
        "\n",
        "        # Average clean losses and Q-values\n",
        "        avg_loss_alpha = (self.curr_ep_loss_alpha / self.curr_ep_loss_length_alpha\n",
        "                          if self.curr_ep_loss_length_alpha > 0 else 0)\n",
        "        avg_q_alpha = (self.curr_ep_q_alpha / self.curr_ep_loss_length_alpha\n",
        "                       if self.curr_ep_loss_length_alpha > 0 else 0)\n",
        "        avg_loss_beta = (self.curr_ep_loss_beta / self.curr_ep_loss_length_beta\n",
        "                         if self.curr_ep_loss_length_beta > 0 else 0)\n",
        "        avg_q_beta = (self.curr_ep_q_beta / self.curr_ep_loss_length_beta\n",
        "                      if self.curr_ep_loss_length_beta > 0 else 0)\n",
        "\n",
        "        self.ep_avg_losses_alpha.append(avg_loss_alpha)\n",
        "        self.ep_avg_qs_alpha.append(avg_q_alpha)\n",
        "        self.ep_avg_losses_beta.append(avg_loss_beta)\n",
        "        self.ep_avg_qs_beta.append(avg_q_beta)\n",
        "\n",
        "        # Average adversarial losses and Q-values\n",
        "        avg_loss_alpha_adv = (self.curr_ep_loss_alpha_adv / self.curr_ep_loss_length_alpha_adv\n",
        "                              if self.curr_ep_loss_length_alpha_adv > 0 else 0)\n",
        "        avg_q_alpha_adv = (self.curr_ep_q_alpha_adv / self.curr_ep_loss_length_alpha_adv\n",
        "                           if self.curr_ep_loss_length_alpha_adv > 0 else 0)\n",
        "        avg_loss_beta_adv = (self.curr_ep_loss_beta_adv / self.curr_ep_loss_length_beta_adv\n",
        "                             if self.curr_ep_loss_length_beta_adv > 0 else 0)\n",
        "        avg_q_beta_adv = (self.curr_ep_q_beta_adv / self.curr_ep_loss_length_beta_adv\n",
        "                          if self.curr_ep_loss_length_beta_adv > 0 else 0)\n",
        "\n",
        "        self.ep_avg_losses_alpha_adv.append(avg_loss_alpha_adv)\n",
        "        self.ep_avg_qs_alpha_adv.append(avg_q_alpha_adv)\n",
        "        self.ep_avg_losses_beta_adv.append(avg_loss_beta_adv)\n",
        "        self.ep_avg_qs_beta_adv.append(avg_q_beta_adv)\n",
        "\n",
        "        # Reset episode stats\n",
        "        self.init_episode()\n",
        "\n",
        "    def record(self, episode, epsilon, step):\n",
        "        \"\"\"Print and save episode-level metrics to the log.\"\"\"\n",
        "        # Retrieve moving averages for clean and adversarial metrics\n",
        "        mean_length = self.moving_avg_ep_lengths[-1]\n",
        "        mean_reward_alpha = self.moving_avg_ep_rewards_alpha[-1]\n",
        "        mean_reward_alpha_adv = self.moving_avg_ep_rewards_alpha_adv[-1]\n",
        "        mean_reward_beta = self.moving_avg_ep_rewards_beta[-1]\n",
        "        mean_reward_beta_adv = self.moving_avg_ep_rewards_beta_adv[-1]\n",
        "\n",
        "        # Clean losses and Q-values\n",
        "        mean_loss_alpha = (np.round(np.mean(self.ep_avg_losses_alpha[-100:]), 3)\n",
        "                          if len(self.ep_avg_losses_alpha) >= 100 else 0)\n",
        "        mean_q_alpha = (np.round(np.mean(self.ep_avg_qs_alpha[-100:]), 3)\n",
        "                        if len(self.ep_avg_qs_alpha) >= 100 else 0)\n",
        "        mean_loss_beta = (np.round(np.mean(self.ep_avg_losses_beta[-100:]), 3)\n",
        "                          if len(self.ep_avg_losses_beta) >= 100 else 0)\n",
        "        mean_q_beta = (np.round(np.mean(self.ep_avg_qs_beta[-100:]), 3)\n",
        "                      if len(self.ep_avg_qs_beta) >= 100 else 0)\n",
        "\n",
        "        # Adversarial losses and Q-values\n",
        "        mean_loss_alpha_adv = (np.round(np.mean(self.ep_avg_losses_alpha_adv[-100:]), 3)\n",
        "                              if len(self.ep_avg_losses_alpha_adv) >= 100 else 0)\n",
        "        mean_q_alpha_adv = (np.round(np.mean(self.ep_avg_qs_alpha_adv[-100:]), 3)\n",
        "                            if len(self.ep_avg_qs_alpha_adv) >= 100 else 0)\n",
        "        mean_loss_beta_adv = (np.round(np.mean(self.ep_avg_losses_beta_adv[-100:]), 3)\n",
        "                              if len(self.ep_avg_losses_beta_adv) >= 100 else 0)\n",
        "        mean_q_beta_adv = (np.round(np.mean(self.ep_avg_qs_beta_adv[-100:]), 3)\n",
        "                          if len(self.ep_avg_qs_beta_adv) >= 100 else 0)\n",
        "\n",
        "        # Calculate time since last record\n",
        "        time_since_last_record = np.round(time.time() - self.record_time, 3)\n",
        "        self.record_time = time.time()\n",
        "\n",
        "        # Print episode-level summary\n",
        "        print(\n",
        "            f\"Episode {episode} | Step {step} | Epsilon {epsilon:.3f} | \"\n",
        "            f\"Clean Rewards: Alpha {mean_reward_alpha}, Beta {mean_reward_beta} | \"\n",
        "            f\"Adversarial Rewards: Alpha {mean_reward_alpha_adv}, Beta {mean_reward_beta_adv} | \"\n",
        "            f\"Clean Losses: Alpha {mean_loss_alpha}, Beta {mean_loss_beta} | \"\n",
        "            f\"Adversarial Losses: Alpha {mean_loss_alpha_adv}, Beta {mean_loss_beta_adv} | \"\n",
        "            f\"Clean Qs: Alpha {mean_q_alpha}, Beta {mean_q_beta} | \"\n",
        "            f\"Adversarial Qs: Alpha {mean_q_alpha_adv}, Beta {mean_q_beta_adv} | \"\n",
        "            f\"Avg Episode Length: {mean_length} | \"\n",
        "            f\"Time Delta: {time_since_last_record}s\"\n",
        "        )\n",
        "\n",
        "        # Save to log file\n",
        "        with open(self.save_log, \"a\") as f:\n",
        "            f.write(\n",
        "                f\"{episode:8d} | {step:8d} | {epsilon:10.3f} | \"\n",
        "                f\"{mean_reward_alpha:15.3f} | {mean_reward_beta:15.3f} | \"\n",
        "                f\"{mean_reward_alpha_adv:15.3f} | {mean_reward_beta_adv:15.3f} | \"\n",
        "                f\"{mean_loss_alpha:15.3f} | {mean_loss_beta:15.3f} | \"\n",
        "                f\"{mean_loss_alpha_adv:15.3f} | {mean_loss_beta_adv:15.3f} | \"\n",
        "                f\"{mean_q_alpha:15.3f} | {mean_q_beta:15.3f} | \"\n",
        "                f\"{mean_q_alpha_adv:15.3f} | {mean_q_beta_adv:15.3f} | \"\n",
        "                f\"{mean_length:10.3f} | {time_since_last_record:10.3f}\\n\"\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJuxEfXV8WqY"
      },
      "source": [
        "## Agents Implementation\n",
        "\n",
        "This section implements the core agent classes for the federated recommendation system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMN-E-xD8WqY"
      },
      "outputs": [],
      "source": [
        "class AgentAlpha(SlateQ):\n",
        "    def __init__(self, user_features, doc_features, num_of_candidates, slate_size, batch_size, num_contex,\n",
        "                 capacity=2000):\n",
        "        self.user_features = user_features\n",
        "        self.doc_features = doc_features\n",
        "        self.num_of_candidates = num_of_candidates\n",
        "        self.slate_size = slate_size\n",
        "        self.batch_size = batch_size\n",
        "        self.num_contex = num_contex\n",
        "\n",
        "        # Original state dimension\n",
        "        self.state_dim = user_features + (\n",
        "            doc_features * num_of_candidates + num_of_candidates) + num_contex * slate_size\n",
        "\n",
        "        self.action_dim = slate_size\n",
        "\n",
        "        self.response = deque(maxlen=num_contex)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.net = QNet(self.state_dim, self.num_of_candidates).to(self.device)\n",
        "        self.replay = ReplayMemory(capacity, (self.state_dim,), (self.action_dim,))\n",
        "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.01)\n",
        "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
        "        self.gamma = 0.9\n",
        "\n",
        "    def compute_q_local_ini(self, env):\n",
        "        for _ in range(self.num_contex):\n",
        "            self.response.append(torch.zeros([self.slate_size], device=self.device))\n",
        "\n",
        "        user, doc_id, doc_fea, click, engagement, reward, done = env.env_ini(0)\n",
        "        state = torch.cat([\n",
        "            torch.tensor(user, device=self.device).view(-1),\n",
        "            torch.tensor(doc_fea, device=self.device).view(-1),\n",
        "            torch.tensor(doc_id, device=self.device).to(torch.float).view(-1)\n",
        "        ])\n",
        "\n",
        "        for responses in self.response:\n",
        "            state = torch.cat([state, responses.squeeze()])\n",
        "\n",
        "        assert state.shape == torch.Size([self.state_dim])\n",
        "        self.state = state\n",
        "        return self.net(state, \"online\")\n",
        "\n",
        "    def compute_q_local(self):\n",
        "        return self.net.forward(self.state.view(1, -1), \"online\")\n",
        "\n",
        "    def recommend(self, q_fed_alpha, env):\n",
        "        user_obs = self.state[:self.user_features]\n",
        "        doc_obs = self.state[self.user_features:(self.user_features + self.num_of_candidates * self.doc_features)]\n",
        "        s, s_no_click = super().score_documents_torch(user_obs, doc_obs)\n",
        "        slate = super().select_slate_greedy(s_no_click, s, q_fed_alpha)\n",
        "\n",
        "        # Pass agent_q_values and clean_q_values to env_step\n",
        "        clean_q_values = self.net(self.state.view(1, -1), \"online\").detach()\n",
        "        agent_q_values = q_fed_alpha\n",
        "\n",
        "        user, doc_id, doc_fea, click, engagement, reward, done = env.env_step(\n",
        "            slate.cpu().numpy().tolist(), 0, agent_q_values, clean_q_values)\n",
        "\n",
        "        self.response.append(torch.tensor(engagement, device=self.device))\n",
        "\n",
        "        next_state = torch.cat([\n",
        "            torch.tensor(user, device=self.device).view(-1),\n",
        "            torch.tensor(doc_fea, device=self.device).view(-1),\n",
        "            torch.tensor(doc_id, device=self.device).to(torch.float).view(-1)\n",
        "        ])\n",
        "\n",
        "        for responses in self.response:\n",
        "            next_state = torch.cat([next_state, responses.view(-1)])\n",
        "\n",
        "        self.replay.push(self.state.view(-1), slate.view(-1),\n",
        "                         torch.tensor(reward, device=self.device).view(-1),\n",
        "                         torch.tensor(click, device=self.device),\n",
        "                         next_state.squeeze(),\n",
        "                         torch.tensor(done, device=self.device).view(-1))\n",
        "\n",
        "        self.state = next_state\n",
        "        return done, reward\n",
        "\n",
        "    def recommend_random(self, env):\n",
        "        nums = list(range(self.num_of_candidates))\n",
        "        random.shuffle(nums)\n",
        "        slate = nums[:self.slate_size]\n",
        "\n",
        "        # Since this is a random recommendation, agent_q_values and clean_q_values are None\n",
        "        user, doc_id, doc_fea, click, engagement, reward, done = env.env_step(slate, 0)\n",
        "\n",
        "        self.response.append(torch.tensor(engagement, device=self.device))\n",
        "\n",
        "        next_state = torch.cat([\n",
        "            torch.tensor(user, device=self.device).view(-1),\n",
        "            torch.tensor(doc_fea, device=self.device).view(-1),\n",
        "            torch.tensor(doc_id, device=self.device).to(torch.float).view(-1)\n",
        "        ])\n",
        "\n",
        "        for responses in self.response:\n",
        "            next_state = torch.cat([next_state, responses.view(-1)])\n",
        "\n",
        "        self.replay.push(self.state.squeeze(),\n",
        "                         torch.tensor(slate, device=self.device),\n",
        "                         torch.tensor(reward, device=self.device).view(1),\n",
        "                         torch.tensor(click, device=self.device),\n",
        "                         next_state.squeeze(),\n",
        "                         torch.tensor(done, device=self.device).view(1))\n",
        "\n",
        "        self.state = next_state\n",
        "        return done, reward\n",
        "\n",
        "    def compute_q_local_batch(self, ids):\n",
        "        self.batch_states, self.batch_actions, self.batch_rewards, self.batch_clicks, \\\n",
        "        self.batch_next_states, self.batch_terminals = self.replay.recall(ids)\n",
        "        return self.net.forward(self.batch_states, \"online\"), self.net.forward(self.batch_next_states, \"target\")\n",
        "\n",
        "    def update_q_net(self, q, q_next, agent_fed):\n",
        "        assert q.shape == torch.Size([self.batch_size, self.num_of_candidates])\n",
        "        assert q_next.shape == torch.Size([self.batch_size, self.num_of_candidates])\n",
        "\n",
        "        doc_id_start = self.user_features + self.num_of_candidates * self.doc_features\n",
        "        doc_id_end = doc_id_start + self.num_of_candidates\n",
        "\n",
        "        doc_id = self.batch_states[:, doc_id_start:doc_id_end]\n",
        "\n",
        "        assert doc_id.shape == torch.Size([self.batch_size, self.num_of_candidates])\n",
        "\n",
        "        selected_item = self.batch_actions * self.batch_clicks\n",
        "        selected_item = selected_item.type(torch.int)\n",
        "        assert selected_item.shape == torch.Size([self.batch_size, self.slate_size])\n",
        "        selected_item = torch.sum(selected_item, dim=1, keepdim=True)\n",
        "\n",
        "        q = torch.gather(q, 1, selected_item)\n",
        "        q_next = super().compute_target_greedy_q(self.batch_rewards, self.gamma, q_next,\n",
        "                                                 self.batch_next_states, self.batch_terminals)\n",
        "\n",
        "        loss = self.loss_fn(q.view(self.batch_size, 1), q_next.view(self.batch_size, 1))\n",
        "        agent_fed.optimizer.zero_grad()\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward(retain_graph=True)\n",
        "        agent_fed.optimizer.step()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss, q_next\n",
        "\n",
        "class AgentBeta(SlateQ):\n",
        "    def __init__(self, user_features, doc_features, num_of_candidates, slate_size, batch_size, capacity=2000):\n",
        "        self.user_features = user_features\n",
        "        self.doc_features = doc_features\n",
        "        self.num_of_candidates = num_of_candidates\n",
        "        self.slate_size = slate_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Original state dimension\n",
        "        self.state_dim = user_features + (doc_features * num_of_candidates + num_of_candidates)\n",
        "\n",
        "        self.action_dim = slate_size\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.net = QNet(self.state_dim, self.num_of_candidates).to(self.device)\n",
        "        self.replay = ReplayMemory(capacity, (self.state_dim,), (self.action_dim,))\n",
        "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.01)\n",
        "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
        "        self.gamma = 0.9\n",
        "\n",
        "    def compute_q_local_ini(self, env):\n",
        "        user, doc_id, doc_fea, click, engagement, reward, done = env.env_ini(1)\n",
        "        state = torch.cat([\n",
        "            torch.tensor(user, device=self.device).view(-1),\n",
        "            torch.tensor(doc_fea, device=self.device).view(-1),\n",
        "            torch.tensor(doc_id, device=self.device).to(torch.float).view(-1)\n",
        "        ])\n",
        "\n",
        "        assert state.shape == torch.Size([self.state_dim])\n",
        "        self.state = state\n",
        "        return self.net(state, \"online\")\n",
        "\n",
        "    def compute_q_local(self):\n",
        "        return self.net.forward(self.state.view(1, -1), \"online\")\n",
        "\n",
        "    def recommend(self, q_fed_beta, env):\n",
        "        user_obs = self.state[:self.user_features]\n",
        "        doc_obs = self.state[self.user_features:(self.user_features + self.num_of_candidates * self.doc_features)]\n",
        "        s, s_no_click = super().score_documents_torch(user_obs, doc_obs)\n",
        "        slate = super().select_slate_greedy(s_no_click, s, q_fed_beta)\n",
        "\n",
        "        # Pass agent_q_values and clean_q_values to env_step\n",
        "        clean_q_values = self.net(self.state.view(1, -1), \"online\").detach()\n",
        "        agent_q_values = q_fed_beta\n",
        "\n",
        "        user, doc_id, doc_fea, click, engagement, reward, done = env.env_step(\n",
        "            slate.cpu().numpy().tolist(), 1, agent_q_values, clean_q_values)\n",
        "\n",
        "        next_state = torch.cat([\n",
        "            torch.tensor(user, device=self.device).view(-1),\n",
        "            torch.tensor(doc_fea, device=self.device).view(-1),\n",
        "            torch.tensor(doc_id, device=self.device).to(torch.float).view(-1)\n",
        "        ])\n",
        "\n",
        "        self.replay.push(self.state.view(-1), slate.view(-1),\n",
        "                         torch.tensor(reward, device=self.device).view(-1),\n",
        "                         torch.tensor(click, device=self.device),\n",
        "                         next_state.squeeze(),\n",
        "                         torch.tensor(done, device=self.device).view(-1))\n",
        "\n",
        "        self.state = next_state\n",
        "        return done, reward\n",
        "\n",
        "    def recommend_random(self, env):\n",
        "        nums = list(range(self.num_of_candidates))\n",
        "        random.shuffle(nums)\n",
        "        slate = nums[:self.slate_size]\n",
        "\n",
        "        # Since this is a random recommendation, agent_q_values and clean_q_values are None\n",
        "        user, doc_id, doc_fea, click, engagement, reward, done = env.env_step(slate, 1)\n",
        "\n",
        "        next_state = torch.cat([\n",
        "            torch.tensor(user, device=self.device).view(-1),\n",
        "            torch.tensor(doc_fea, device=self.device).view(-1),\n",
        "            torch.tensor(doc_id, device=self.device).to(torch.float).view(-1)\n",
        "        ])\n",
        "\n",
        "        self.replay.push(self.state.squeeze(),\n",
        "                         torch.tensor(slate, device=self.device),\n",
        "                         torch.tensor(reward, device=self.device).view(1),\n",
        "                         torch.tensor(click, device=self.device),\n",
        "                         next_state.squeeze(),\n",
        "                         torch.tensor(done, device=self.device).view(1))\n",
        "\n",
        "        self.state = next_state\n",
        "        return done, reward\n",
        "\n",
        "    def compute_q_local_batch(self, ids):\n",
        "        self.batch_states, self.batch_actions, _, _, _, _ = self.replay.recall(ids)\n",
        "        return self.net.forward(self.batch_states, \"online\")\n",
        "\n",
        "    def update_q_net(self, q_online, q_target, agent_fed):\n",
        "        assert q_online.shape == torch.Size([self.batch_size, self.num_of_candidates])\n",
        "\n",
        "        # Ensure target shape matches reduced output\n",
        "        q_target = q_target.mean(dim=1, keepdim=True)  # Reduce target to match greedy Q-values\n",
        "\n",
        "        user_obs = self.batch_states[:, :self.user_features]\n",
        "        doc_obs_start = self.user_features\n",
        "        doc_obs_end = doc_obs_start + self.num_of_candidates * self.doc_features\n",
        "        doc_obs = self.batch_states[:, doc_obs_start:doc_obs_end]\n",
        "\n",
        "        assert user_obs.shape == torch.Size([self.batch_size, self.user_features])\n",
        "        assert doc_obs.shape == torch.Size([self.batch_size, self.num_of_candidates])\n",
        "\n",
        "        greedy_q_list = []\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            s, s_no_click = super().score_documents_torch(user_obs[i], doc_obs[i])\n",
        "            q = q_online[i]\n",
        "\n",
        "            # Select a slate of 3 items\n",
        "            slate = super().select_slate_greedy(s_no_click, s, q)\n",
        "            assert slate.shape[0] == self.slate_size  # Ensure slate size is 3\n",
        "\n",
        "            # Gather Q-values for the selected candidates\n",
        "            q_selected = torch.gather(q, 0, slate)\n",
        "\n",
        "            # Compute probabilities for the selected slate\n",
        "            p_selected = super().compute_probs_torch(slate, s, s_no_click)\n",
        "\n",
        "            # Aggregate probabilities and Q-values for the slate\n",
        "            greedy_q_value = torch.sum(p_selected * q_selected)\n",
        "            greedy_q_list.append(greedy_q_value)\n",
        "\n",
        "        # Stack into a tensor of shape [batch_size, 1]\n",
        "        greedy_q_values = torch.stack(greedy_q_list, dim=0).view(self.batch_size, 1)\n",
        "\n",
        "        # Compute loss between reduced Q-values and target\n",
        "        loss = self.loss_fn(greedy_q_values, q_target)\n",
        "\n",
        "        # Optimize\n",
        "        agent_fed.optimizer.zero_grad()\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        agent_fed.optimizer.step()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss\n",
        "\n",
        "class AgentFed():\n",
        "    def __init__(self, user_features, doc_features, num_of_candidates, slate_size, batch_size, capacity=2000):\n",
        "        self.user_features = user_features\n",
        "        self.doc_features = doc_features\n",
        "        self.num_of_candidates = num_of_candidates\n",
        "        self.slate_size = slate_size\n",
        "        self.batch_size = batch_size\n",
        "        self.capacity = capacity\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.exploration_rate = 1\n",
        "        self.exploration_rate_decay = 0.99995\n",
        "        self.exploration_rate_min = 0\n",
        "        self.burnin = 5000  # min. experiences before training [change back to 5000]\n",
        "        self.learn_every = 3  # no. of experiences between updates to Q_online\n",
        "        self.sync_every = 500  # no. of experiences between Q_target & Q_online sync\n",
        "        self.curr_step = 0\n",
        "\n",
        "        self.net = MLPNet(num_of_candidates * 2, num_of_candidates).to(self.device)\n",
        "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.01)\n",
        "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
        "\n",
        "        # Initialize attack penalty parameter\n",
        "        self.lambda_attack = 0.1  # Attack penalty weight λ\n",
        "\n",
        "    def sync(self, agent_alpha, agent_beta):\n",
        "        self.net.target.load_state_dict(self.net.online.state_dict())\n",
        "        agent_alpha.net.target.load_state_dict(agent_alpha.net.online.state_dict())\n",
        "        agent_beta.net.target.load_state_dict(agent_beta.net.online.state_dict())\n",
        "\n",
        "    def act_ini(self, agent_alpha, agent_beta, env):\n",
        "        q_alpha = agent_alpha.compute_q_local_ini(env).view(1, -1)\n",
        "        q_beta = agent_beta.compute_q_local_ini(env).view(1, -1)\n",
        "        q_alpha_fed = self.net_forward(q_alpha, q_beta)\n",
        "        q_beta_fed = self.net_forward(q_beta, q_alpha)\n",
        "\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "            done_alpha, reward_alpha = agent_alpha.recommend_random(env)\n",
        "        else:\n",
        "            done_alpha, reward_alpha = agent_alpha.recommend(q_alpha_fed, env)\n",
        "\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "            done_beta, reward_beta = agent_beta.recommend_random(env)\n",
        "        else:\n",
        "            done_beta, reward_beta = agent_beta.recommend(q_beta_fed, env)\n",
        "\n",
        "        # Decrease exploration_rate\n",
        "        self.exploration_rate *= self.exploration_rate_decay\n",
        "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
        "\n",
        "        # Increment step\n",
        "        self.curr_step += 1\n",
        "        return done_alpha, reward_alpha, done_beta, reward_beta\n",
        "\n",
        "    def act(self, agent_alpha, agent_beta, env):\n",
        "        q_alpha = agent_alpha.compute_q_local().view(1, -1)\n",
        "        q_beta = agent_beta.compute_q_local().view(1, -1)\n",
        "        q_alpha_fed = self.net_forward(q_alpha, q_beta)\n",
        "        q_beta_fed = self.net_forward(q_beta, q_alpha)\n",
        "\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "            done_alpha, reward_alpha = agent_alpha.recommend_random(env)\n",
        "        else:\n",
        "            done_alpha, reward_alpha = agent_alpha.recommend(q_alpha_fed, env)\n",
        "\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "            done_beta, reward_beta = agent_beta.recommend_random(env)\n",
        "        else:\n",
        "            done_beta, reward_beta = agent_beta.recommend(q_beta_fed, env)\n",
        "\n",
        "        # Decrease exploration_rate\n",
        "        self.exploration_rate *= self.exploration_rate_decay\n",
        "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
        "\n",
        "        if self.exploration_rate < 0.1:\n",
        "            self.exploration_rate = 0\n",
        "\n",
        "        # Increment step\n",
        "        self.curr_step += 1\n",
        "        return done_alpha, reward_alpha, done_beta, reward_beta\n",
        "\n",
        "    def net_forward(self, q_agent, q_other_agent):\n",
        "        # Concatenate Q-values from both agents\n",
        "        q_concat = torch.cat([q_agent, q_other_agent], dim=1)\n",
        "        # Pass through MLP\n",
        "        q_fed = self.net(q_concat, \"online\")\n",
        "        return q_fed\n",
        "\n",
        "    def learn(self, agent_alpha, agent_beta):\n",
        "        if self.curr_step % self.sync_every == 0:\n",
        "            self.sync(agent_alpha, agent_beta)\n",
        "\n",
        "        if self.curr_step < self.burnin:\n",
        "            return None, None, None, None\n",
        "\n",
        "        if self.curr_step % self.learn_every != 0:\n",
        "            return None, None, None, None\n",
        "\n",
        "        # Prepare data for batch learning\n",
        "        ids = random.sample(range(len(agent_alpha.replay)), self.batch_size)\n",
        "\n",
        "        # Collect batches from agents\n",
        "        batch_q_alpha_online, batch_q_alpha_target = agent_alpha.compute_q_local_batch(ids)\n",
        "        batch_q_beta_online = agent_beta.compute_q_local_batch(ids)\n",
        "\n",
        "        q_alpha_fed_online = self.net_forward(batch_q_alpha_online, batch_q_beta_online)\n",
        "        q_alpha_fed_target = self.net_forward(batch_q_alpha_target, batch_q_beta_online)\n",
        "\n",
        "        # Update Agent Alpha's network\n",
        "        loss_alpha, q_alpha_target = agent_alpha.update_q_net(q_alpha_fed_online, q_alpha_fed_target, self)\n",
        "\n",
        "        # Recompute batch_q_alpha_online after updating Agent Alpha\n",
        "        batch_q_alpha_online_new, _ = agent_alpha.compute_q_local_batch(ids)\n",
        "\n",
        "        q_beta_fed_online = self.net_forward(batch_q_beta_online, batch_q_alpha_online_new)\n",
        "        # Since we don't have a target for beta in this context, we can use q_beta_fed_online as both online and target\n",
        "        loss_beta = agent_beta.update_q_net(q_beta_fed_online, q_beta_fed_online.detach(), self)\n",
        "\n",
        "        return batch_q_alpha_online.detach().cpu().mean().item(), loss_alpha.detach().cpu(), \\\n",
        "               batch_q_beta_online.detach().cpu().mean().item(), loss_beta.detach().cpu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jf6gJPRY8WqX"
      },
      "source": [
        "## Training Setup and Execution\n",
        "\n",
        "This section includes the main training loops and experiment configurations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ibru5by8WqX"
      },
      "source": [
        "### Standard Training Setup (run_fed.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ias3FKUT8WqX",
        "outputId": "b583e42c-7045-48ce-cf88-67addd1cec89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using CUDA: True\n",
            "\n",
            "Episode 0 | Step 61 | Epsilon 0.997 | Clean Rewards: Alpha 924.837, Beta 1157.484 | Adversarial Rewards: Alpha 0.0, Beta 0.0 | Clean Losses: Alpha 0, Beta 0 | Adversarial Losses: Alpha 0, Beta 0 | Clean Qs: Alpha 0, Beta 0 | Adversarial Qs: Alpha 0, Beta 0 | Avg Episode Length: 60.0 | Time Delta: 7.932s\n",
            "Episode 20 | Step 1281 | Epsilon 0.938 | Clean Rewards: Alpha 868.916, Beta 901.451 | Adversarial Rewards: Alpha 0.0, Beta 0.0 | Clean Losses: Alpha 0, Beta 0 | Adversarial Losses: Alpha 0, Beta 0 | Clean Qs: Alpha 0, Beta 0 | Adversarial Qs: Alpha 0, Beta 0 | Avg Episode Length: 60.0 | Time Delta: 6.826s\n",
            "Episode 40 | Step 2501 | Epsilon 0.882 | Clean Rewards: Alpha 900.601, Beta 924.275 | Adversarial Rewards: Alpha 0.0, Beta 0.0 | Clean Losses: Alpha 0, Beta 0 | Adversarial Losses: Alpha 0, Beta 0 | Clean Qs: Alpha 0, Beta 0 | Adversarial Qs: Alpha 0, Beta 0 | Avg Episode Length: 60.0 | Time Delta: 6.57s\n",
            "Episode 60 | Step 3721 | Epsilon 0.830 | Clean Rewards: Alpha 926.029, Beta 930.798 | Adversarial Rewards: Alpha 0.0, Beta 0.0 | Clean Losses: Alpha 0, Beta 0 | Adversarial Losses: Alpha 0, Beta 0 | Clean Qs: Alpha 0, Beta 0 | Adversarial Qs: Alpha 0, Beta 0 | Avg Episode Length: 60.0 | Time Delta: 6.71s\n",
            "Episode 80 | Step 4941 | Epsilon 0.781 | Clean Rewards: Alpha 929.142, Beta 939.423 | Adversarial Rewards: Alpha 0.0, Beta 0.0 | Clean Losses: Alpha 0, Beta 0 | Adversarial Losses: Alpha 0, Beta 0 | Clean Qs: Alpha 0, Beta 0 | Adversarial Qs: Alpha 0, Beta 0 | Avg Episode Length: 60.0 | Time Delta: 6.939s\n",
            "Episode 100 | Step 6161 | Epsilon 0.735 | Clean Rewards: Alpha 922.311, Beta 923.151 | Adversarial Rewards: Alpha 0.0, Beta 0.0 | Clean Losses: Alpha 0.04, Beta 0.04 | Adversarial Losses: Alpha 0.0, Beta 0.0 | Clean Qs: Alpha 3.08, Beta 0.067 | Adversarial Qs: Alpha 0.0, Beta 0.0 | Avg Episode Length: 60.0 | Time Delta: 53.22s\n",
            "Episode 120 | Step 7381 | Epsilon 0.691 | Clean Rewards: Alpha 929.748, Beta 930.305 | Adversarial Rewards: Alpha 0.0, Beta 0.0 | Clean Losses: Alpha 0.08, Beta 0.08 | Adversarial Losses: Alpha 0.0, Beta 0.0 | Clean Qs: Alpha 6.089, Beta 0.138 | Adversarial Qs: Alpha 0.0, Beta 0.0 | Avg Episode Length: 60.0 | Time Delta: 55.26s\n",
            "Episode 140 | Step 8601 | Epsilon 0.650 | Clean Rewards: Alpha 915.58, Beta 921.949 | Adversarial Rewards: Alpha 0.0, Beta 0.0 | Clean Losses: Alpha 0.12, Beta 0.12 | Adversarial Losses: Alpha 0.0, Beta 0.0 | Clean Qs: Alpha 9.071, Beta 0.209 | Adversarial Qs: Alpha 0.0, Beta 0.0 | Avg Episode Length: 60.0 | Time Delta: 55.362s\n",
            "Episode 160 | Step 9821 | Epsilon 0.612 | Clean Rewards: Alpha 905.775, Beta 909.392 | Adversarial Rewards: Alpha 0.0, Beta 0.0 | Clean Losses: Alpha 0.16, Beta 0.16 | Adversarial Losses: Alpha 0.0, Beta 0.0 | Clean Qs: Alpha 11.947, Beta 0.279 | Adversarial Qs: Alpha 0.0, Beta 0.0 | Avg Episode Length: 60.0 | Time Delta: 55.577s\n",
            "Episode 180 | Step 11041 | Epsilon 0.576 | Clean Rewards: Alpha 897.347, Beta 899.9 | Adversarial Rewards: Alpha 0.0, Beta 0.0 | Clean Losses: Alpha 0.2, Beta 0.2 | Adversarial Losses: Alpha 0.0, Beta 0.0 | Clean Qs: Alpha 15.015000343322754, Beta 0.3499999940395355 | Adversarial Qs: Alpha 0.0, Beta 0.0 | Avg Episode Length: 60.0 | Time Delta: 55.685s\n",
            "Episode 200 | Step 12263 | Epsilon 0.542 | Clean Rewards: Alpha 903.193, Beta 901.612 | Adversarial Rewards: Alpha 4.233, Beta 3.674 | Clean Losses: Alpha 0.2, Beta 0.2 | Adversarial Losses: Alpha 0.002, Beta 0.002 | Clean Qs: Alpha 14.980999946594238, Beta 0.3540000021457672 | Adversarial Qs: Alpha 0.153, Beta 0.004 | Avg Episode Length: 60.02 | Time Delta: 56.159s\n",
            "Episode 220 | Step 13523 | Epsilon 0.509 | Clean Rewards: Alpha 812.284, Beta 814.981 | Adversarial Rewards: Alpha 96.699, Beta 89.635 | Clean Losses: Alpha 0.193, Beta 0.2 | Adversarial Losses: Alpha 0.036, Beta 0.042 | Clean Qs: Alpha 14.975000381469727, Beta 0.3540000021457672 | Adversarial Qs: Alpha 3.221, Beta 0.074 | Avg Episode Length: 60.42 | Time Delta: 57.502s\n",
            "Episode 240 | Step 14783 | Epsilon 0.478 | Clean Rewards: Alpha 725.319, Beta 727.206 | Adversarial Rewards: Alpha 189.446, Beta 185.189 | Clean Losses: Alpha 0.176, Beta 0.2 | Adversarial Losses: Alpha 0.058, Beta 0.082 | Clean Qs: Alpha 15.005000114440918, Beta 0.3540000021457672 | Adversarial Qs: Alpha 6.175, Beta 0.145 | Avg Episode Length: 60.82 | Time Delta: 57.985s\n",
            "Episode 260 | Step 16043 | Epsilon 0.448 | Clean Rewards: Alpha 634.524, Beta 645.423 | Adversarial Rewards: Alpha 284.525, Beta 284.952 | Clean Losses: Alpha 0.157, Beta 0.2 | Adversarial Losses: Alpha 0.079, Beta 0.122 | Clean Qs: Alpha 15.196000099182129, Beta 0.3540000021457672 | Adversarial Qs: Alpha 9.167, Beta 0.216 | Avg Episode Length: 61.22 | Time Delta: 57.829s\n",
            "Episode 280 | Step 17303 | Epsilon 0.421 | Clean Rewards: Alpha 543.026, Beta 553.263 | Adversarial Rewards: Alpha 377.212, Beta 376.073 | Clean Losses: Alpha 0.138, Beta 0.2 | Adversarial Losses: Alpha 0.099, Beta 0.162 | Clean Qs: Alpha 15.092000007629395, Beta 0.3540000021457672 | Adversarial Qs: Alpha 12.09, Beta 0.287 | Avg Episode Length: 61.62 | Time Delta: 57.88s\n",
            "Episode 300 | Step 18563 | Epsilon 0.395 | Clean Rewards: Alpha 452.084, Beta 466.371 | Adversarial Rewards: Alpha 463.25, Beta 459.428 | Clean Losses: Alpha 0.119, Beta 0.2 | Adversarial Losses: Alpha 0.118, Beta 0.2 | Clean Qs: Alpha 15.026000022888184, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.781000137329102, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 58.074s\n",
            "Episode 320 | Step 19823 | Epsilon 0.371 | Clean Rewards: Alpha 455.586, Beta 455.345 | Adversarial Rewards: Alpha 466.153, Beta 466.403 | Clean Losses: Alpha 0.106, Beta 0.2 | Adversarial Losses: Alpha 0.105, Beta 0.2 | Clean Qs: Alpha 14.977999687194824, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.717000007629395, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 58.292s\n",
            "Episode 340 | Step 21083 | Epsilon 0.348 | Clean Rewards: Alpha 458.71, Beta 461.175 | Adversarial Rewards: Alpha 458.861, Beta 477.514 | Clean Losses: Alpha 0.105, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.79800033569336, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.60200023651123, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 58.446s\n",
            "Episode 360 | Step 22343 | Epsilon 0.327 | Clean Rewards: Alpha 458.087, Beta 454.071 | Adversarial Rewards: Alpha 460.959, Beta 475.2 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.576000213623047, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.458999633789062, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 58.523s\n",
            "Episode 380 | Step 23603 | Epsilon 0.307 | Clean Rewards: Alpha 465.774, Beta 458.737 | Adversarial Rewards: Alpha 462.849, Beta 471.305 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.536999702453613, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.515000343322754, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 58.548s\n",
            "Episode 400 | Step 24863 | Epsilon 0.288 | Clean Rewards: Alpha 470.013, Beta 470.282 | Adversarial Rewards: Alpha 458.395, Beta 477.413 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.565999984741211, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.685999870300293, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 58.872s\n",
            "Episode 420 | Step 26123 | Epsilon 0.271 | Clean Rewards: Alpha 467.566, Beta 477.556 | Adversarial Rewards: Alpha 450.55, Beta 486.865 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.555000305175781, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.619999885559082, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 58.766s\n",
            "Episode 440 | Step 27383 | Epsilon 0.254 | Clean Rewards: Alpha 466.262, Beta 468.7 | Adversarial Rewards: Alpha 462.216, Beta 472.543 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.5600004196167, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.621000289916992, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 58.732s\n",
            "Episode 460 | Step 28643 | Epsilon 0.239 | Clean Rewards: Alpha 449.044, Beta 483.453 | Adversarial Rewards: Alpha 454.182, Beta 464.714 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.52400016784668, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.609999656677246, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 58.721s\n",
            "Episode 480 | Step 29903 | Epsilon 0.224 | Clean Rewards: Alpha 443.391, Beta 482.86 | Adversarial Rewards: Alpha 454.34, Beta 467.312 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.369000434875488, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.517999649047852, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 58.803s\n",
            "Episode 500 | Step 31163 | Epsilon 0.211 | Clean Rewards: Alpha 437.52, Beta 471.81 | Adversarial Rewards: Alpha 459.901, Beta 467.982 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.178000450134277, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.470000267028809, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 58.847s\n",
            "Episode 520 | Step 32423 | Epsilon 0.198 | Clean Rewards: Alpha 432.312, Beta 471.125 | Adversarial Rewards: Alpha 459.369, Beta 457.126 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.163000106811523, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.409000396728516, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.083s\n",
            "Episode 540 | Step 33683 | Epsilon 0.186 | Clean Rewards: Alpha 436.157, Beta 469.942 | Adversarial Rewards: Alpha 447.316, Beta 458.756 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.107999801635742, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.281000137329102, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.145s\n",
            "Episode 560 | Step 34943 | Epsilon 0.174 | Clean Rewards: Alpha 456.97, Beta 456.737 | Adversarial Rewards: Alpha 445.481, Beta 458.401 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.105, Beta 0.2 | Clean Qs: Alpha 14.053000450134277, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.256999969482422, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.126s\n",
            "Episode 580 | Step 36203 | Epsilon 0.164 | Clean Rewards: Alpha 454.85, Beta 456.82 | Adversarial Rewards: Alpha 442.698, Beta 451.516 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.105, Beta 0.2 | Clean Qs: Alpha 14.269000053405762, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.329000473022461, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.111s\n",
            "Episode 600 | Step 37463 | Epsilon 0.154 | Clean Rewards: Alpha 459.831, Beta 465.807 | Adversarial Rewards: Alpha 448.197, Beta 459.234 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.105, Beta 0.2 | Clean Qs: Alpha 14.414999961853027, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.229000091552734, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.115s\n",
            "Episode 620 | Step 38723 | Epsilon 0.144 | Clean Rewards: Alpha 469.999, Beta 462.081 | Adversarial Rewards: Alpha 458.716, Beta 451.512 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.458000183105469, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.319000244140625, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.156s\n",
            "Episode 640 | Step 39983 | Epsilon 0.135 | Clean Rewards: Alpha 467.003, Beta 463.668 | Adversarial Rewards: Alpha 468.062, Beta 455.775 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.696000099182129, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.630999565124512, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.234s\n",
            "Episode 660 | Step 41243 | Epsilon 0.127 | Clean Rewards: Alpha 449.504, Beta 464.996 | Adversarial Rewards: Alpha 471.155, Beta 456.956 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.802000045776367, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.60200023651123, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.232s\n",
            "Episode 680 | Step 42503 | Epsilon 0.119 | Clean Rewards: Alpha 450.398, Beta 461.605 | Adversarial Rewards: Alpha 469.241, Beta 460.945 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.651000022888184, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.378000259399414, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.313s\n",
            "Episode 700 | Step 43763 | Epsilon 0.112 | Clean Rewards: Alpha 443.636, Beta 448.115 | Adversarial Rewards: Alpha 459.226, Beta 442.783 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.491000175476074, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.388999938964844, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.283s\n",
            "Episode 720 | Step 45023 | Epsilon 0.105 | Clean Rewards: Alpha 437.897, Beta 439.932 | Adversarial Rewards: Alpha 450.083, Beta 452.485 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.251999855041504, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.227999687194824, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.258s\n",
            "Episode 740 | Step 46283 | Epsilon 0.000 | Clean Rewards: Alpha 437.237, Beta 438.151 | Adversarial Rewards: Alpha 448.625, Beta 439.695 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.154000282287598, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.934000015258789, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.337s\n",
            "Episode 760 | Step 47543 | Epsilon 0.000 | Clean Rewards: Alpha 444.324, Beta 434.274 | Adversarial Rewards: Alpha 450.164, Beta 438.693 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.109999656677246, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.906999588012695, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.725s\n",
            "Episode 780 | Step 48803 | Epsilon 0.000 | Clean Rewards: Alpha 461.499, Beta 435.078 | Adversarial Rewards: Alpha 451.671, Beta 445.763 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.156000137329102, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.020000457763672, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.147s\n",
            "Episode 800 | Step 50063 | Epsilon 0.000 | Clean Rewards: Alpha 459.296, Beta 438.534 | Adversarial Rewards: Alpha 448.416, Beta 449.129 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.336999893188477, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.173999786376953, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.139s\n",
            "Episode 820 | Step 51323 | Epsilon 0.000 | Clean Rewards: Alpha 457.947, Beta 448.955 | Adversarial Rewards: Alpha 448.295, Beta 447.307 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.305999755859375, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.125, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.847s\n",
            "Episode 840 | Step 52583 | Epsilon 0.000 | Clean Rewards: Alpha 454.064, Beta 445.089 | Adversarial Rewards: Alpha 449.652, Beta 451.877 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.133000373840332, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.210000038146973, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.332s\n",
            "Episode 860 | Step 53843 | Epsilon 0.000 | Clean Rewards: Alpha 453.34, Beta 440.406 | Adversarial Rewards: Alpha 450.468, Beta 450.624 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.027999877929688, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.269000053405762, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.539s\n",
            "Episode 880 | Step 55103 | Epsilon 0.000 | Clean Rewards: Alpha 448.888, Beta 442.417 | Adversarial Rewards: Alpha 449.215, Beta 454.124 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.053000450134277, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.3149995803833, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.024s\n",
            "Episode 900 | Step 56363 | Epsilon 0.000 | Clean Rewards: Alpha 461.632, Beta 439.872 | Adversarial Rewards: Alpha 453.269, Beta 461.584 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.121999740600586, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.211000442504883, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.141s\n",
            "Episode 920 | Step 57623 | Epsilon 0.000 | Clean Rewards: Alpha 459.95, Beta 433.413 | Adversarial Rewards: Alpha 447.55, Beta 462.515 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.322999954223633, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.258999824523926, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.942s\n",
            "Episode 940 | Step 58883 | Epsilon 0.000 | Clean Rewards: Alpha 468.674, Beta 428.351 | Adversarial Rewards: Alpha 444.965, Beta 463.705 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.32800006866455, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.159000396728516, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.881s\n",
            "Episode 960 | Step 60143 | Epsilon 0.000 | Clean Rewards: Alpha 474.566, Beta 437.232 | Adversarial Rewards: Alpha 440.904, Beta 464.471 | Clean Losses: Alpha 0.105, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.423999786376953, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.128000259399414, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.076s\n",
            "Episode 980 | Step 61403 | Epsilon 0.000 | Clean Rewards: Alpha 458.455, Beta 425.541 | Adversarial Rewards: Alpha 433.817, Beta 458.345 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.105, Beta 0.2 | Clean Qs: Alpha 14.206000328063965, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.098999977111816, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.966s\n",
            "Episode 1000 | Step 62663 | Epsilon 0.000 | Clean Rewards: Alpha 446.64, Beta 425.194 | Adversarial Rewards: Alpha 441.45, Beta 455.14 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.105, Beta 0.2 | Clean Qs: Alpha 13.888999938964844, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.923999786376953, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.987s\n",
            "Episode 1020 | Step 63923 | Epsilon 0.000 | Clean Rewards: Alpha 449.159, Beta 422.631 | Adversarial Rewards: Alpha 456.554, Beta 453.387 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.105, Beta 0.2 | Clean Qs: Alpha 13.779999732971191, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.791999816894531, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.864s\n",
            "Episode 1040 | Step 65183 | Epsilon 0.000 | Clean Rewards: Alpha 444.658, Beta 434.319 | Adversarial Rewards: Alpha 454.966, Beta 447.305 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.843999862670898, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.963000297546387, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.989s\n",
            "Episode 1060 | Step 66443 | Epsilon 0.000 | Clean Rewards: Alpha 435.256, Beta 423.367 | Adversarial Rewards: Alpha 447.309, Beta 448.584 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.741000175476074, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.984999656677246, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.884s\n",
            "Episode 1080 | Step 67703 | Epsilon 0.000 | Clean Rewards: Alpha 433.994, Beta 439.149 | Adversarial Rewards: Alpha 451.184, Beta 444.885 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.645999908447266, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.755000114440918, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.706s\n",
            "Episode 1100 | Step 68963 | Epsilon 0.000 | Clean Rewards: Alpha 442.113, Beta 447.829 | Adversarial Rewards: Alpha 444.275, Beta 445.608 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.623000144958496, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.751999855041504, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.813s\n",
            "Episode 1120 | Step 70223 | Epsilon 0.000 | Clean Rewards: Alpha 439.182, Beta 454.449 | Adversarial Rewards: Alpha 436.496, Beta 446.267 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.694000244140625, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.876999855041504, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.627s\n",
            "Episode 1140 | Step 71483 | Epsilon 0.000 | Clean Rewards: Alpha 452.235, Beta 450.391 | Adversarial Rewards: Alpha 438.565, Beta 447.379 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.782999992370605, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.883999824523926, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.655s\n",
            "Episode 1160 | Step 72743 | Epsilon 0.000 | Clean Rewards: Alpha 456.944, Beta 468.835 | Adversarial Rewards: Alpha 448.323, Beta 439.578 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.031000137329102, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.970999717712402, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.789s\n",
            "Episode 1180 | Step 74003 | Epsilon 0.000 | Clean Rewards: Alpha 462.814, Beta 461.591 | Adversarial Rewards: Alpha 449.55, Beta 438.657 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.187000274658203, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.147000312805176, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.783s\n",
            "Episode 1200 | Step 75263 | Epsilon 0.000 | Clean Rewards: Alpha 455.536, Beta 454.472 | Adversarial Rewards: Alpha 446.44, Beta 434.852 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.267999649047852, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.079999923706055, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.905s\n",
            "Episode 1220 | Step 76523 | Epsilon 0.000 | Clean Rewards: Alpha 453.943, Beta 451.972 | Adversarial Rewards: Alpha 445.551, Beta 441.849 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.041999816894531, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.975000381469727, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.734s\n",
            "Episode 1240 | Step 77783 | Epsilon 0.000 | Clean Rewards: Alpha 447.067, Beta 450.467 | Adversarial Rewards: Alpha 439.466, Beta 454.035 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 13.831999778747559, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.836000442504883, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.746s\n",
            "Episode 1260 | Step 79043 | Epsilon 0.000 | Clean Rewards: Alpha 448.045, Beta 441.913 | Adversarial Rewards: Alpha 446.008, Beta 457.097 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.715999603271484, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.79800033569336, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.852s\n",
            "Episode 1280 | Step 80303 | Epsilon 0.000 | Clean Rewards: Alpha 439.678, Beta 441.174 | Adversarial Rewards: Alpha 452.72, Beta 464.356 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.847999572753906, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.760000228881836, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.821s\n",
            "Episode 1300 | Step 81563 | Epsilon 0.000 | Clean Rewards: Alpha 431.482, Beta 445.289 | Adversarial Rewards: Alpha 454.948, Beta 464.606 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.802000045776367, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.871999740600586, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.099s\n",
            "Episode 1320 | Step 82823 | Epsilon 0.000 | Clean Rewards: Alpha 427.207, Beta 446.252 | Adversarial Rewards: Alpha 460.022, Beta 455.897 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.935999870300293, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.87600040435791, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.798s\n",
            "Episode 1340 | Step 84083 | Epsilon 0.000 | Clean Rewards: Alpha 422.76, Beta 442.655 | Adversarial Rewards: Alpha 466.188, Beta 447.51 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.105, Beta 0.2 | Clean Qs: Alpha 13.920000076293945, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.805999755859375, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.84s\n",
            "Episode 1360 | Step 85343 | Epsilon 0.000 | Clean Rewards: Alpha 424.661, Beta 444.941 | Adversarial Rewards: Alpha 452.09, Beta 453.67 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.105, Beta 0.2 | Clean Qs: Alpha 13.875, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.704000473022461, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.863s\n",
            "Episode 1380 | Step 86603 | Epsilon 0.000 | Clean Rewards: Alpha 431.741, Beta 443.835 | Adversarial Rewards: Alpha 450.869, Beta 454.422 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.777999877929688, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.67300033569336, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.867s\n",
            "Episode 1400 | Step 87863 | Epsilon 0.000 | Clean Rewards: Alpha 436.311, Beta 440.761 | Adversarial Rewards: Alpha 452.858, Beta 455.084 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.770000457763672, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.729000091552734, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.081s\n",
            "Episode 1420 | Step 89123 | Epsilon 0.000 | Clean Rewards: Alpha 434.62, Beta 435.082 | Adversarial Rewards: Alpha 453.37, Beta 446.964 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.732999801635742, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.861000061035156, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.883s\n",
            "Episode 1440 | Step 90383 | Epsilon 0.000 | Clean Rewards: Alpha 431.583, Beta 441.533 | Adversarial Rewards: Alpha 454.208, Beta 443.255 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.758999824523926, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.045999526977539, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.11s\n",
            "Episode 1460 | Step 91643 | Epsilon 0.000 | Clean Rewards: Alpha 438.885, Beta 436.143 | Adversarial Rewards: Alpha 457.417, Beta 438.267 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.817000389099121, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.133999824523926, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.079s\n",
            "Episode 1480 | Step 92903 | Epsilon 0.000 | Clean Rewards: Alpha 447.426, Beta 438.239 | Adversarial Rewards: Alpha 460.527, Beta 439.952 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.041999816894531, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.39900016784668, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.927s\n",
            "Episode 1500 | Step 94163 | Epsilon 0.000 | Clean Rewards: Alpha 449.071, Beta 439.966 | Adversarial Rewards: Alpha 466.375, Beta 435.645 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.253999710083008, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.456999778747559, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.014s\n",
            "Episode 1520 | Step 95423 | Epsilon 0.000 | Clean Rewards: Alpha 457.714, Beta 440.962 | Adversarial Rewards: Alpha 466.057, Beta 450.185 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.383000373840332, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.494000434875488, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.793s\n",
            "Episode 1540 | Step 96683 | Epsilon 0.000 | Clean Rewards: Alpha 468.375, Beta 440.978 | Adversarial Rewards: Alpha 469.51, Beta 449.544 | Clean Losses: Alpha 0.102, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.61299991607666, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.526000022888184, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.875s\n",
            "Episode 1560 | Step 97943 | Epsilon 0.000 | Clean Rewards: Alpha 464.783, Beta 438.066 | Adversarial Rewards: Alpha 470.477, Beta 448.497 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.758999824523926, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.647000312805176, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.872s\n",
            "Episode 1580 | Step 99203 | Epsilon 0.000 | Clean Rewards: Alpha 450.35, Beta 433.737 | Adversarial Rewards: Alpha 457.137, Beta 442.729 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.496000289916992, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.394000053405762, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.897s\n",
            "Episode 1600 | Step 100463 | Epsilon 0.000 | Clean Rewards: Alpha 439.636, Beta 423.855 | Adversarial Rewards: Alpha 450.606, Beta 450.348 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.130000114440918, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.072999954223633, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.827s\n",
            "Episode 1620 | Step 101723 | Epsilon 0.000 | Clean Rewards: Alpha 443.084, Beta 431.739 | Adversarial Rewards: Alpha 450.894, Beta 439.191 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 13.984999656677246, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.956999778747559, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.849s\n",
            "Episode 1640 | Step 102983 | Epsilon 0.000 | Clean Rewards: Alpha 441.327, Beta 434.344 | Adversarial Rewards: Alpha 435.765, Beta 440.886 | Clean Losses: Alpha 0.105, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 13.892999649047852, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.932999610900879, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.734s\n",
            "Episode 1660 | Step 104243 | Epsilon 0.000 | Clean Rewards: Alpha 439.436, Beta 433.658 | Adversarial Rewards: Alpha 427.987, Beta 440.947 | Clean Losses: Alpha 0.105, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 13.666999816894531, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.645000457763672, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.806s\n",
            "Episode 1680 | Step 105503 | Epsilon 0.000 | Clean Rewards: Alpha 441.222, Beta 432.137 | Adversarial Rewards: Alpha 427.679, Beta 446.999 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 13.614999771118164, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.45199966430664, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.718s\n",
            "Episode 1700 | Step 106763 | Epsilon 0.000 | Clean Rewards: Alpha 451.796, Beta 440.283 | Adversarial Rewards: Alpha 432.657, Beta 444.458 | Clean Losses: Alpha 0.105, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.62399959564209, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.496999740600586, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.007s\n",
            "Episode 1720 | Step 108023 | Epsilon 0.000 | Clean Rewards: Alpha 450.556, Beta 441.159 | Adversarial Rewards: Alpha 431.805, Beta 451.839 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.722999572753906, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.595000267028809, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.987s\n",
            "Episode 1740 | Step 109283 | Epsilon 0.000 | Clean Rewards: Alpha 434.844, Beta 433.177 | Adversarial Rewards: Alpha 435.485, Beta 455.096 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.595999717712402, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.512999534606934, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.189s\n",
            "Episode 1760 | Step 110543 | Epsilon 0.000 | Clean Rewards: Alpha 428.465, Beta 434.9 | Adversarial Rewards: Alpha 446.175, Beta 465.355 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.489999771118164, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.442000389099121, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.077s\n",
            "Episode 1780 | Step 111803 | Epsilon 0.000 | Clean Rewards: Alpha 434.49, Beta 425.27 | Adversarial Rewards: Alpha 451.743, Beta 453.73 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.526000022888184, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.734000205993652, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.89s\n",
            "Episode 1800 | Step 113063 | Epsilon 0.000 | Clean Rewards: Alpha 436.064, Beta 425.657 | Adversarial Rewards: Alpha 444.425, Beta 456.189 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.701000213623047, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.809000015258789, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.768s\n",
            "Episode 1820 | Step 114323 | Epsilon 0.000 | Clean Rewards: Alpha 438.281, Beta 418.951 | Adversarial Rewards: Alpha 446.334, Beta 456.048 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.807000160217285, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.803999900817871, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.755s\n",
            "Episode 1840 | Step 115583 | Epsilon 0.000 | Clean Rewards: Alpha 451.192, Beta 418.447 | Adversarial Rewards: Alpha 450.626, Beta 450.661 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.03499984741211, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.902999877929688, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.859s\n",
            "Episode 1860 | Step 116843 | Epsilon 0.000 | Clean Rewards: Alpha 455.922, Beta 423.393 | Adversarial Rewards: Alpha 451.564, Beta 443.87 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.163000106811523, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.067000389099121, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.914s\n",
            "Episode 1880 | Step 118103 | Epsilon 0.000 | Clean Rewards: Alpha 455.853, Beta 434.914 | Adversarial Rewards: Alpha 448.342, Beta 454.654 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.20300006866455, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.989999771118164, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.95s\n",
            "Episode 1900 | Step 119363 | Epsilon 0.000 | Clean Rewards: Alpha 457.523, Beta 437.017 | Adversarial Rewards: Alpha 449.201, Beta 450.357 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.213000297546387, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.086000442504883, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.192s\n",
            "Episode 1920 | Step 120623 | Epsilon 0.000 | Clean Rewards: Alpha 455.867, Beta 432.88 | Adversarial Rewards: Alpha 445.263, Beta 451.828 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.088000297546387, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.045000076293945, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.22s\n",
            "Episode 1940 | Step 121883 | Epsilon 0.000 | Clean Rewards: Alpha 446.044, Beta 448.494 | Adversarial Rewards: Alpha 443.428, Beta 454.915 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.835000038146973, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.888999938964844, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.006s\n",
            "Episode 1960 | Step 123143 | Epsilon 0.000 | Clean Rewards: Alpha 438.822, Beta 450.32 | Adversarial Rewards: Alpha 442.234, Beta 452.209 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.765000343322754, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.83899974822998, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.01s\n",
            "Episode 1980 | Step 124403 | Epsilon 0.000 | Clean Rewards: Alpha 430.567, Beta 449.3 | Adversarial Rewards: Alpha 441.04, Beta 437.343 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.666000366210938, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.72599983215332, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.026s\n",
            "Episode 2000 | Step 125663 | Epsilon 0.000 | Clean Rewards: Alpha 427.267, Beta 450.321 | Adversarial Rewards: Alpha 442.364, Beta 444.808 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 13.51200008392334, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.618000030517578, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.04s\n",
            "Episode 2020 | Step 126923 | Epsilon 0.000 | Clean Rewards: Alpha 423.2, Beta 452.183 | Adversarial Rewards: Alpha 436.505, Beta 434.148 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 13.463000297546387, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.550000190734863, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.966s\n",
            "Episode 2040 | Step 128183 | Epsilon 0.000 | Clean Rewards: Alpha 431.06, Beta 445.395 | Adversarial Rewards: Alpha 437.241, Beta 438.775 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 13.482999801635742, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.472000122070312, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.013s\n",
            "Episode 2060 | Step 129443 | Epsilon 0.000 | Clean Rewards: Alpha 435.587, Beta 443.596 | Adversarial Rewards: Alpha 439.351, Beta 443.51 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.102, Beta 0.2 | Clean Qs: Alpha 13.555999755859375, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.595999717712402, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.984s\n",
            "Episode 2080 | Step 130703 | Epsilon 0.000 | Clean Rewards: Alpha 440.287, Beta 439.078 | Adversarial Rewards: Alpha 444.204, Beta 451.133 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.102, Beta 0.2 | Clean Qs: Alpha 13.701000213623047, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.802000045776367, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.97s\n",
            "Episode 2100 | Step 131963 | Epsilon 0.000 | Clean Rewards: Alpha 448.012, Beta 437.357 | Adversarial Rewards: Alpha 447.63, Beta 451.909 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.102, Beta 0.2 | Clean Qs: Alpha 13.869999885559082, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.895999908447266, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.098s\n",
            "Episode 2120 | Step 133223 | Epsilon 0.000 | Clean Rewards: Alpha 444.388, Beta 436.35 | Adversarial Rewards: Alpha 455.001, Beta 453.242 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 13.9350004196167, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.00100040435791, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.121s\n",
            "Episode 2140 | Step 134483 | Epsilon 0.000 | Clean Rewards: Alpha 444.132, Beta 437.094 | Adversarial Rewards: Alpha 455.867, Beta 444.356 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 13.956999778747559, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.083000183105469, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.037s\n",
            "Episode 2160 | Step 135743 | Epsilon 0.000 | Clean Rewards: Alpha 452.22, Beta 439.891 | Adversarial Rewards: Alpha 459.062, Beta 447.81 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.036999702453613, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.133999824523926, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.99s\n",
            "Episode 2180 | Step 137003 | Epsilon 0.000 | Clean Rewards: Alpha 445.784, Beta 448.546 | Adversarial Rewards: Alpha 463.09, Beta 443.598 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.152999877929688, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.184000015258789, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.221s\n",
            "Episode 2200 | Step 138263 | Epsilon 0.000 | Clean Rewards: Alpha 431.303, Beta 447.325 | Adversarial Rewards: Alpha 456.059, Beta 431.425 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.005999565124512, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.99899959564209, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.031s\n",
            "Episode 2220 | Step 139523 | Epsilon 0.000 | Clean Rewards: Alpha 442.555, Beta 457.213 | Adversarial Rewards: Alpha 455.659, Beta 437.129 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.906999588012695, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.937999725341797, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.0s\n",
            "Episode 2240 | Step 140783 | Epsilon 0.000 | Clean Rewards: Alpha 447.726, Beta 456.761 | Adversarial Rewards: Alpha 464.086, Beta 445.396 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.130999565124512, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.123000144958496, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.03s\n",
            "Episode 2260 | Step 142043 | Epsilon 0.000 | Clean Rewards: Alpha 442.827, Beta 465.019 | Adversarial Rewards: Alpha 457.996, Beta 437.78 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.131999969482422, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.125, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.997s\n",
            "Episode 2280 | Step 143303 | Epsilon 0.000 | Clean Rewards: Alpha 444.338, Beta 456.554 | Adversarial Rewards: Alpha 456.862, Beta 440.941 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.163999557495117, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.956999778747559, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.816s\n",
            "Episode 2300 | Step 144563 | Epsilon 0.000 | Clean Rewards: Alpha 442.268, Beta 462.407 | Adversarial Rewards: Alpha 470.573, Beta 447.441 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.15999984741211, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.154000282287598, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.981s\n",
            "Episode 2320 | Step 145823 | Epsilon 0.000 | Clean Rewards: Alpha 441.618, Beta 465.775 | Adversarial Rewards: Alpha 464.102, Beta 450.192 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.277999877929688, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.199999809265137, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.001s\n",
            "Episode 2340 | Step 147083 | Epsilon 0.000 | Clean Rewards: Alpha 428.899, Beta 462.203 | Adversarial Rewards: Alpha 452.883, Beta 449.665 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.026000022888184, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.027000427246094, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.939s\n",
            "Episode 2360 | Step 148343 | Epsilon 0.000 | Clean Rewards: Alpha 431.64, Beta 448.274 | Adversarial Rewards: Alpha 452.695, Beta 449.661 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.970000267028809, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.82699966430664, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.959s\n",
            "Episode 2380 | Step 149603 | Epsilon 0.000 | Clean Rewards: Alpha 433.193, Beta 455.818 | Adversarial Rewards: Alpha 444.783, Beta 449.715 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.697999954223633, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.805000305175781, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.155s\n",
            "Episode 2400 | Step 150863 | Epsilon 0.000 | Clean Rewards: Alpha 445.594, Beta 458.389 | Adversarial Rewards: Alpha 435.912, Beta 442.8 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.74899959564209, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.769000053405762, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.995s\n",
            "Episode 2420 | Step 152123 | Epsilon 0.000 | Clean Rewards: Alpha 442.36, Beta 449.147 | Adversarial Rewards: Alpha 434.019, Beta 435.183 | Clean Losses: Alpha 0.105, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.654999732971191, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.697999954223633, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.009s\n",
            "Episode 2440 | Step 153383 | Epsilon 0.000 | Clean Rewards: Alpha 442.315, Beta 454.433 | Adversarial Rewards: Alpha 437.94, Beta 432.026 | Clean Losses: Alpha 0.105, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 13.663999557495117, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.734000205993652, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.038s\n",
            "Episode 2460 | Step 154643 | Epsilon 0.000 | Clean Rewards: Alpha 439.609, Beta 457.106 | Adversarial Rewards: Alpha 432.511, Beta 437.956 | Clean Losses: Alpha 0.105, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 13.524999618530273, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.760000228881836, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.224s\n",
            "Episode 2480 | Step 155903 | Epsilon 0.000 | Clean Rewards: Alpha 448.305, Beta 455.37 | Adversarial Rewards: Alpha 444.819, Beta 440.72 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 13.597999572753906, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.843000411987305, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.089s\n",
            "Episode 2500 | Step 157163 | Epsilon 0.000 | Clean Rewards: Alpha 457.872, Beta 456.035 | Adversarial Rewards: Alpha 453.8, Beta 444.793 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 13.916000366210938, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.067999839782715, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.211s\n",
            "Episode 2520 | Step 158423 | Epsilon 0.000 | Clean Rewards: Alpha 451.71, Beta 454.898 | Adversarial Rewards: Alpha 458.456, Beta 446.477 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.173999786376953, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.36400032043457, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.083s\n",
            "Episode 2540 | Step 159683 | Epsilon 0.000 | Clean Rewards: Alpha 469.055, Beta 443.811 | Adversarial Rewards: Alpha 452.263, Beta 449.135 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.34000015258789, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.413000106811523, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.032s\n",
            "Episode 2560 | Step 160943 | Epsilon 0.000 | Clean Rewards: Alpha 463.263, Beta 439.436 | Adversarial Rewards: Alpha 450.345, Beta 447.572 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.508999824523926, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.510000228881836, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.055s\n",
            "Episode 2580 | Step 162203 | Epsilon 0.000 | Clean Rewards: Alpha 463.635, Beta 441.588 | Adversarial Rewards: Alpha 447.528, Beta 449.351 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.501999855041504, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.371000289916992, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.103s\n",
            "Episode 2600 | Step 163463 | Epsilon 0.000 | Clean Rewards: Alpha 453.77, Beta 440.397 | Adversarial Rewards: Alpha 435.633, Beta 454.788 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.331999778747559, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.225000381469727, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.031s\n",
            "Episode 2620 | Step 164723 | Epsilon 0.000 | Clean Rewards: Alpha 461.927, Beta 446.877 | Adversarial Rewards: Alpha 431.856, Beta 457.978 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.081000328063965, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.963000297546387, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.383s\n",
            "Episode 2640 | Step 165983 | Epsilon 0.000 | Clean Rewards: Alpha 452.856, Beta 445.706 | Adversarial Rewards: Alpha 432.455, Beta 460.542 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.940999984741211, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.829000473022461, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.238s\n",
            "Episode 2660 | Step 167243 | Epsilon 0.000 | Clean Rewards: Alpha 458.594, Beta 458.132 | Adversarial Rewards: Alpha 439.02, Beta 454.117 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.765000343322754, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.701000213623047, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.918s\n",
            "Episode 2680 | Step 168503 | Epsilon 0.000 | Clean Rewards: Alpha 446.661, Beta 454.798 | Adversarial Rewards: Alpha 432.552, Beta 445.24 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.105, Beta 0.2 | Clean Qs: Alpha 13.854000091552734, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.824999809265137, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.861s\n",
            "Episode 2700 | Step 169763 | Epsilon 0.000 | Clean Rewards: Alpha 450.16, Beta 451.13 | Adversarial Rewards: Alpha 434.772, Beta 445.831 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.791999816894531, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.659000396728516, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.544s\n",
            "Episode 2720 | Step 171023 | Epsilon 0.000 | Clean Rewards: Alpha 441.372, Beta 452.331 | Adversarial Rewards: Alpha 437.76, Beta 445.04 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.920000076293945, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.644000053405762, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.516s\n",
            "Episode 2740 | Step 172283 | Epsilon 0.000 | Clean Rewards: Alpha 445.199, Beta 457.109 | Adversarial Rewards: Alpha 438.8, Beta 434.066 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 13.88700008392334, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.623000144958496, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.887s\n",
            "Episode 2760 | Step 173543 | Epsilon 0.000 | Clean Rewards: Alpha 443.692, Beta 443.652 | Adversarial Rewards: Alpha 436.159, Beta 446.152 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 13.92199993133545, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.729000091552734, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.634s\n",
            "Episode 2780 | Step 174803 | Epsilon 0.000 | Clean Rewards: Alpha 454.543, Beta 449.459 | Adversarial Rewards: Alpha 444.592, Beta 453.52 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 13.812999725341797, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.758000373840332, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.433s\n",
            "Episode 2800 | Step 176063 | Epsilon 0.000 | Clean Rewards: Alpha 447.91, Beta 447.973 | Adversarial Rewards: Alpha 445.173, Beta 452.275 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.890999794006348, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.904999732971191, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.653s\n",
            "Episode 2820 | Step 177323 | Epsilon 0.000 | Clean Rewards: Alpha 450.471, Beta 444.887 | Adversarial Rewards: Alpha 452.137, Beta 451.75 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 13.831999778747559, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.001999855041504, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.285s\n",
            "Episode 2840 | Step 178583 | Epsilon 0.000 | Clean Rewards: Alpha 446.696, Beta 437.961 | Adversarial Rewards: Alpha 447.129, Beta 458.39 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 13.916999816894531, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.097999572753906, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.498s\n",
            "Episode 2860 | Step 179843 | Epsilon 0.000 | Clean Rewards: Alpha 442.398, Beta 435.235 | Adversarial Rewards: Alpha 460.595, Beta 450.972 | Clean Losses: Alpha 0.105, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 13.911999702453613, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.00100040435791, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.451s\n",
            "Episode 2880 | Step 181103 | Epsilon 0.000 | Clean Rewards: Alpha 438.77, Beta 433.01 | Adversarial Rewards: Alpha 449.895, Beta 445.178 | Clean Losses: Alpha 0.105, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.998000144958496, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.987000465393066, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.415s\n",
            "Episode 2900 | Step 182363 | Epsilon 0.000 | Clean Rewards: Alpha 438.076, Beta 427.941 | Adversarial Rewards: Alpha 454.182, Beta 434.001 | Clean Losses: Alpha 0.105, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 13.90999984741211, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.913999557495117, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.97s\n",
            "Episode 2920 | Step 183623 | Epsilon 0.000 | Clean Rewards: Alpha 435.484, Beta 429.197 | Adversarial Rewards: Alpha 452.937, Beta 444.243 | Clean Losses: Alpha 0.105, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 13.91100025177002, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.803999900817871, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.909s\n",
            "Episode 2940 | Step 184883 | Epsilon 0.000 | Clean Rewards: Alpha 439.115, Beta 442.902 | Adversarial Rewards: Alpha 465.946, Beta 442.007 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.899999618530273, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.8100004196167, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.883s\n",
            "Episode 2960 | Step 186143 | Epsilon 0.000 | Clean Rewards: Alpha 441.209, Beta 453.101 | Adversarial Rewards: Alpha 458.965, Beta 445.089 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.060999870300293, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.029000282287598, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.93s\n",
            "Episode 2980 | Step 187403 | Epsilon 0.000 | Clean Rewards: Alpha 441.066, Beta 453.074 | Adversarial Rewards: Alpha 466.318, Beta 454.206 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.079000473022461, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.003000259399414, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.997s\n",
            "Episode 3000 | Step 188663 | Epsilon 0.000 | Clean Rewards: Alpha 451.148, Beta 466.098 | Adversarial Rewards: Alpha 460.521, Beta 456.0 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.152999877929688, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.031000137329102, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.05s\n",
            "Episode 3020 | Step 189923 | Epsilon 0.000 | Clean Rewards: Alpha 458.696, Beta 460.533 | Adversarial Rewards: Alpha 466.286, Beta 441.251 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.105, Beta 0.2 | Clean Qs: Alpha 14.131999969482422, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.211000442504883, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.071s\n",
            "Episode 3040 | Step 191183 | Epsilon 0.000 | Clean Rewards: Alpha 447.158, Beta 466.26 | Adversarial Rewards: Alpha 475.477, Beta 446.328 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.211999893188477, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.276000022888184, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.017s\n",
            "Episode 3060 | Step 192443 | Epsilon 0.000 | Clean Rewards: Alpha 442.928, Beta 457.933 | Adversarial Rewards: Alpha 473.035, Beta 442.812 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.239999771118164, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.36400032043457, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.934s\n",
            "Episode 3080 | Step 193703 | Epsilon 0.000 | Clean Rewards: Alpha 443.89, Beta 457.037 | Adversarial Rewards: Alpha 474.839, Beta 427.024 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.225000381469727, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.291000366210938, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.13s\n",
            "Episode 3100 | Step 194963 | Epsilon 0.000 | Clean Rewards: Alpha 428.797, Beta 443.605 | Adversarial Rewards: Alpha 477.376, Beta 433.779 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.142000198364258, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.251999855041504, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.904s\n",
            "Episode 3120 | Step 196223 | Epsilon 0.000 | Clean Rewards: Alpha 429.376, Beta 439.794 | Adversarial Rewards: Alpha 468.363, Beta 443.828 | Clean Losses: Alpha 0.105, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.053000450134277, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.02400016784668, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.074s\n",
            "Episode 3140 | Step 197483 | Epsilon 0.000 | Clean Rewards: Alpha 435.215, Beta 423.928 | Adversarial Rewards: Alpha 454.156, Beta 442.753 | Clean Losses: Alpha 0.105, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 13.99899959564209, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.057999610900879, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.89s\n",
            "Episode 3160 | Step 198743 | Epsilon 0.000 | Clean Rewards: Alpha 457.609, Beta 428.076 | Adversarial Rewards: Alpha 459.912, Beta 443.841 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.972000122070312, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.920000076293945, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.806s\n",
            "Episode 3180 | Step 200003 | Epsilon 0.000 | Clean Rewards: Alpha 459.198, Beta 423.893 | Adversarial Rewards: Alpha 456.837, Beta 448.304 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.272000312805176, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.258999824523926, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.844s\n",
            "Episode 3200 | Step 201263 | Epsilon 0.000 | Clean Rewards: Alpha 462.305, Beta 426.373 | Adversarial Rewards: Alpha 456.475, Beta 439.764 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.399999618530273, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.38599967956543, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.891s\n",
            "Episode 3220 | Step 202523 | Epsilon 0.000 | Clean Rewards: Alpha 464.999, Beta 425.715 | Adversarial Rewards: Alpha 454.9, Beta 436.159 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.607000350952148, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.569000244140625, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.999s\n",
            "Episode 3240 | Step 203783 | Epsilon 0.000 | Clean Rewards: Alpha 460.521, Beta 431.657 | Adversarial Rewards: Alpha 460.737, Beta 440.677 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.611000061035156, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.440999984741211, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.885s\n",
            "Episode 3260 | Step 205043 | Epsilon 0.000 | Clean Rewards: Alpha 437.774, Beta 433.631 | Adversarial Rewards: Alpha 446.146, Beta 438.782 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.48799991607666, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.342000007629395, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.838s\n",
            "Episode 3280 | Step 206303 | Epsilon 0.000 | Clean Rewards: Alpha 430.149, Beta 427.915 | Adversarial Rewards: Alpha 458.574, Beta 454.225 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.059000015258789, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.951000213623047, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.043s\n",
            "Episode 3300 | Step 207563 | Epsilon 0.000 | Clean Rewards: Alpha 435.396, Beta 442.275 | Adversarial Rewards: Alpha 450.195, Beta 465.219 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 13.954999923706055, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.956000328063965, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.148s\n",
            "Episode 3320 | Step 208823 | Epsilon 0.000 | Clean Rewards: Alpha 426.18, Beta 442.415 | Adversarial Rewards: Alpha 445.626, Beta 460.384 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 13.677000045776367, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.802000045776367, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.055s\n",
            "Episode 3340 | Step 210083 | Epsilon 0.000 | Clean Rewards: Alpha 431.446, Beta 441.572 | Adversarial Rewards: Alpha 444.103, Beta 455.883 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.545999526977539, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.822999954223633, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.981s\n",
            "Episode 3360 | Step 211343 | Epsilon 0.000 | Clean Rewards: Alpha 437.455, Beta 442.003 | Adversarial Rewards: Alpha 448.352, Beta 457.26 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.557999610900879, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.923999786376953, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.916s\n",
            "Episode 3380 | Step 212603 | Epsilon 0.000 | Clean Rewards: Alpha 438.277, Beta 447.596 | Adversarial Rewards: Alpha 426.632, Beta 445.823 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.105, Beta 0.2 | Clean Qs: Alpha 13.595000267028809, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.92199993133545, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.969s\n",
            "Episode 3400 | Step 213863 | Epsilon 0.000 | Clean Rewards: Alpha 438.508, Beta 433.63 | Adversarial Rewards: Alpha 439.349, Beta 438.142 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.105, Beta 0.2 | Clean Qs: Alpha 13.513999938964844, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.78600025177002, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.148s\n",
            "Episode 3420 | Step 215123 | Epsilon 0.000 | Clean Rewards: Alpha 446.511, Beta 439.814 | Adversarial Rewards: Alpha 438.578, Beta 438.541 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.105, Beta 0.2 | Clean Qs: Alpha 13.736000061035156, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.92300033569336, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.098s\n",
            "Episode 3440 | Step 216383 | Epsilon 0.000 | Clean Rewards: Alpha 446.217, Beta 451.237 | Adversarial Rewards: Alpha 437.915, Beta 448.746 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.895000457763672, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.02400016784668, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.93s\n",
            "Episode 3460 | Step 217643 | Epsilon 0.000 | Clean Rewards: Alpha 444.194, Beta 450.107 | Adversarial Rewards: Alpha 449.893, Beta 448.244 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.003000259399414, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.987000465393066, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.835s\n",
            "Episode 3480 | Step 218903 | Epsilon 0.000 | Clean Rewards: Alpha 457.589, Beta 450.871 | Adversarial Rewards: Alpha 459.563, Beta 450.07 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.130000114440918, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.25100040435791, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.853s\n",
            "Episode 3500 | Step 220163 | Epsilon 0.000 | Clean Rewards: Alpha 453.393, Beta 451.698 | Adversarial Rewards: Alpha 450.138, Beta 453.005 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.220999717712402, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.357999801635742, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.901s\n",
            "Episode 3520 | Step 221423 | Epsilon 0.000 | Clean Rewards: Alpha 452.025, Beta 448.794 | Adversarial Rewards: Alpha 457.054, Beta 453.574 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.11299991607666, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.26200008392334, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.433s\n",
            "Episode 3540 | Step 222683 | Epsilon 0.000 | Clean Rewards: Alpha 452.15, Beta 431.645 | Adversarial Rewards: Alpha 454.222, Beta 444.294 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.187000274658203, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.241999626159668, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.976s\n",
            "Episode 3560 | Step 223943 | Epsilon 0.000 | Clean Rewards: Alpha 454.53, Beta 435.551 | Adversarial Rewards: Alpha 445.055, Beta 451.459 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.107000350952148, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.227999687194824, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.946s\n",
            "Episode 3580 | Step 225203 | Epsilon 0.000 | Clean Rewards: Alpha 456.606, Beta 437.195 | Adversarial Rewards: Alpha 440.798, Beta 455.603 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.098999977111816, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.071999549865723, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.915s\n",
            "Episode 3600 | Step 226463 | Epsilon 0.000 | Clean Rewards: Alpha 458.6, Beta 431.194 | Adversarial Rewards: Alpha 447.131, Beta 452.776 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.211999893188477, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.09000015258789, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.926s\n",
            "Episode 3620 | Step 227723 | Epsilon 0.000 | Clean Rewards: Alpha 450.933, Beta 437.803 | Adversarial Rewards: Alpha 445.14, Beta 453.128 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.105, Beta 0.2 | Clean Qs: Alpha 14.291999816894531, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.005999565124512, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.902s\n",
            "Episode 3640 | Step 228983 | Epsilon 0.000 | Clean Rewards: Alpha 455.248, Beta 440.821 | Adversarial Rewards: Alpha 451.886, Beta 454.331 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.105, Beta 0.2 | Clean Qs: Alpha 14.135000228881836, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.890000343322754, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.136s\n",
            "Episode 3660 | Step 230243 | Epsilon 0.000 | Clean Rewards: Alpha 462.255, Beta 437.202 | Adversarial Rewards: Alpha 457.114, Beta 453.214 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.105, Beta 0.2 | Clean Qs: Alpha 14.197999954223633, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.081999778747559, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.085s\n",
            "Episode 3680 | Step 231503 | Epsilon 0.000 | Clean Rewards: Alpha 451.35, Beta 439.768 | Adversarial Rewards: Alpha 461.15, Beta 450.939 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.258000373840332, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.170999526977539, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.98s\n",
            "Episode 3700 | Step 232763 | Epsilon 0.000 | Clean Rewards: Alpha 453.02, Beta 446.487 | Adversarial Rewards: Alpha 467.419, Beta 454.954 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.255000114440918, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.187999725341797, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.95s\n",
            "Episode 3720 | Step 234023 | Epsilon 0.000 | Clean Rewards: Alpha 447.982, Beta 443.595 | Adversarial Rewards: Alpha 474.647, Beta 454.953 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.21500015258789, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.331999778747559, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.125s\n",
            "Episode 3740 | Step 235283 | Epsilon 0.000 | Clean Rewards: Alpha 436.037, Beta 453.531 | Adversarial Rewards: Alpha 469.385, Beta 452.998 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.28600025177002, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.35200023651123, Beta 0.35600000619888306 | Avg Episode Length: 62.0 | Time Delta: 60.242s\n",
            "Episode 3760 | Step 236543 | Epsilon 0.000 | Clean Rewards: Alpha 428.722, Beta 452.667 | Adversarial Rewards: Alpha 463.469, Beta 445.176 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.241999626159668, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.10099983215332, Beta 0.35600000619888306 | Avg Episode Length: 62.0 | Time Delta: 59.936s\n",
            "Episode 3780 | Step 237803 | Epsilon 0.000 | Clean Rewards: Alpha 424.167, Beta 447.759 | Adversarial Rewards: Alpha 459.378, Beta 443.888 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.01200008392334, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.00100040435791, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.946s\n",
            "Episode 3800 | Step 239063 | Epsilon 0.000 | Clean Rewards: Alpha 420.204, Beta 444.604 | Adversarial Rewards: Alpha 457.565, Beta 442.598 | Clean Losses: Alpha 0.105, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.779000282287598, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.845000267028809, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.085s\n",
            "Episode 3820 | Step 240323 | Epsilon 0.000 | Clean Rewards: Alpha 427.775, Beta 440.229 | Adversarial Rewards: Alpha 452.405, Beta 437.105 | Clean Losses: Alpha 0.105, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.772000312805176, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.807000160217285, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.995s\n",
            "Episode 3840 | Step 241583 | Epsilon 0.000 | Clean Rewards: Alpha 429.963, Beta 433.643 | Adversarial Rewards: Alpha 449.244, Beta 429.73 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.704000473022461, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.82800006866455, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.1s\n",
            "Episode 3860 | Step 242843 | Epsilon 0.000 | Clean Rewards: Alpha 422.564, Beta 439.448 | Adversarial Rewards: Alpha 451.601, Beta 439.952 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.105, Beta 0.2 | Clean Qs: Alpha 13.567000389099121, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.71500015258789, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.987s\n",
            "Episode 3880 | Step 244103 | Epsilon 0.000 | Clean Rewards: Alpha 426.497, Beta 444.754 | Adversarial Rewards: Alpha 453.324, Beta 435.704 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.105, Beta 0.2 | Clean Qs: Alpha 13.595999717712402, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.630999565124512, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.222s\n",
            "Episode 3900 | Step 245363 | Epsilon 0.000 | Clean Rewards: Alpha 425.124, Beta 447.329 | Adversarial Rewards: Alpha 453.001, Beta 442.3 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.105, Beta 0.2 | Clean Qs: Alpha 13.708999633789062, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.642000198364258, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.865s\n",
            "Episode 3920 | Step 246623 | Epsilon 0.000 | Clean Rewards: Alpha 423.206, Beta 456.956 | Adversarial Rewards: Alpha 449.476, Beta 454.565 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.710000038146973, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.640000343322754, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.575s\n",
            "Episode 3940 | Step 247883 | Epsilon 0.000 | Clean Rewards: Alpha 423.622, Beta 453.832 | Adversarial Rewards: Alpha 445.633, Beta 459.065 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.57699966430664, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.565999984741211, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.506s\n",
            "Episode 3960 | Step 249143 | Epsilon 0.000 | Clean Rewards: Alpha 434.692, Beta 450.721 | Adversarial Rewards: Alpha 443.76, Beta 444.216 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.697999954223633, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.583000183105469, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.179s\n",
            "Episode 3980 | Step 250403 | Epsilon 0.000 | Clean Rewards: Alpha 436.404, Beta 442.226 | Adversarial Rewards: Alpha 446.914, Beta 452.372 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.736000061035156, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.732000350952148, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.385s\n",
            "Episode 4000 | Step 251663 | Epsilon 0.000 | Clean Rewards: Alpha 443.372, Beta 443.593 | Adversarial Rewards: Alpha 437.74, Beta 453.78 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.76200008392334, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.814000129699707, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.012s\n",
            "Episode 4020 | Step 252923 | Epsilon 0.000 | Clean Rewards: Alpha 441.476, Beta 436.285 | Adversarial Rewards: Alpha 445.987, Beta 447.342 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 13.770000457763672, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.781999588012695, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.242s\n",
            "Episode 4040 | Step 254183 | Epsilon 0.000 | Clean Rewards: Alpha 455.721, Beta 437.319 | Adversarial Rewards: Alpha 455.554, Beta 440.77 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.02400016784668, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.956000328063965, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.187s\n",
            "Episode 4060 | Step 255443 | Epsilon 0.000 | Clean Rewards: Alpha 449.743, Beta 444.724 | Adversarial Rewards: Alpha 465.36, Beta 455.867 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.230999946594238, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.180999755859375, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.296s\n",
            "Episode 4080 | Step 256703 | Epsilon 0.000 | Clean Rewards: Alpha 452.301, Beta 454.385 | Adversarial Rewards: Alpha 456.397, Beta 448.798 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.312999725341797, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.246000289916992, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.171s\n",
            "Episode 4100 | Step 257963 | Epsilon 0.000 | Clean Rewards: Alpha 455.916, Beta 452.96 | Adversarial Rewards: Alpha 461.08, Beta 447.022 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.32800006866455, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.272000312805176, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.121s\n",
            "Episode 4120 | Step 259223 | Epsilon 0.000 | Clean Rewards: Alpha 468.385, Beta 454.595 | Adversarial Rewards: Alpha 452.057, Beta 453.58 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.4399995803833, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.390000343322754, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.4s\n",
            "Episode 4140 | Step 260483 | Epsilon 0.000 | Clean Rewards: Alpha 457.792, Beta 468.24 | Adversarial Rewards: Alpha 447.384, Beta 456.177 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.447999954223633, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.37399959564209, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.117s\n",
            "Episode 4160 | Step 261743 | Epsilon 0.000 | Clean Rewards: Alpha 465.079, Beta 459.777 | Adversarial Rewards: Alpha 437.386, Beta 445.132 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.314000129699707, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.293000221252441, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.016s\n",
            "Episode 4180 | Step 263003 | Epsilon 0.000 | Clean Rewards: Alpha 459.598, Beta 457.54 | Adversarial Rewards: Alpha 438.795, Beta 449.419 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.102, Beta 0.2 | Clean Qs: Alpha 14.217000007629395, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.125, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.121s\n",
            "Episode 4200 | Step 264263 | Epsilon 0.000 | Clean Rewards: Alpha 446.843, Beta 462.314 | Adversarial Rewards: Alpha 436.867, Beta 451.754 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.086999893188477, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.973999977111816, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.924s\n",
            "Episode 4220 | Step 265523 | Epsilon 0.000 | Clean Rewards: Alpha 433.035, Beta 469.105 | Adversarial Rewards: Alpha 444.052, Beta 442.344 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 13.918999671936035, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.803999900817871, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.034s\n",
            "Episode 4240 | Step 266783 | Epsilon 0.000 | Clean Rewards: Alpha 432.281, Beta 453.43 | Adversarial Rewards: Alpha 447.636, Beta 440.539 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 13.765000343322754, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.722000122070312, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.033s\n",
            "Episode 4260 | Step 268043 | Epsilon 0.000 | Clean Rewards: Alpha 423.192, Beta 452.95 | Adversarial Rewards: Alpha 456.591, Beta 443.66 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.755000114440918, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.626999855041504, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.079s\n",
            "Episode 4280 | Step 269303 | Epsilon 0.000 | Clean Rewards: Alpha 427.902, Beta 447.95 | Adversarial Rewards: Alpha 467.135, Beta 444.081 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.911999702453613, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.835000038146973, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.232s\n",
            "Episode 4300 | Step 270563 | Epsilon 0.000 | Clean Rewards: Alpha 431.11, Beta 438.46 | Adversarial Rewards: Alpha 462.583, Beta 435.065 | Clean Losses: Alpha 0.105, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.059000015258789, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.920999526977539, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.865s\n",
            "Episode 4320 | Step 271823 | Epsilon 0.000 | Clean Rewards: Alpha 440.136, Beta 435.432 | Adversarial Rewards: Alpha 461.251, Beta 441.81 | Clean Losses: Alpha 0.105, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.10099983215332, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.029999732971191, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.917s\n",
            "Episode 4340 | Step 273083 | Epsilon 0.000 | Clean Rewards: Alpha 435.127, Beta 448.286 | Adversarial Rewards: Alpha 454.157, Beta 439.122 | Clean Losses: Alpha 0.105, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.032999992370605, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.01200008392334, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.827s\n",
            "Episode 4360 | Step 274343 | Epsilon 0.000 | Clean Rewards: Alpha 432.177, Beta 448.01 | Adversarial Rewards: Alpha 453.655, Beta 445.359 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.82800006866455, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.904000282287598, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.828s\n",
            "Episode 4380 | Step 275603 | Epsilon 0.000 | Clean Rewards: Alpha 437.423, Beta 447.373 | Adversarial Rewards: Alpha 445.672, Beta 437.295 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 13.82699966430664, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.875, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.822s\n",
            "Episode 4400 | Step 276863 | Epsilon 0.000 | Clean Rewards: Alpha 445.817, Beta 448.453 | Adversarial Rewards: Alpha 453.18, Beta 440.57 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 13.894000053405762, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.001999855041504, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.135s\n",
            "Episode 4420 | Step 278123 | Epsilon 0.000 | Clean Rewards: Alpha 450.638, Beta 445.432 | Adversarial Rewards: Alpha 448.794, Beta 431.502 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.01099967956543, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.055999755859375, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.988s\n",
            "Episode 4440 | Step 279383 | Epsilon 0.000 | Clean Rewards: Alpha 470.338, Beta 434.543 | Adversarial Rewards: Alpha 449.86, Beta 443.693 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.288999557495117, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.159000396728516, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.04s\n",
            "Episode 4460 | Step 280643 | Epsilon 0.000 | Clean Rewards: Alpha 470.258, Beta 424.936 | Adversarial Rewards: Alpha 449.089, Beta 430.062 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.467000007629395, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.404999732971191, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.852s\n",
            "Episode 4480 | Step 281903 | Epsilon 0.000 | Clean Rewards: Alpha 459.63, Beta 428.702 | Adversarial Rewards: Alpha 453.378, Beta 436.746 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.369999885559082, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.255999565124512, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 59.876s\n",
            "Episode 4500 | Step 283163 | Epsilon 0.000 | Clean Rewards: Alpha 447.087, Beta 430.983 | Adversarial Rewards: Alpha 452.005, Beta 437.474 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.262999534606934, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.12600040435791, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.92s\n",
            "Episode 4520 | Step 284423 | Epsilon 0.000 | Clean Rewards: Alpha 430.662, Beta 431.95 | Adversarial Rewards: Alpha 445.317, Beta 436.622 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.02299976348877, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.914999961853027, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.89s\n",
            "Episode 4540 | Step 285683 | Epsilon 0.000 | Clean Rewards: Alpha 416.801, Beta 429.136 | Adversarial Rewards: Alpha 454.323, Beta 425.432 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.657999992370605, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.678999900817871, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.975s\n",
            "Episode 4560 | Step 286943 | Epsilon 0.000 | Clean Rewards: Alpha 427.535, Beta 430.652 | Adversarial Rewards: Alpha 443.502, Beta 421.769 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.675999641418457, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.562999725341797, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.072s\n",
            "Episode 4580 | Step 288203 | Epsilon 0.000 | Clean Rewards: Alpha 429.788, Beta 430.744 | Adversarial Rewards: Alpha 442.753, Beta 425.669 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.67199993133545, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.625, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 59.863s\n",
            "Episode 4600 | Step 289463 | Epsilon 0.000 | Clean Rewards: Alpha 444.895, Beta 438.725 | Adversarial Rewards: Alpha 441.194, Beta 419.195 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.626999855041504, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.567000389099121, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.065s\n",
            "Episode 4620 | Step 290723 | Epsilon 0.000 | Clean Rewards: Alpha 451.901, Beta 439.553 | Adversarial Rewards: Alpha 449.764, Beta 431.336 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.9350004196167, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.798999786376953, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.299s\n",
            "Episode 4640 | Step 291983 | Epsilon 0.000 | Clean Rewards: Alpha 447.903, Beta 445.634 | Adversarial Rewards: Alpha 448.15, Beta 442.877 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.12399959564209, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.012999534606934, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.263s\n",
            "Episode 4660 | Step 293243 | Epsilon 0.000 | Clean Rewards: Alpha 435.99, Beta 453.01 | Adversarial Rewards: Alpha 460.571, Beta 441.44 | Clean Losses: Alpha 0.105, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.10099983215332, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.053999900817871, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.33s\n",
            "Episode 4680 | Step 294503 | Epsilon 0.000 | Clean Rewards: Alpha 440.205, Beta 449.36 | Adversarial Rewards: Alpha 462.229, Beta 429.945 | Clean Losses: Alpha 0.105, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.107000350952148, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.019000053405762, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.19s\n",
            "Episode 4700 | Step 295763 | Epsilon 0.000 | Clean Rewards: Alpha 438.22, Beta 435.203 | Adversarial Rewards: Alpha 460.352, Beta 431.527 | Clean Losses: Alpha 0.105, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.184000015258789, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.133000373840332, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.191s\n",
            "Episode 4720 | Step 297023 | Epsilon 0.000 | Clean Rewards: Alpha 445.18, Beta 432.245 | Adversarial Rewards: Alpha 462.994, Beta 425.73 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.157999992370605, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.21399974822998, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.519s\n",
            "Episode 4740 | Step 298283 | Epsilon 0.000 | Clean Rewards: Alpha 453.299, Beta 435.186 | Adversarial Rewards: Alpha 450.891, Beta 426.902 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.190999984741211, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.13599967956543, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.22s\n",
            "Episode 4760 | Step 299543 | Epsilon 0.000 | Clean Rewards: Alpha 459.01, Beta 429.113 | Adversarial Rewards: Alpha 454.279, Beta 431.998 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.133999824523926, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.161999702453613, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.453s\n",
            "Episode 4780 | Step 300803 | Epsilon 0.000 | Clean Rewards: Alpha 455.715, Beta 431.313 | Adversarial Rewards: Alpha 445.788, Beta 442.275 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 14.069999694824219, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.161999702453613, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.327s\n",
            "Episode 4800 | Step 302063 | Epsilon 0.000 | Clean Rewards: Alpha 449.657, Beta 443.786 | Adversarial Rewards: Alpha 445.516, Beta 443.31 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 13.960000038146973, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.020999908447266, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.516s\n",
            "Episode 4820 | Step 303323 | Epsilon 0.000 | Clean Rewards: Alpha 438.955, Beta 450.368 | Adversarial Rewards: Alpha 441.301, Beta 447.375 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 13.720999717712402, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.774999618530273, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.325s\n",
            "Episode 4840 | Step 304583 | Epsilon 0.000 | Clean Rewards: Alpha 434.99, Beta 441.993 | Adversarial Rewards: Alpha 448.857, Beta 441.478 | Clean Losses: Alpha 0.103, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 13.609999656677246, Beta 0.35499998927116394 | Adversarial Qs: Alpha 13.758000373840332, Beta 0.35499998927116394 | Avg Episode Length: 62.0 | Time Delta: 60.36s\n",
            "Episode 4860 | Step 305843 | Epsilon 0.000 | Clean Rewards: Alpha 432.384, Beta 448.102 | Adversarial Rewards: Alpha 445.775, Beta 447.424 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.103, Beta 0.2 | Clean Qs: Alpha 13.593000411987305, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.729999542236328, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.814s\n",
            "Episode 4880 | Step 307103 | Epsilon 0.000 | Clean Rewards: Alpha 424.986, Beta 457.568 | Adversarial Rewards: Alpha 458.11, Beta 451.673 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.723999977111816, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.770000457763672, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.519s\n",
            "Episode 4900 | Step 308363 | Epsilon 0.000 | Clean Rewards: Alpha 421.26, Beta 468.904 | Adversarial Rewards: Alpha 462.049, Beta 447.981 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.840999603271484, Beta 0.3540000021457672 | Adversarial Qs: Alpha 13.904999732971191, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.383s\n",
            "Episode 4920 | Step 309623 | Epsilon 0.000 | Clean Rewards: Alpha 432.445, Beta 469.482 | Adversarial Rewards: Alpha 466.469, Beta 451.17 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 13.947999954223633, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.008000373840332, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.409s\n",
            "Episode 4940 | Step 310883 | Epsilon 0.000 | Clean Rewards: Alpha 426.795, Beta 480.814 | Adversarial Rewards: Alpha 468.65, Beta 451.06 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.105, Beta 0.2 | Clean Qs: Alpha 14.095999717712402, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.232999801635742, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.351s\n",
            "Episode 4960 | Step 312143 | Epsilon 0.000 | Clean Rewards: Alpha 434.109, Beta 475.773 | Adversarial Rewards: Alpha 468.061, Beta 459.188 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.196000099182129, Beta 0.35499998927116394 | Adversarial Qs: Alpha 14.33899974822998, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.37s\n",
            "Episode 4980 | Step 313403 | Epsilon 0.000 | Clean Rewards: Alpha 448.894, Beta 467.079 | Adversarial Rewards: Alpha 464.938, Beta 451.631 | Clean Losses: Alpha 0.104, Beta 0.2 | Adversarial Losses: Alpha 0.104, Beta 0.2 | Clean Qs: Alpha 14.277999877929688, Beta 0.3540000021457672 | Adversarial Qs: Alpha 14.401000022888184, Beta 0.3540000021457672 | Avg Episode Length: 62.0 | Time Delta: 60.229s\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import datetime\n",
        "\n",
        "# Check if CUDA is available\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print(f\"Using CUDA: {use_cuda}\\n\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "#torch.manual_seed(0)\n",
        "#np.random.seed(0)\n",
        "\n",
        "# Model parameters\n",
        "user_features = 1\n",
        "doc_features = 1\n",
        "num_candidates = 10\n",
        "slate_size = 3\n",
        "batch_size = 32\n",
        "num_contexts = 5\n",
        "\n",
        "# Setup save directory for logs and plots\n",
        "save_dir = Path(\"checkpoints_fed_adv_manual\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
        "save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Initialize logger\n",
        "logger = Logger(save_dir)\n",
        "\n",
        "# Initialize agents\n",
        "agent_alpha = AgentAlpha(user_features, doc_features, num_candidates, slate_size, batch_size, num_contexts)\n",
        "agent_beta = AgentBeta(user_features, doc_features, num_candidates, slate_size, batch_size)\n",
        "agent_fed = AgentFed(user_features, doc_features, num_candidates, slate_size, batch_size)\n",
        "\n",
        "# Initialize environment\n",
        "env = RecsimEnv(num_candidates, slate_size, True, 42, 42)\n",
        "\n",
        "# Training parameters\n",
        "episodes = 5000\n",
        "epsilon_adv = 0.01  # Magnitude for FGSM perturbation\n",
        "episodes_until_perturb = 200 # change to 200 episodes\n",
        "\n",
        "for episode in range(episodes):\n",
        "    apply_attack = episode >= episodes_until_perturb\n",
        "    # Reset the environment and initialize agent states\n",
        "    agent_fed.act_ini(agent_alpha, agent_beta, env)\n",
        "\n",
        "    while True:\n",
        "        # ===== Clean Action and Learning Steps =====\n",
        "        # Agents take actions based on clean states and receive feedback\n",
        "        done_alpha_clean, reward_alpha_clean, done_beta_clean, reward_beta_clean = agent_fed.act(agent_alpha, agent_beta, env)\n",
        "        loss_alpha_clean, q_alpha_clean, loss_beta_clean, q_beta_clean = agent_fed.learn(agent_alpha, agent_beta)\n",
        "\n",
        "        # Log metrics for clean states\n",
        "        logger.log_step(\n",
        "            total_reward_alpha=reward_alpha_clean,\n",
        "            total_reward_alpha_adv=None,\n",
        "            loss_alpha_clean=loss_alpha_clean,\n",
        "            loss_alpha_adv=None,\n",
        "            q_alpha_clean=q_alpha_clean,\n",
        "            q_alpha_adv=None,\n",
        "            reward_beta_clean=reward_beta_clean,\n",
        "            reward_beta_adv=None,\n",
        "            loss_beta_clean=loss_beta_clean,\n",
        "            loss_beta_adv=None,\n",
        "            q_beta_clean=q_beta_clean,\n",
        "            q_beta_adv=None,\n",
        "        )\n",
        "\n",
        "        # ===== Adversarial Action and Learning Steps =====\n",
        "        if apply_attack and agent_fed.curr_step >= agent_fed.burnin:\n",
        "            # Generate adversarial state for Agent Alpha\n",
        "            agent_alpha.state.requires_grad = True\n",
        "            q_alpha_local = agent_alpha.compute_q_local()\n",
        "            loss_alpha_adv = -torch.mean(q_alpha_local)\n",
        "            agent_alpha.optimizer.zero_grad()\n",
        "            loss_alpha_adv.backward()\n",
        "            state_grad = agent_alpha.state.grad.data\n",
        "            adversarial_state_alpha = agent_alpha.state + epsilon_adv * state_grad.sign()\n",
        "            adversarial_state_alpha = torch.clamp(adversarial_state_alpha, -1, 1).detach()\n",
        "\n",
        "            # Replace the agent's state with the adversarial state\n",
        "            agent_alpha.state = adversarial_state_alpha\n",
        "\n",
        "            # Agents take actions based on adversarial states and receive feedback\n",
        "            done_alpha_adv, reward_alpha_adv, done_beta_adv, reward_beta_adv = agent_fed.act(agent_alpha, agent_beta, env)\n",
        "            loss_alpha_adv, q_alpha_adv, loss_beta_adv, q_beta_adv = agent_fed.learn(agent_alpha, agent_beta)\n",
        "\n",
        "            # Log metrics for adversarial states\n",
        "            logger.log_step(\n",
        "                total_reward_alpha=None,  # Already logged in clean step\n",
        "                total_reward_alpha_adv=reward_alpha_adv,\n",
        "                loss_alpha_clean=None,  # Already logged in clean step\n",
        "                loss_alpha_adv=loss_alpha_adv,\n",
        "                q_alpha_clean=None,  # Already logged in clean step\n",
        "                q_alpha_adv=q_alpha_adv,\n",
        "                reward_beta_clean=None,  # Beta is not attacked here\n",
        "                reward_beta_adv=reward_beta_adv,\n",
        "                loss_beta_clean=None,\n",
        "                loss_beta_adv=loss_beta_adv,\n",
        "                q_beta_clean=None,\n",
        "                q_beta_adv=q_beta_adv,\n",
        "            )\n",
        "\n",
        "            # Restore the clean state\n",
        "            agent_alpha.state = agent_alpha.state.detach()\n",
        "            agent_alpha.state.requires_grad = False\n",
        "\n",
        "        if done_alpha_clean or done_beta_clean:\n",
        "            break\n",
        "\n",
        "    # Log episode metrics\n",
        "    logger.log_episode()\n",
        "\n",
        "    if episode % 20 == 0:\n",
        "        logger.record(episode=episode, epsilon=agent_fed.exploration_rate, step=agent_fed.curr_step)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
